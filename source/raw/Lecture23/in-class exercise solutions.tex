\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{enumitem}

\begin{document}

\begin{center}
{\Large \textbf{In-Class Exercise --- Questions + Solutions}}\\[6pt]
{\large \textbf{Lecture 23: Explainability Paradigms and Evaluation}}
\end{center}

\vspace{10pt}

\begin{enumerate}[label=(\arabic*), itemsep=12pt]

\item \textbf{Indirect vs Direct.}  
For each method below, label it as a \textbf{Direct} or \textbf{Indirect} explanation:
\begin{enumerate}[label=(\alph*), itemsep=4pt]
    \item Activation visualizations of intermediate layers.
    \item Saliency map produced by occluding patches of the image and measuring the change in confidence for class $P$.
\end{enumerate}

\textbf{Solution.}
\begin{enumerate}[label=(\alph*), itemsep=4pt]
    \item Activation visualizations $\rightarrow \boxed{\textbf{Indirect}}$ (auxiliary insight about internal representations).
    \item Occlusion saliency $\rightarrow \boxed{\textbf{Direct}}$ (tests which input regions directly affect the prediction).
\end{enumerate}

\item \textbf{Targeted explanation type.}  
For each question below, classify it as:
\[
\text{Correlation (Why $P$?)} \quad\text{or}\quad \text{Counterfactual (What if?)} \quad\text{or}\quad \text{Contrastive (Why $P$ rather than $Q$?)}
\]
\begin{enumerate}[label=(\alph*), itemsep=4pt]
    \item ``Why Bullmastiff?''
    \item ``What if the dogâ€™s head shape were narrower?''
    \item ``Why Bullmastiff rather than Boxer?''
\end{enumerate}

\textbf{Solution.}
\begin{enumerate}[label=(\alph*), itemsep=4pt]
    \item \boxed{\textbf{Correlation (Why $P$?)}} (what features support class $P$).
    \item \boxed{\textbf{Counterfactual (What if?)}} (how prediction changes under a hypothetical modification).
    \item \boxed{\textbf{Contrastive (Why $P$ rather than $Q$?)}} (why $P$ is preferred over a specific alternative $Q$).
\end{enumerate}

\item \textbf{Evaluation type.}  
Match each method to the correct category:
\[
\text{Human Evaluation} \quad\text{or}\quad \text{Application Evaluation} \quad\text{or}\quad \text{Network Evaluation}
\]
\begin{enumerate}[label=(\alph*), itemsep=4pt]
    \item 43 Mechanical Turk workers choose which explanation is ``better.''
    \item Gaze tracking is used as a reference of salient regions and compared to explanation maps.
    \item Progressive pixel-wise deletion is performed and an AUC score is computed.
\end{enumerate}

\textbf{Solution.}
\begin{enumerate}[label=(\alph*), itemsep=4pt]
    \item \boxed{\textbf{Human Evaluation}} (humans directly judge explanation quality).
    \item \boxed{\textbf{Application Evaluation}} (uses a human-derived signal like gaze as a reference in a task setting).
    \item \boxed{\textbf{Network Evaluation}} (tests explanation quality via model behavior under deletion/insertion).
\end{enumerate}

\item \textbf{Insertion / Deletion metric logic.}  
Fill \textbf{``low"} or \textbf{``high"} in the blanks:
\begin{enumerate}[label=(\alph*), itemsep=4pt]
    \item A good explanation produces a \underline{\hspace{2cm}} AUC for deletion.
    \item A good explanation produces a \underline{\hspace{2cm}} AUC for insertion.
\end{enumerate}

\textbf{Solution.}
\begin{enumerate}[label=(\alph*), itemsep=4pt]
    \item \boxed{\textbf{low}} AUC for deletion (removing important pixels should quickly degrade confidence).
    \item \boxed{\textbf{high}} AUC for insertion (adding important pixels should quickly restore confidence).
\end{enumerate}

\item \textbf{Reasoning (masking evaluation pitfall).}  
A masking-based network evaluation keeps only the pixels inside an explanation heatmap and measures the model's confidence for class $P$.
Why might a \textbf{larger heatmap} appear to perform better under this evaluation, even if it is not minimal?
\begin{enumerate}[label=(\alph*), itemsep=4pt]
    \item Because a larger heatmap preserves more evidence, so the model's confidence stays higher even if the explanation is not specific.
    \item Because larger heatmaps always identify the unique minimal set of truly important pixels.
    \item Because larger heatmaps guarantee a lower deletion AUC, which always indicates better explanations.
    \item Because larger heatmaps necessarily reduce insertion AUC by adding evidence too quickly.
\end{enumerate}

\textbf{Solution.}
Keeping only pixels inside the heatmap means a larger heatmap typically removes less information from the input, making it easier for the model to maintain high confidence. This can inflate masking-based scores even when the explanation is not selective.
\[
\boxed{\textbf{Answer: (a)}}
\]

\end{enumerate}


\end{document}
