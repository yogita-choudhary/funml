\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{enumitem}

\begin{document}

\begin{center}
{\Large \textbf{In-Class Exercise (Canvas Quiz) --- 15 minutes}}\\[6pt]
{\large \textbf{Lecture 23: Explainability Paradigms and Evaluation}}
\end{center}

\vspace{8pt}

\noindent
\textbf{Context (only what you need for this quiz).}
We consider explanation \emph{types} (indirect vs.\ direct), \emph{targeted questions} (correlational / counterfactual / contrastive),
and \emph{evaluation} (human / application / network). In network evaluation, \emph{insertion} and \emph{deletion} curves summarize how predictions change as we add or remove pixels according to an explanation.

\vspace{10pt}
\noindent
\textbf{Scenario.}
A CNN classifies an input image as \textbf{Bullmastiff (P)}. A second plausible class is \textbf{Boxer (Q)}.

\vspace{8pt}
\noindent
\textbf{Tasks (answer directly):}
\begin{enumerate}[label=(\arabic*), itemsep=9pt]

\item \textbf{Indirect vs Direct.}  
For each method below, label it as a \textbf{Direct} or \textbf{Indirect} explanation:
\begin{enumerate}[label=(\alph*), itemsep=4pt]
    \item Activation visualizations of intermediate layers.
    \item Saliency map produced by occluding patches of the image and measuring the change in confidence for class $P$.
\end{enumerate}

\item \textbf{Targeted explanation type.}  
For each question below, classify it as:
\[
\text{Correlation (Why $P$?)} \quad\text{or}\quad \text{Counterfactual (What if?)} \quad\text{or}\quad \text{Contrastive (Why $P$ rather than $Q$?)}
\]
\begin{enumerate}[label=(\alph*), itemsep=4pt]
    \item ``Why Bullmastiff?''
    \item ``What if the dogâ€™s head shape were narrower?''
    \item ``Why Bullmastiff rather than Boxer?''
\end{enumerate}

\item \textbf{Evaluation type.}  
Match each method to the correct category:
\[
\text{Human Evaluation} \quad\text{or}\quad \text{Application Evaluation} \quad\text{or}\quad \text{Network Evaluation}
\]
\begin{enumerate}[label=(\alph*), itemsep=4pt]
    \item 43 Mechanical Turk workers choose which explanation is ``better.''
    \item Gaze tracking is used as a reference of salient regions and compared to explanation maps.
    \item Progressive pixel-wise deletion is performed and an AUC score is computed.
\end{enumerate}

\item \textbf{Insertion / Deletion metric logic.}  
Fill \textbf{``low"} or \textbf{``high"} in the blanks:
\begin{enumerate}[label=(\alph*), itemsep=4pt]
    \item A good explanation produces a \underline{\hspace{2cm}} AUC for deletion.
    \item A good explanation produces a \underline{\hspace{2cm}} AUC for insertion.
\end{enumerate}

\item \textbf{Reasoning (masking evaluation pitfall).}  
A masking-based network evaluation keeps only the pixels inside an explanation heatmap and measures the model's confidence for class $P$.
Why might a \textbf{larger heatmap} appear to perform better under this evaluation, even if it is not minimal?
\begin{enumerate}[label=(\alph*), itemsep=4pt]
    \item Because a larger heatmap preserves more evidence, so the model's confidence stays higher even if the explanation is not specific.
    \item Because larger heatmaps always identify the unique minimal set of truly important pixels.
    \item Because larger heatmaps guarantee a lower deletion AUC, which always indicates better explanations.
    \item Because larger heatmaps necessarily reduce insertion AUC by adding evidence too quickly.
\end{enumerate}

\vspace{6pt}
\noindent
\textbf{Canvas note:} Canvas contains only keywords; full questions are shown on the projector.

\end{enumerate}

\end{document}
