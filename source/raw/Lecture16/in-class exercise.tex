\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{enumitem}

\begin{document}

\begin{center}
{\Large \textbf{In-Class Exercise (Canvas Quiz) --- 10 minutes}}\\[6pt]
{\large \textbf{Lecture 16: Convolutional Neural Networks (Training)}}
\end{center}

\vspace{8pt}

\noindent
This exercise is based on the lecture discussion of training optimization for CNNs, including:
Gradient Descent variants (BGD, SGD, MBGD) and adaptive optimizers (AdaGrad, RMSProp, Adam).\\
(Full questions are shown in class on projector. Canvas contains keywords only.)

\vspace{8pt}
\noindent
\textbf{Tasks:} (Answer ALL)
\begin{enumerate}[label=(\arabic*), itemsep=8pt]

\item \textbf{(Multiple choice)} You have a dataset with $N=1,000,000$ datapoints.
Which method generally has the \textbf{highest memory/computation cost per epoch}?

\begin{enumerate}[label=(\Alph*)]
\item SGD
\item Mini-batch GD (MBGD)
\item Batch GD (BGD)
\end{enumerate}

\item \textbf{(Multiple choice)} Which method tends to have the \textbf{noisiest / highest-variance update path}?

\begin{enumerate}[label=(\Alph*)]
\item SGD
\item Mini-batch GD (MBGD)
\item Batch GD (BGD)
\end{enumerate}

\item \textbf{(True/False)} The stochasticity (noise) of SGD can sometimes help escape shallow local minima and saddle points.

\item \textbf{(Multiple choice)} Which optimizer can suffer from \textbf{learning rate decaying too aggressively over time} due to accumulating squared gradients?

\begin{enumerate}[label=(\Alph*)]
\item RMSProp
\item AdaGrad
\item Adam
\end{enumerate}

\item \textbf{(Multiple choice)} Which optimizer is best described as combining \textbf{Momentum + RMSProp-style adaptive learning rates}?

\begin{enumerate}[label=(\Alph*)]
\item RMSProp
\item AdaGrad
\item Adam
\end{enumerate}

\end{enumerate}

\end{document}
