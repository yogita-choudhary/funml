\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{enumitem}

\begin{document}

\begin{center}
{\Large \textbf{In-Class Exercise (Canvas Quiz) --- 10 minutes}}\\[6pt]
{\large \textbf{Lecture 16: Convolutional Neural Networks (Training)}}
\end{center}

\vspace{10pt}

\begin{enumerate}[label=(\arabic*), itemsep=12pt]


\item \textbf{Highest computation cost per epoch:}\\
You have a dataset with $N=1{,}000{,}000$ datapoints.
Which method generally has the \textbf{highest memory/computation cost per epoch}?

\begin{enumerate}[label=(\Alph*), itemsep=2pt]
\item SGD
\item Mini-batch GD (MBGD)
\item Batch GD (BGD)
\end{enumerate}

\vspace{4pt}
\noindent\textbf{Solution 1:}\\
\textbf{(C) BGD}. Batch gradient descent computes gradients using the entire dataset each epoch.
\[
\boxed{\textbf{Answer: (C)}}
\]


\item \textbf{Noisiest / highest variance update path:}\\
Which method tends to have the \textbf{noisiest / highest-variance update path}?

\begin{enumerate}[label=(\Alph*), itemsep=2pt]
\item SGD
\item Mini-batch GD (MBGD)
\item Batch GD (BGD)
\end{enumerate}

\vspace{4pt}
\noindent\textbf{Solution 2:}\\
\textbf{(A) SGD}. SGD uses a single datapoint per update, which produces high variance updates.
\[
\boxed{\textbf{Answer: (A)}}
\]


\item \textbf{SGD noise can help escape shallow minima / saddle points:}\\
\textbf{True/False:} The stochasticity (noise) of SGD can sometimes help escape shallow local minima and saddle points.

\vspace{4pt}
\noindent\textbf{Solution 3:}\\
\textbf{True}. Noisy gradients can help avoid getting stuck in saddle points or shallow local minima.
\[
\boxed{\textbf{Answer: True}}
\]


\item \textbf{Optimizer with aggressive LR decay from accumulating squared gradients:}\\
Which optimizer can suffer from \textbf{learning rate decaying too aggressively over time} due to accumulating squared gradients?

\begin{enumerate}[label=(\Alph*), itemsep=2pt]
\item RMSProp
\item AdaGrad
\item Adam
\end{enumerate}

\vspace{4pt}
\noindent\textbf{Solution 4:}\\
\textbf{(B) AdaGrad}. The accumulated gradient sum grows over time, shrinking the effective learning rate.
\[
\boxed{\textbf{Answer: (B)}}
\]


\item \textbf{Momentum + adaptive LR (RMSProp-style) combination:}\\
Which optimizer is best described as combining \textbf{Momentum + RMSProp-style adaptive learning rates}?

\begin{enumerate}[label=(\Alph*), itemsep=2pt]
\item RMSProp
\item AdaGrad
\item Adam
\end{enumerate}

\vspace{4pt}
\noindent\textbf{Solution 5:}\\
\textbf{(C) Adam}. Adam combines momentum (first moment) with adaptive learning rate scaling (second moment).
\[
\boxed{\textbf{Answer: (C)}}
\]

\end{enumerate}


\end{document}
