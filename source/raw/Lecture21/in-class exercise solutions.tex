\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{enumitem}

\begin{document}

\begin{center}
{\Large \textbf{In-Class Exercise --- Questions + Solutions}}\\[6pt]
{\large \textbf{Lecture 21: Sequence Modeling II (TCN + Attention)}}
\end{center}

\vspace{10pt}


\noindent
\textbf{Question 1 (TCN: single-layer receptive field).}\\
A \emph{single} dilated causal convolution layer has kernel size $k=3$ and dilation $d=2$.
Which input indices can affect $y[t]$ for this single layer?

\begin{enumerate}[label=(\Alph*), itemsep=3pt]
\item $\{t,\;t-1,\;t-2\}$
\item $\{t,\;t-2,\;t-4\}$
\item $\{t,\;t-2,\;t-3\}$
\item $\{t,\;t-4,\;t-8\}$
\end{enumerate}

\vspace{4pt}
\noindent\textbf{Solution 1:}\\
A dilated causal convolution with dilation $d$ and kernel size $k$ uses
\[
x[t],\;x[t-d],\;x[t-2d],\;\dots,\;x[t-(k-1)d].
\]
With $k=3$ and $d=2$, the indices are $\{t,\;t-2,\;t-4\}$.
\[
\boxed{\textbf{Answer: (B)}}
\]

\vspace{12pt}


\noindent
\textbf{Question 2 (TCN: concrete time index).}\\
Using the same layer ($k=3,d=2$), for $t=10$ which exact input indices are used?

\begin{enumerate}[label=(\Alph*), itemsep=3pt]
\item $\{10,\;9,\;8\}$
\item $\{10,\;8,\;6\}$
\item $\{10,\;6,\;2\}$
\item $\{10,\;12,\;14\}$
\end{enumerate}

\vspace{4pt}
\noindent\textbf{Solution 2:}\\
Plug $t=10$ into $\{t,\;t-2,\;t-4\}$:
\[
\{10,\;8,\;6\}.
\]
\[
\boxed{\textbf{Answer: (B)}}
\]

\vspace{12pt}


\noindent
\textbf{Question 3 (Attention: which token gets most weight?).}\\
Dot-product attention uses
\[
e_i=\mathbf{s}^\top \mathbf{h}_i,\qquad
\alpha_i=\frac{\exp(e_i)}{\sum_{j=1}^3 \exp(e_j)}.
\]
Given
\[
\mathbf{s}=\begin{bmatrix}1\\0\end{bmatrix},\quad
\mathbf{h}_1=\begin{bmatrix}1\\0\end{bmatrix},\quad
\mathbf{h}_2=\begin{bmatrix}0\\1\end{bmatrix},\quad
\mathbf{h}_3=\begin{bmatrix}-1\\0\end{bmatrix},
\]
which vector receives the \emph{largest} attention weight?

\begin{enumerate}[label=(\Alph*), itemsep=3pt]
\item $\mathbf{h}_1$
\item $\mathbf{h}_2$
\item $\mathbf{h}_3$
\item All equal
\end{enumerate}

\vspace{4pt}
\noindent\textbf{Solution 3:}\\
The largest weight corresponds to the largest score $e_i=\mathbf{s}^\top \mathbf{h}_i$.
Compute scores conceptually from alignment with $\mathbf{s}=[1,0]^\top$:
\[
e_1=1,\qquad e_2=0,\qquad e_3=-1.
\]
So $\alpha_1$ is the largest.
\[
\boxed{\textbf{Answer: (A)}}
\]

\vspace{12pt}


\noindent
\textbf{Question 4 (Attention: score ordering).}\\
With the same $\mathbf{s},\mathbf{h}_1,\mathbf{h}_2,\mathbf{h}_3$, which ordering is correct?

\begin{enumerate}[label=(\Alph*), itemsep=3pt]
\item $e_1 > e_2 > e_3$
\item $e_2 > e_1 > e_3$
\item $e_3 > e_2 > e_1$
\item $e_1 = e_2 = e_3$
\end{enumerate}

\vspace{4pt}
\noindent\textbf{Solution 4:}\\
From Solution 3, $(e_1,e_2,e_3)=(1,0,-1)$, so
\[
e_1>e_2>e_3.
\]
\[
\boxed{\textbf{Answer: (A)}}
\]

\vspace{12pt}


\noindent
\textbf{Question 5 (Attention: direction/sign of context vector).}\\
The context vector is
\[
\mathbf{c}=\sum_{i=1}^3 \alpha_i \mathbf{h}_i.
\]
Without computing the full softmax numerically, which statement is correct about $\mathbf{c}$?

\begin{enumerate}[label=(\Alph*), itemsep=3pt]
\item The first component of $\mathbf{c}$ is positive and the second component is positive.
\item The first component of $\mathbf{c}$ is negative and the second component is positive.
\item The first component of $\mathbf{c}$ is zero and the second component is positive.
\item Both components of $\mathbf{c}$ are zero.
\end{enumerate}

\vspace{4pt}
\noindent\textbf{Solution 5:}\\
We have $\alpha_1>\alpha_2>\alpha_3$ and
\[
\mathbf{h}_1=\begin{bmatrix}1\\0\end{bmatrix},\quad
\mathbf{h}_2=\begin{bmatrix}0\\1\end{bmatrix},\quad
\mathbf{h}_3=\begin{bmatrix}-1\\0\end{bmatrix}.
\]
So
\[
\mathbf{c}
=\alpha_1\begin{bmatrix}1\\0\end{bmatrix}
+\alpha_2\begin{bmatrix}0\\1\end{bmatrix}
+\alpha_3\begin{bmatrix}-1\\0\end{bmatrix}
=
\begin{bmatrix}\alpha_1-\alpha_3\\ \alpha_2\end{bmatrix}.
\]
Since $\alpha_1>\alpha_3$, the first component is positive; since $\alpha_2>0$, the second component is positive.
\[
\boxed{\textbf{Answer: (A)}}
\]

\end{document}
