\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{enumitem}

\begin{document}

\begin{center}
{\Large \textbf{In-Class Exercise (Canvas Quiz) --- 10 minutes}}\\[6pt]
{\large \textbf{Lecture 22: Explainability in Neural Networks}}
\end{center}

\vspace{8pt}

We study explanation methods for CNN classifiers:
\begin{itemize}
\item \textbf{Occlusion saliency:} mask a region and observe the drop in class probability.
\item \textbf{Gradient saliency:} use $\frac{\partial y_c}{\partial \mathbf{x}}$ as pixel importance.
\item \textbf{Grad-CAM:} uses gradients w.r.t. the last convolutional feature maps $\mathbf{A}^k$:
\[
\alpha_k^c = \frac{1}{Z}\sum_i\sum_j \frac{\partial y_c}{\partial A_{ij}^k},
\qquad
L_{\text{Grad-CAM}}^{c} = \mathrm{ReLU}\!\left(\sum_k \alpha_k^c \mathbf{A}^k\right).
\]
\end{itemize}

\vspace{6pt}
\noindent
\textbf{Given:} A classifier predicts class $c$ with probability $p=0.85$.  
Masking three patches gives probabilities:
\[
p_1=0.60,\qquad p_2=0.82,\qquad p_3=0.30.
\]

\noindent
For Grad-CAM, suppose the last convolutional layer has two feature maps:
\[
\mathbf{A}^1=
\begin{bmatrix}
1 & 2\\
0 & 1
\end{bmatrix},
\qquad
\mathbf{A}^2=
\begin{bmatrix}
2 & 1\\
1 & 0
\end{bmatrix},
\qquad
\alpha_1^c=0.50,\quad \alpha_2^c=-0.25.
\]

\begin{enumerate}[label=(\arabic*), itemsep=10pt]

\item \textbf{Occlusion Saliency (Most Important Patch)}\\
Which patch is \textbf{most important} for predicting class $c$ (largest probability drop)?
\begin{enumerate}[label=(\Alph*)]
    \item Patch 1
    \item Patch 2
    \item Patch 3
    \item All patches are equally important
\end{enumerate}

\item \textbf{Faithfulness vs Cost (Occlusion)}\\
Why is occlusion saliency considered \textbf{computationally expensive}?
\begin{enumerate}[label=(\Alph*)]
    \item It requires retraining the model.
    \item It requires many forward passes with masked inputs.
    \item It needs gradients of all layers.
    \item It increases the number of model parameters.
\end{enumerate}

\item \textbf{Gradient Saliency (Concept)}\\
Why do input gradients approximate pixel importance?
\begin{enumerate}[label=(\Alph*)]
    \item They linearize the model locally around the input.
    \item They guarantee causal explanations.
    \item They remove nonlinearities from the network.
    \item They compute feature correlations.
\end{enumerate}

\item \textbf{Why the Last Convolutional Layer?}\\
Grad-CAM uses the \textbf{last convolutional layer} because it:
\begin{enumerate}[label=(\Alph*)]
    \item Has the largest number of parameters.
    \item Preserves spatial structure and captures high-level semantics.
    \item Produces the final class probabilities.
    \item Eliminates the need for backpropagation.
\end{enumerate}

\item \textbf{Grad-CAM (After ReLU)}\\
Let $S=\alpha_1^c\mathbf{A}^1+\alpha_2^c\mathbf{A}^2$. After applying
$L_{\text{Grad-CAM}}^{c}=\mathrm{ReLU}(S)$, what happens to \textbf{negative} entries of $S$?
\begin{enumerate}[label=(\Alph*)]
    \item They become positive.
    \item They remain negative.
    \item They become zero.
    \item They are doubled in magnitude.
\end{enumerate}

\item \textbf{Concept Check (True/False)}\\
Guided Backpropagation can produce sharp visual explanations but may fail to be \textbf{class-discriminative}.
\begin{enumerate}[label=(\Alph*)]
    \item True
    \item False
\end{enumerate}

\end{enumerate}

\end{document}
