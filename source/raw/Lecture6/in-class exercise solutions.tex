\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{enumitem}

\begin{document}

\begin{center}
{\Large \textbf{In-Class Exercise --- Solutions}}\\[6pt]
{\large \textbf{Lecture 6: Classification Performance Evaluation}}
\end{center}

\vspace{8pt}

\noindent \textbf{Question 1} 

\noindent A dataset has \textbf{95\% class 0, 5\% class 1}. 

\noindent A classifier predicts \textbf{all samples as class 0}.

\noindent Which of these statements is true?

(A) Accuracy is high and the model is good. 

(B) Recall for class 1 is high.

(C) Precision for class 1 is undefined (or indeterminate $\frac{0}{0}$).

(D) The classifier has learned meaningful decision boundaries. 

\noindent \textbf{Solution:} \textbf{C} 

Although its accuracy is $95\%$, which is high, this classifier is not necessarily a good model if it only predicts class 0 all the time, as it is often used as a baseline, so (A) is incorrect. 

The classifier predicts no positive (all negative) labels, so there are no true positives, and 5\% of all predictions are false negatives, and $\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}$, so its recall is zero. (B) is incorrect. 

The classifier predicts no positive labels, so there are no true positives or false positives, and $\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}$, so we get $\text{Precision} = \frac{0}{0}$, which is indeterminate form (or undefined). \textbf{(C) is correct.}

If a classifier has predicted all negative labels, it has never learned any meaningful decision boundary. Hence, (D) is incorrect. \\

\noindent \textbf{Question 2} 

\noindent When training with \textbf{class-weighted loss}, increasing the weight of the minority class will:

(A) Reduce gradients from minority samples

(B) Increase gradients from minority samples

(C) Leave gradients unchanged

(D) Only affect evaluation metrics 


\noindent \textbf{Solution:} \textbf{B} 

Increasing the weight of the minority class multiplies the loss for those samples, which directly scales up their gradient contributions; hence, updates caused by minority samples become larger, and their impact on parameter learning becomes larger. \textbf{(B) is correct.} \\

\noindent \textbf{Question 3} 

\noindent Lowering the classification threshold for the positive class generally:

(A) Increases precision, decreases recall

(B) Increases recall, decreases precision

(C) Increases both precision and recall

(D) Has no effect

\noindent \textbf{Solution:} \textbf{B} 

$\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}$

$\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}$

Lowering the classification threshold for the positive class increases the number of positive predictions, increasing the number of true positives (TP) and false positives (FP), decreasing the number of false negatives (FN). 

Since FN decreases, the denominator $\text{TP} + \text{FN}$ shrinks and recall increases.

However, lowering the threshold allows more \textit{low-confidence positive predictions}, which increases FP along with TP. Since FP increases, the denominator $\text{TP} + \text{FP}$ grows, so precision typically decreases.

Hence, recall increases wile precision decreases - \textbf{(B) is correct}. 

\end{document}