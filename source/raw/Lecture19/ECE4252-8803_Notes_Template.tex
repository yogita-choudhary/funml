%% Adapted from GaTech CS 7545 Machine Learning Theory given by Prof. Jacob Abernathy Fall 2018. Original latex file: https://github.com/mltheory/CS7545/blob/master/scribe/CS7545scribe_template.tex 



%%%%%PLEASE CONSIDER CORRECTIONS AT PLACES INDICATED%%%%%%%%
\documentclass{article}
%%%%%Packages Used, add more if necessary%%%%
\usepackage{amsmath,amsfonts,amssymb,graphicx,fullpage, float}
\setlength{\topmargin}{-0.6 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\usepackage{mdframed}
\usepackage{multicol}
\usepackage{microtype}
\usepackage{animate}
\usepackage{amsmath}
\usepackage{float}
\usepackage{url, hyperref}
\usepackage{breakurl}



%%%%% NO NEED TO EDIT THIS PREAMBLE %%%%%%%%
%%% PREAMBLE %%%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
      
      \hbox to 6.28in { 
      {\bf ECE 4252/6252 (FunML) Fundamentals of Machine Learning \hfill 2021--2028} }
      
      \vspace{2mm}
      \hbox to 6.28in { 
      {Courses developed by Professor Ghassan AlRegib 
      (\href{https://alregib.ece.gatech.edu/}{alregib.ece.gatech.edu}) 
      \hfill For inquiries: \href{mailto:alregib@gatech.edu}{alregib@gatech.edu}} }
      
      \vspace{3mm}
      \hbox to 6.28in { {\Large \hfill Lecture #1: #2 \hfill} }
      
      \vspace{2mm}
      \hbox to 6.28in { {\it Co-Instructors: #3 \hfill #4} }
      
      \vspace{2mm}}
   }
   \end{center}

   % -------- PEOPLE OUTSIDE BOX --------
   \vspace{2mm}
   \noindent{\bf Contributors:} Dr. Ahmad Mustafa, Dr. Motaz Alfarraj, Dr. Ashraf Alattar, Dr. Chen Zhou

   \vspace{1mm}
   \noindent{\bf Teaching Assistants} with remarkable contributions include: Kuo-Wei Lai, Wuyang Du, Shiva Mahato, Michael Zhou, Ninghan Zhong

   \markboth{Lecture #1: #2}{Lecture #1: #2}

   \vspace{2mm}
\noindent {\bf Disclaimer}: 
{All content of these notes are part of this course at Georgia Tech. Any re-use or distribution is not permitted without pre-approved permission. All these notes belong to, created by, and copyrighted for Ghassan AlRegib and Mohit Prabhushankar, Georgia Tech, 2021--2028.}

\vspace{2mm}
\noindent {\bf License}: 
{These lecture notes are licensed under the 
\href{https://creativecommons.org/licenses/by-nc-sa/4.0/}{Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License}.}

   \vspace{2mm}
   \noindent {\bf Errata}: 
   {\it Please submit any errata you find using the following form: 
   \href{https://forms.office.com/r/fbg9dMWPgY}{Errata Form for FunML Textbook} 
   or visit: \url{https://forms.office.com/r/fbg9dMWPgY}}

   \vspace*{4mm}
}
\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

\newcommand{\challenge}[2]{\noindent \textbf{(Challenge Problem)} \emph{#1}: #2 }

\newcommand{\exercise}[1]{\noindent \textbf{(Exercise)} #1 }

\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
%%% END_OF_PREAMBLE %%%


	
%%%%You may add more \newtheorem if necessary%%%%%%
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}


\newcommand{\dom}{\mathrm{dom}}
\begin{document}

%%%%%CHANGE HERE%%%%%%%
%%%%%\section{title of the section} similarly with the rest \section{} or \subsection{} or \subsubsection{} etc


%%%%%CHANGE HERE%%%%%%%%%
%%%%%\lecture{the ordinal number of the lecture}{lecture title}{Jacob Abernethy}{scriber's name}%%%%%%%%
\lecture{19}{AutoEncoder Extensions}{Ghassan AlRegib and Mohit Prabhushankar}{}%%

%%%%%CHANGE HERE%%%%%%%
%%%%%\section{title of the section} similarly with the rest \section{} or \subsection{} or \subsubsection{} etc

\section{Lecture Objectives}

\hspace{1.3em}
This lecture dives into different forms of autoencoders and their applications, specifically focusing on \textbf{regularized autoencoders}, including \textbf{sparse autoencoders}, \textbf{denoising autoencoders}, and \textbf{variational autoencoders}. Regularized autoencoders gain distinct properties by introducing constraints through various forms of regularization, enhancing their versatility for tasks such as data compression, feature extraction, noise removal, and new content generation.

First, we examine \textbf{sparse autoencoders}, which encourage only a few neurons to activate for each input, resulting in compact representations that highlight essential data features. Another example is \textbf{denoising autoencoders}, designed to reconstruct clean data from noisy inputs, making them effective for tasks like noise removal. Finally, we explore \textbf{variational autoencoders}, which employ probabilistic techniques to create smooth, flexible data representations, enabling the generation of new samples with characteristics similar to the original data.


% \section{Recap of last lecture} 
% %%%%%Use itemize to layout bullet points that are not numbered%%%%%
% \hspace{1.3em}
%  In our last lecture, we explored autoencoders -- neural networks designed to encode input data into a compressed latent representation to capture and retain essential features while discarding redundant information. This compressed representation is then decoded to accurately reconstruct the original data on the output side. The last lecture focused on the architecture of autoencoders to explore how they perform dimensional reduction, feature extraction, and data reconstruction effectively.

% \subsection{Highlights from last lecture} 
% %%%%%Use itemize to layout bullet points that are not numbered%%%%%

% \begin{itemize}
%     \item \textbf{Intro to Autoencoders}: Autoencoders are neural networks composed of two main parts: an encoder, which compresses the input \textbf{unlabeled data} into a compact, lower-dimensional latent representation, and a decoder, which reconstructs the original data from this representation. They are \textbf{unsupervised} learning tools that help simplify data by identifying essential features and hidden structures, making data easier to store and process through dimensionality reduction.


    
%     \item \textbf{Types of Autoencoders}:
%     \begin{itemize}
%         \item Fully-Connected Autoencoders: We dove into the structure of basic autoencoders, which are applied to the MNIST dataset (a classic set of handwritten digit images). We also covered how both linear and nonlinear versions reconstruct images, with visuals showing what each type learns.
        
%         \item Convolutional Autoencoders: These use convolutional layers to capture spatial details in images, which is great for preserving patterns. Techniques like max pooling and unpooling were introduced to adjust image resolution as needed.
%     \end{itemize}
    
%     \item \textbf{Transposed Convolutions}: This section explained how transposed convolutions work as a learnable way to “scale up” images, comparing this to max-unpooling for upsampling.
    
%     \item \textbf{Experimental Results}: We wrapped up the lecture by comparing how well linear, nonlinear, and convolutional autoencoders perform in terms of quality and accuracy when reconstructing images.
    
%     \item \textbf{Limitations of Autoencoders}: Basic autoencoders can often overfit the training data, capturing noise and irrelevant details rather than general, meaningful patterns. This is especially problematic with high-dimensional data, where the model may memorize specific inputs instead of learning generalizable features.
% \end{itemize}


% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.75\linewidth]{img/lecture19/graphs2.JPG}
%     \caption{Example of Overfitting}
%     \label{fig:enter-label}
% \end{figure}

% \begin{itemize}
%     \item Notice in Figure 19.1 with each application of the Reconstruction Loss equation, the line fits the data better each time in graphs 1-4. However, notice in graphs 5 and 6 that overfitting takes place.  The curve swirls up and down adding noise to our curve fitting.  In graph 6, the model has memorized the exact locations of the input data.  If I were to introduce new data to this model, it would struggle to properly classify the data and yield poor performance overall.  To solve overfitting, we introduce regularization techniques for better classification and thus performance.
% \end{itemize}

\section{Regularized Autoencoders}

While basic autoencoders are effective at learning compressed representations, they often struggle with overfitting and may learn trivial identity mappings when the model has sufficient capacity. As neural networks became larger and more expressive, these limitations motivated the development of \textbf{regularized autoencoder variants}. 

Regularized autoencoders introduce additional constraints that encourage the model to learn meaningful and robust features rather than simply memorizing the training data. In this section, we explore several important extensions, including \textbf{sparse autoencoders}, \textbf{denoising autoencoders}, and \textbf{variational autoencoders}, which improve generalization and enable autoencoders to handle more complex real-world tasks.


\subsection{Regularized Autoencoders Overview}

Basic autoencoders rely on a low-dimensional bottleneck to force the network to learn compressed representations of the data. However, modern neural networks often have enough capacity to bypass this constraint and learn a trivial identity mapping, simply copying the input to the output without extracting meaningful structure. Regularized autoencoders address this limitation by encouraging the model to learn informative representations through explicit constraints on the learning process rather than relying solely on dimensionality reduction.

The key idea is that \textbf{compression is enforced through regularization instead of a small latent space}. This allows the latent representation \(Z\) to be high-dimensional while still capturing useful and generalizable features. By constraining how the encoder and decoder learn, the model is encouraged to discover structure in the data rather than memorize the training set. In this lecture, we study three major regularization strategies: \textbf{sparse autoencoders}, \textbf{denoising autoencoders}, and \textbf{variational autoencoders}.

The objective of regularization is to prevent the encoder–decoder network from learning a direct 1:1 mapping between the input and the output. Without regularization, a high-capacity autoencoder can ``cheat'' by memorizing the training data instead of learning meaningful patterns. This is analogous to memorizing answers for an exam without understanding the underlying concepts. Regularization forces the model to extract structure and learn features that generalize to new data.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{img/lecture19/121mapping1.JPG}
    \caption{Regularization prevents the undesirable identity mapping and encourages the model to learn meaningful patterns.}
    \label{fig:enter-label}
\end{figure}

\paragraph{Key Takeaways}
\begin{itemize}
    \item Regularized autoencoders can use \textbf{high-dimensional latent spaces}. Representation quality is enforced by regularization rather than by a dimensional bottleneck.
    \item Regularization discourages redundant or trivial features and improves the model's ability to \textbf{generalize to unseen data}.
\end{itemize}




\subsection{Sparse Autoencoders}

Sparse autoencoders encourage the network to activate only a small subset of neurons for any given input. Instead of forcing compression through a small latent dimension, sparsity forces the model to represent each input using only a few active features. This encourages the network to discover distinctive and interpretable patterns in the data.

The key idea is that even if the latent space is large, the model is restricted in how many neurons it can use at once. As a result, the autoencoder cannot simply copy the input using all available neurons and must instead learn a compact set of meaningful features.

\paragraph{Motivation.}
Sparse representations are widely used in machine learning and neuroscience because they tend to capture the most informative structure in data. By activating only a subset of neurons during each forward pass, the model is prevented from memorizing the input and is encouraged to learn reusable features that are useful for downstream tasks such as classification and clustering.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{img/lecture19/SparseMapping.JPG}
    \caption{Sparse autoencoders activate only a subset of neurons for any given input.}
\end{figure}

\paragraph{How sparsity is enforced.}
Sparse autoencoders introduce a regularization term into the loss function:

\[
\mathcal{L}_{total} = \mathcal{L}_{reconstruction} + \lambda \, \Omega(Z)
\]

where the regularization term \( \Omega(Z) \) penalizes excessive neuron activation.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{img/lecture19/LossFunction4.JPG}
    \caption{Sparse autoencoders add a regularization term to the reconstruction loss.}
\end{figure}

Two common techniques are used to enforce sparsity.

\paragraph{L1 Regularization on Activations.}

One simple method is to apply an \(L_1\) penalty to the latent activations:

\[
\Omega(Z) = \sum_i \lVert z_i \rVert_1
\]

The \(L_1\) norm encourages many activations to become exactly zero. This leads to a compact and efficient representation where only the most important neurons remain active.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.55\linewidth]{img/lecture19/L1norm.JPG}
    \caption{L1 regularization encourages many latent activations to become zero.}
\end{figure}

By suppressing less important activations, the model effectively performs feature selection and noise reduction, focusing on the most relevant patterns in the data.

\paragraph{KL-Divergence Sparsity Constraint.}

Another common approach is to control the \emph{average activation} of each neuron. We enforce that the expected activation \( \hat{\rho}_j \) of neuron \( j \) remains close to a small target value \( \rho \):

\[
\Omega(Z) = \sum_j D_{KL}(\rho \,\|\, \hat{\rho}_j)
\]

This penalty increases when neurons activate too frequently, encouraging each neuron to fire only rarely. As a result, each neuron specializes in detecting a small number of patterns.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{img/lecture19/KL.JPG}
    \caption{KL divergence penalizes neurons that activate too frequently.}
\end{figure}

This constraint produces a distributed but sparse representation in which different neurons respond to different structures in the data.

\paragraph{Training.}
During training, the reconstruction loss and sparsity penalty are optimized jointly. The strength of the sparsity constraint is controlled by the hyperparameter \( \lambda \), which determines how strongly sparsity is enforced.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{img/lecture19/Sparse4.JPG}
    \caption{Sparse autoencoders represent inputs using only a few active features.}
\end{figure}

\paragraph{Key Takeaways:}
\begin{itemize}
    \item Sparse autoencoders learn representations using only a small number of active neurons.
    \item Sparsity encourages feature discovery, noise reduction, and better generalization.
    \item Sparsity can be enforced using \(L_1\) penalties or KL-divergence constraints on activations.
\end{itemize}


\subsection{Denoising Autoencoders}

Denoising autoencoders learn robust representations by training the model to reconstruct clean inputs from corrupted versions. Instead of simply copying the input, the network must learn the underlying structure of the data in order to remove noise. This encourages the encoder to capture stable and meaningful features that are useful even when the input is partially corrupted.

\paragraph{Objective}
The goal of a denoising autoencoder is to learn features that can remove noise from corrupted data. During training, noise is intentionally added to the input, and the model is asked to reconstruct the original clean version. Because the model never sees the clean input directly, it must learn the true structure of the data rather than memorizing pixel-level details.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\linewidth]{img/lecture19/DeNoiseManifold.JPG}
    \caption{Denoising autoencoder: corrupted inputs are mapped back to clean outputs.}
\end{figure}

A useful way to understand denoising autoencoders is through the concept of a \emph{data manifold}. Real-world data often lies on a lower-dimensional, smooth manifold embedded in a high-dimensional space. Clean data points lie on this manifold, while noisy or corrupted inputs are pushed away from it. The denoising autoencoder learns to map corrupted inputs back to the nearest point on the learned manifold, effectively pulling noisy samples toward the region where true data lives.

\paragraph{Training Process}

During training, a corrupted version of the input, denoted by \( \tilde{X} \), is passed through the encoder. The decoder then attempts to reconstruct the original clean input \( X \). The dimensionality of the latent space is not the key factor in denoising; instead, robustness emerges from the training objective and the corruption process.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{img/lecture19/NoiseLoss.JPG}
    \caption{Loss function used to train a denoising autoencoder.}
\end{figure}

The model is trained by minimizing the reconstruction loss:

\[
\mathcal{L} = \|X - \hat{X}\|^2, \quad \hat{X} = G_\phi(E_\theta(\tilde{X}))
\]

Here, \(E_\theta\) is the encoder that maps the noisy input to a latent representation, and \(G_\phi\) is the decoder that reconstructs the clean data. The squared error places larger penalties on large reconstruction mistakes, encouraging the model to recover the clean signal as accurately as possible.

\paragraph{Why Denoising Works}

By learning to remove noise, the model is forced to capture stable patterns that are shared across many examples. This acts as a powerful form of regularization. The learned representation becomes robust to perturbations, small variations, and measurement errors.

\paragraph{Use Cases and Limitations}

Denoising autoencoders are widely used in image restoration, signal processing, and representation learning for noisy datasets. They are especially useful when real-world data is imperfect or corrupted.

However, denoising autoencoders do not impose strong structure on the latent space. If we randomly sample points from the latent space, there is no guarantee that they will decode into realistic data. This limitation motivates the development of \emph{variational autoencoders}, which explicitly structure the latent space for generative modeling.

\subsection{Variational Autoencoders}

Variational Autoencoders (VAEs) extend traditional autoencoders by combining representation learning with generative modeling. While standard autoencoders learn how to compress and reconstruct data, they do not impose structure on the latent space. As a result, randomly sampling from the latent space often produces unrealistic outputs. VAEs solve this problem by learning a \emph{probabilistic} latent space that is continuous, smooth, and suitable for generation.

\paragraph{Probabilistic Latent Space}

Instead of encoding each input as a single point in latent space, a VAE encodes each input as a \emph{distribution}. The encoder outputs the mean \( \mu \) and variance \( \sigma^2 \) of a Gaussian distribution that represents where the data point is likely to lie in the latent space.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{img/lecture19/VAECloudVisualization.JPG}
    \caption{Variational autoencoder and continuous latent space visualization.}
\end{figure}

Sampling from these distributions produces latent points that are close to one another in a continuous manner. This continuity allows smooth interpolation between data points and enables the model to generate realistic new samples.

\paragraph{Generative Modeling View}

VAEs model the true data distribution \(p(X)\) using a latent variable model. The generative process assumes:

\[
Z \sim p(Z), \qquad X \sim p_\phi(X|Z)
\]

where the prior \(p(Z)\) is typically a standard Gaussian. The marginal likelihood of the data is:

\[
p_\phi(X) = \int p(Z)p_\phi(X|Z)\, dZ
\]

Maximizing this likelihood allows the model to generate samples that resemble the training data.

\paragraph{Why Direct Likelihood is Intractable}

Computing the marginal likelihood requires integrating over all possible latent variables \(Z\), which becomes computationally infeasible for high-dimensional spaces. The posterior distribution

\[
p(Z|X) = \frac{p(X|Z)p(Z)}{p(X)}
\]

is also intractable because it depends on the same difficult integral. VAEs solve this challenge using \emph{variational inference}.

\paragraph{Variational Inference and the ELBO}

Instead of computing the true posterior, VAEs introduce a simpler approximation \(q_\theta(Z|X)\). Training then maximizes the \emph{Evidence Lower Bound (ELBO)}, which serves as a tractable objective:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.70\linewidth]{img/lecture19/ELBO2.JPG}
    \caption{Evidence Lower Bound (ELBO) objective.}
\end{figure}

The ELBO has two components:

\[
\mathcal{L}_{ELBO} =
\underbrace{\mathbb{E}_{q(z|x)}[\log p(x|z)]}_{\text{Reconstruction}}
-
\underbrace{D_{KL}(q(z|x)\|p(z))}_{\text{Regularization}}
\]

The reconstruction term encourages accurate decoding, while the KL-divergence term forces the learned latent distribution to remain close to the prior. This creates a structured and smooth latent space suitable for sampling.

\paragraph{Reparameterization Trick}

Training VAEs requires backpropagation through random sampling. Direct sampling would block gradient flow, so VAEs use the \emph{reparameterization trick}. Instead of sampling \(z \sim \mathcal{N}(\mu,\sigma^2)\), we rewrite:

\[
z = \mu + \sigma \cdot \epsilon, \qquad \epsilon \sim \mathcal{N}(0,1)
\]

This separates randomness from the learnable parameters, allowing gradients to flow through the network.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\linewidth]{img/lecture19/Trick2.JPG}
    \caption{Reparameterization trick for gradient-based training.}
\end{figure}

\paragraph{Reconstruction and Generation Modes}

Because the latent space is probabilistic and structured, VAEs support two modes of operation. During reconstruction, an input is encoded into a distribution and decoded back to recreate the original data. During generation, we bypass the encoder and directly sample from the latent prior to create new data.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\linewidth]{img/lecture19/NovelSamples.JPG}
    \caption{Generation mode: sampling from latent space.}
\end{figure}

\paragraph{Smooth Interpolation in Latent Space}

The Gaussian prior encourages nearby latent points to decode into similar outputs. Moving smoothly through latent space therefore produces gradual and realistic changes in generated data.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{img/lecture19/VAEMorphing1.JPG}
    \caption{Smooth transitions in VAE latent space.}
\end{figure}

\paragraph{Key Advantages}

VAEs provide a principled framework for learning meaningful latent representations while enabling realistic data generation. They support interpolation, anomaly detection, and unsupervised representation learning, making them a powerful extension of traditional autoencoders.

% \section{Summary}
% \hspace{1.3em}
% Autoencoders are unsupervised neural networks designed to learn efficient data representations by encoding input data into a compressed latent space and then reconstructing it as closely as possible at the output. While basic autoencoders focus solely on minimizing reconstruction error, regularized variations introduce additional constraints that enhance their utility and versatility. L1 regularization encourages sparsity in the latent space, making the model focus on key features and filter out noise. KL divergence regularization, often used in sparse and variational autoencoders, enforces a specific distribution (typically Gaussian) on the latent space, creating a structured and continuous space that is useful for generation and interpretability. Denoising autoencoders add robustness by training the model to reconstruct clean data from noisy inputs, improving generalization and noise tolerance. Variational autoencoders (VAEs) take this a step further by using probabilistic sampling in the latent space, allowing for smooth interpolation between data points and the generation of novel, realistic outputs. Together, these variations extend the foundational autoencoder’s capabilities, making it applicable in feature extraction, dimensionality reduction, data generation, and anomaly detection. Each regularization approach refines the autoencoder's performance to fit specific tasks, whether through sparsity, structured continuity, robustness, or generative power.


\section{Summary}

Regularized autoencoders extend basic autoencoders by introducing constraints that prevent trivial identity mapping and encourage meaningful feature learning.

\paragraph{Key Takeaways}
\begin{itemize}
    \item Basic autoencoders rely on dimensional bottlenecks, while regularized autoencoders enforce structure through additional loss terms.
    \item \textbf{Sparse autoencoders} encourage feature discovery by activating only a small subset of neurons.
    \item \textbf{Denoising autoencoders} learn robust representations by reconstructing clean data from corrupted inputs.
    \item \textbf{Variational autoencoders (VAEs)} introduce a probabilistic latent space, enabling generation of new data and smooth interpolation.
    \item Regularized autoencoders are widely used for representation learning, dimensionality reduction, anomaly detection, and generative modeling.
\end{itemize}

\section{Q \& A Section}

\begin{enumerate}

\item \textbf{Question:} 
Why can a standard autoencoder fail when the latent dimension is large?

\textbf{Solution:}  
If the latent space is large, the network has enough capacity to simply copy the input to the output. Instead of learning meaningful structure, it learns a near identity mapping, which limits its usefulness for representation learning.

\item \textbf{Question:} 
How does a sparse autoencoder address this issue?

\textbf{Solution:}  
Sparse autoencoders add a penalty that forces most latent neurons to remain inactive. Because only a few neurons can activate at once, the model must learn efficient and informative features instead of copying the entire input.

\item \textbf{Question:} 
What is the main idea behind denoising autoencoders?

\textbf{Solution:}  
Denoising autoencoders are trained on corrupted inputs but must reconstruct the original clean data. This forces the network to learn stable patterns that reflect the underlying structure of the data, making the learned representation more robust to noise.

\item \textbf{Question:} 
Why are basic autoencoders not suitable for generating new data?

\textbf{Solution:}  
The latent space in a basic autoencoder is not structured or constrained to follow any particular distribution. Randomly sampling from this space may produce unrealistic outputs because there is no guarantee that sampled points correspond to valid data.

\item \textbf{Question:} 
What distinguishes a Variational Autoencoder (VAE) from other autoencoder variants?

\textbf{Solution:}  
A VAE models the latent space probabilistically and enforces a structured distribution (typically Gaussian). This makes the latent space continuous and smooth, allowing meaningful sampling and generation of new data.

\end{enumerate}


\section{References}
% \subsection{List of Resources Used}

\begin{itemize}

\item See Figure 19.14 for the image sourced from Khan Academy. (n.d.). What is an autoencoder? [Video]. YouTube. \url{https://www.youtube.com/watch?v=iwEzwTTalbg}

\item See Figure 19.8 for the image sourced from 3Blue1Brown. (n.d.). Neural networks: How they work [Video]. YouTube. \url{https://www.youtube.com/watch?v=aircAruvnKk}

\item deeplearning.ai. (n.d.). Deep learning explained [Video]. YouTube. \url{https://www.youtube.com/watch?v=2859tNY-G5E}

\item See Figure 19.2, 19.3, 19.9, and 19.11 for the image sourced from Data School. (n.d.). Autoencoders in deep learning [Video]. YouTube. \url{https://www.youtube.com/watch?v=CiexUMrNtBQ}

\item Two Minute Papers. (n.d.). The future of deep learning research [Video]. YouTube. \url{https://www.youtube.com/watch?v=b8AzCgY1gZI}

\item See Figure 19.4 for the image sourced from StatQuest with Josh Starmer. (n.d.). Regularization and overfitting [Video]. YouTube. \url{https://www.youtube.com/watch?v=xwrzh4e8DLs}


\item Saturn Cloud. (n.d.). Sparse autoencoders. \url{https://saturncloud.io/glossary/sparse-autoencoders/}

\sloppy
\item Schafer, A. (n.d.). L1 norm, regularization, and sparsity explained for dummies. ML Review. \url{https://blog.mlreview.com/l1-norm-regularization-and-sparsity-explained-for-dummies-5b0e4be3938a}

\sloppy
\item Rathi, M. (n.d.). Neural network terminology explained. Mukul Rathi. \url{https://mukulrathi.com/demystifying-deep-learning/neural-network-terminology-explained/}


\item See Figure 19.1 for the image sourced from Yadav, S. (n.d.). Overfitting and regularization in machine learning. Towards Data Science. \url{https://towardsdatascience.com/over-fitting-and-regularization-64d16100f45c}


\item Chorri, M. (n.d.). Difference between KL divergence and PSI. Medium. \url{https://medium.com/@mumbaiyachori/difference-between-kl-divergence-and-psi-e7d9aa0ade12}

\item Çaglar, M. (2020, September 10). Kullback-Leibler divergence explained [Post]. X. \url{https://x.com/caglarml/status/1304051370367094787}


\item See Figure 19.7 for the image sourced from Ryan, G. (2022, October 1). Kullback-Leibler divergence: The origin of a key concept in information theory [Post]. LinkedIn. \url{https://www.linkedin.com/posts/gabriel-ryan-frm-ba304915_kullback-liebler-divergence-the-origin-of-activity-6978629222726578178-wAUy}

\item See Figure 19.17 for the image sourced from Van Rensburg, E. (n.d.). Generating the intuition behind variational autoencoders (VAEs). Medium. \url{https://medium.com/@elzettevanrensburg/generating-the-intuition-behind-variational-auto-encoders-vaes-c7d2f8631a87}


\item See Figure 19.13 for the image sourced from Ram, A. (n.d.). Bayes theorem with conditional probability. Medium. \url{https://medium.com/@ram420/bayes-theorem-with-conditional-probability-793bf9caba92}

\item See Figure 19.6, 19.11 and 19.17 for the image sourced from Krasser, F. (2018, April 7). Latent space optimization. \url{https://krasserm.github.io/2018/04/07/latent-space-optimization/}

\item Bose, S. (n.d.). Comparison of autoencoders vs. variational autoencoders. Medium. \url{https://medium.com/@jwbtmf/comparison-of-autoencoders-vs-variational-autoencoders-7993442bb377}

\item Mandal, S. (n.d.). Difference between overfitting and underfitting in machine learning. Medium. \url{https://medium.com/@soumallya160/difference-of-overfitting-underfitting-7f0bd08fb8a6}

\item alregib2024neural] @miscalregib2024neural, author = Ghassan AlRegib and Mohit Prabhushankar, title =
Lecture 18: Autoencoders, year = 2024, howpublished = ECE 4803/8803: Fundamentals of Machine
Learning (FunML), Georgia Institute of Technology, Lecture Notes, note = Available from FunML course
materials

\item See Figure 19.9, 19.15 and 19.16 for the image sourced from alregib2024neural] @miscalregib2024neural, author = Ghassan AlRegib and Mohit Prabhushankar, title =
 Lecture 19: Autoencoder Extensions, year = 2024, howpublished = ECE 4803/8803: Fundamentals of Machine
Learning (FunML), Georgia Institute of Technology, Lecture Notes, note = Available from FunML course
materials

\end{itemize}

% \section{Common Notations}

% \begin{multicols}{2}
% \begin{itemize}
% \item $\mathbf{b}$: Bias vector
% \item $C_k$: K-th cluster
% \item $d(\mathbf{x_j, x_k})$: Dissimilarity between $\mathbf{x_j, x_k}$
% \item $E_\theta$: Encoding function
% \item $f(\cdot)$: Trained neural network
% \item $\mathbf{G}(t)$: Second moment at time t
% \item $G_\Phi$: Decoding function
% \item $\mathbf{H(\theta)}$: Hessian matrix
% \item $h_i, h_j$: Representation space vectors
% \item $k^{(i)}$: Number of neurons in the $i^{th}$ layer
% \item $M$: Number of features in a feature vector
% \item $m$: Degree of polynomial
% \item $m_j$: J-th centroid
% \item $N$: Number of data samples
% \item $P$: Predicted class
% \item $P^{(k)}$: The number of neurons in layer k
% \item $Q$: Contrast class
% \item $Q_k$: Computed clustering for k-th cluster
% \item $R_k$: Ground truth clustering for k-th cluster
% \item $s(\mathbf{x_j, x_k})$: Similarity between $\mathbf{x_j, x_k}$
% \item $v(t)$: First moment at time t
% \item $\mathbf{W}$: Weight matrix
% \item $w_{ij}$: Degree of membership of $\mathbf{x_i}$ in $C_j$
% \item $\mathbf{X}$: Matrix of feature vectors (dataset)
% \item $\mathbf{\hat{X}}$: Reconstruction of data
% \item $\widetilde{\mathbf{X}}$: Corrupted input
% \item $\mathbf{x_i}$: Feature vector (a data sample)
% \item $\mathbf{x_{:,i}}$: Feature vector of all data samples
% \item $x_i$: A single feature
% \item $\mathbf{Y}$: Output matrix
% \item $y_i$: Target class
% \item $y^{c}$: Predicted logit for class P
% \item $y^{i}$: Logit for any class i
% \item $\mathbf{Z}$: Latent representation
% \item \textls[-20]{$z_i$: Latent variables representing the embedding of $\mathbf{x_i}$}
% \item $\alpha$: Learning rate
% \item $\gamma$: Bias factor
% \item $\gamma_i^j$: Posterior of $\mathbf{x_i}$ coming from cluster j
% \item $\epsilon$: Error margin
% \item $\tilde{\lambda_j}$: Average activation of neuron $z_{ij}$
% \item $\boldsymbol{\theta}$: Coefficient vector
% \item $\theta_i$: A single model coefficient (parameter)
% \item $\hat{\rho_j}$: Average activation of neuron $z_{ij}$
% \item $\mathbf{\Omega(Z)}$: Sparsity constraint

% \end{itemize}
% \end{multicols}

\end{document}
