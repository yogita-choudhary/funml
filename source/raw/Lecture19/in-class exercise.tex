\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{enumitem}

\begin{document}

\begin{center}
{\Large \textbf{In-Class Exercise (Canvas Quiz) --- 10 minutes}}\\[6pt]
{\large \textbf{Lecture 19: Autoencoder Extensions (Sparse, Denoising, VAE)}}
\end{center}

\vspace{8pt}

\noindent
This quiz focuses on three regularized autoencoders discussed in Lecture 19:
\begin{itemize}
\item \textbf{Sparse AE}: encourages sparse latent activations (e.g., via an $\ell_1$ penalty)
\item \textbf{Denoising AE}: reconstructs clean $\mathbf{X}$ from corrupted $\tilde{\mathbf{X}}$
\item \textbf{VAE}: models latent variables probabilistically and uses the reparameterization trick
\end{itemize}

\vspace{12pt}


\noindent
\textbf{Question 1 (Sparse AE --- what sparsity does):}

A sparse autoencoder trains with
\[
\mathcal{L}(\mathbf{x},\hat{\mathbf{x}}) = \|\mathbf{x}-\hat{\mathbf{x}}\|_2^2 + \lambda \|\mathbf{z}\|_1.
\]
Which statement best captures the effect of the $\lambda\|\mathbf{z}\|_1$ term?

\begin{enumerate}[label=(\Alph*), itemsep=3pt]
\item It forces every coordinate of $\mathbf{z}$ to be large in magnitude.
\item It encourages many coordinates of $\mathbf{z}$ to be exactly (or near) zero, so only a few latent units are active.
\item It makes the autoencoder supervised by introducing labels.
\item It guarantees perfect reconstruction on any dataset.
\end{enumerate}

\vspace{12pt}


\noindent
\textbf{Question 2 (Sparse AE --- effect of increasing $\lambda$):}

Suppose you increase $\lambda$ while keeping the model architecture fixed.
Which outcome is \textbf{most likely} on the training set?

\begin{enumerate}[label=(\Alph*), itemsep=3pt]
\item Latent activations become less sparse; more units activate.
\item Latent activations become more sparse, but reconstruction quality may worsen.
\item Reconstruction must improve because regularization always helps.
\item The encoder becomes the exact inverse of the decoder.
\end{enumerate}

\vspace{12pt}


\noindent
\textbf{Question 3 (Denoising AE --- what loss compares):}

A denoising autoencoder uses
\[
\hat{\mathbf{X}} = G_{\boldsymbol{\phi}}\!\left(E_{\boldsymbol{\theta}}(\tilde{\mathbf{X}})\right),
\qquad
\min \ \|\mathbf{X}-\hat{\mathbf{X}}\|_2^2.
\]
Choose the correct pairing of \textbf{input to encoder} and \textbf{target in the loss}.

\begin{enumerate}[label=(\Alph*), itemsep=3pt]
\item Input: $\mathbf{X}$ \quad Target: $\tilde{\mathbf{X}}$
\item Input: $\tilde{\mathbf{X}}$ \quad Target: $\mathbf{X}$
\item Input: $\tilde{\mathbf{X}}$ \quad Target: $\tilde{\mathbf{X}}$
\item Input: $\mathbf{X}$ \quad Target: $\mathbf{X}$ (no corruption needed)
\end{enumerate}

\vspace{12pt}


\noindent
\textbf{Question 4 (Denoising AE --- what it learns conceptually):}

Why can denoising autoencoders learn \emph{more robust} representations than plain autoencoders?

\begin{enumerate}[label=(\Alph*), itemsep=3pt]
\item They are trained to copy noise exactly, so they become noise-sensitive.
\item They must map corrupted inputs back toward clean structure, encouraging features stable under corruption.
\item They guarantee the latent space follows a Gaussian distribution.
\item They do not use any reconstruction loss.
\end{enumerate}

\vspace{12pt}


\noindent
\textbf{Question 5 (VAE --- what reparameterization enables):}

A VAE samples
\[
\mathbf{z} = \boldsymbol{\mu} + \boldsymbol{\sigma}\odot \boldsymbol{\epsilon},
\qquad
\boldsymbol{\epsilon}\sim \mathcal{N}(\mathbf{0},\mathbf{I}),
\]
where $\odot$ is elementwise multiplication.
What is the main reason we write sampling in this form?

\begin{enumerate}[label=(\Alph*), itemsep=3pt]
\item It removes randomness completely so the model becomes deterministic.
\item It allows gradients to backpropagate through $\boldsymbol{\mu}$ and $\boldsymbol{\sigma}$ despite stochastic sampling.
\item It makes the decoder a strict inverse of the encoder.
\item It guarantees that $\mathbf{z}$ has no variance.
\end{enumerate}

\end{document}
