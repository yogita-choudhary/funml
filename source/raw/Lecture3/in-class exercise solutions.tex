\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{enumitem}

\begin{document}

\begin{center}
{\Large \textbf{In-Class Exercise --- Solutions}}\\[6pt]
{\large \textbf{Lecture 3: Logistic Regression + Cross-Entropy (log base 2)}}
\end{center}

\vspace{8pt}

We use logistic regression:
\[
s = w^T x + b, 
\qquad 
z = \sigma(s) = \frac{1}{1+e^{-s}}
\]

\noindent
\textbf{Given:}
\[
w = [1,-2], 
\qquad 
b = 0.5, 
\qquad 
x = [2,1]
\]

\vspace{8pt}

\noindent
\textbf{Solutions:}
\begin{enumerate}[label=(\arabic*), itemsep=8pt]

\item \textbf{Compute the logit $s$:}
\[
w^T x = 1\cdot 2 + (-2)\cdot 1 = 2 - 2 = 0
\]
\[
s = w^T x + b = 0 + 0.5 = 0.5
\]
\[
\boxed{s = 0.50}
\]

\item \textbf{Compute the predicted probability $z=\sigma(s)$:}
\[
z = \sigma(0.5) = \frac{1}{1+e^{-0.5}} \approx 0.622459
\]
\[
\boxed{z = 0.62}
\]

\item \textbf{Cross-entropy loss (log base 2):}
\begin{itemize}[itemsep=6pt]
    \item For $y=1$:
    \[
    L = -\log_2(z) = -\log_2(0.622459) \approx 0.68
    \]
    \[
    \boxed{L(y=1) = 0.68}
    \]

    \item For $y=0$:
    \[
    1-z \approx 1-0.622459 = 0.377541
    \]
    \[
    L = -\log_2(1-z) = -\log_2(0.377541) \approx 1.41
    \]
    \[
    \boxed{L(y=0) = 1.41}
    \]
\end{itemize}

\end{enumerate}

\vspace{6pt}
\noindent
\textbf{Final answers (2 decimals):} \quad
$\boxed{s=0.50,\; z=0.62,\; L(y=1)=0.68,\; L(y=0)=1.41}$

\end{document}
