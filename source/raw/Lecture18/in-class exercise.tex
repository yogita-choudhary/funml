\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{enumitem}

\begin{document}

\begin{center}
{\Large \textbf{In-Class Exercise (Canvas Quiz) --- 10 minutes}}\\[6pt]
{\large \textbf{Lecture 18: Autoencoders}}
\end{center}

\vspace{8pt}

\noindent
We study an \textbf{autoencoder} with encoder and decoder
\[
\mathbf{z} = f(\mathbf{x}), \qquad \hat{\mathbf{x}} = g(\mathbf{z}),
\]
trained using \textbf{MSE reconstruction loss}
\[
L(\mathbf{x},\hat{\mathbf{x}})=\frac{1}{n}\sum_{i=1}^{n}(x_i-\hat{x}_i)^2.
\]

\begin{enumerate}[label=(\arabic*), itemsep=14pt]

\item \textbf{What does the bottleneck enforce?}\\
Suppose the latent dimension is smaller than the input dimension: $\dim(\mathbf{z}) \ll \dim(\mathbf{x})$.
What is the \textbf{most accurate} interpretation of what this ``bottleneck'' forces the model to do?

\begin{enumerate}[label=(\Alph*), itemsep=2pt]
\item Memorize $\mathbf{x}$ exactly, since reconstruction is always perfect.
\item Learn a compressed representation that preserves information useful for reconstructing $\mathbf{x}$.
\item Perform supervised classification of $\mathbf{x}$ into labels.
\item Guarantee the learned features are invariant to all transformations of the input.
\end{enumerate}

\item \textbf{Effect of increasing latent dimension}\\
If we \emph{increase} $\dim(\mathbf{z})$ while keeping the model class flexible enough, which outcome is \textbf{most likely}?

\begin{enumerate}[label=(\Alph*), itemsep=2pt]
\item Reconstruction error on the training set tends to decrease (or stay the same).
\item Reconstruction error on the training set must increase.
\item The encoder and decoder become exact inverses of each other.
\item The model becomes supervised.
\end{enumerate}

\item \textbf{What does MSE encourage?}\\
For image reconstruction, MSE tends to produce reconstructions that are:

\begin{enumerate}[label=(\Alph*), itemsep=2pt]
\item Sharper, since it strongly rewards high-frequency details.
\item Often blurrier, because it averages over plausible pixel values.
\item Always identical to the input, regardless of model capacity.
\item Invariant to translation by design.
\end{enumerate}

\item \textbf{Why convolutional autoencoders for images? (True/False)}\\
A convolutional autoencoder is usually preferred over a fully-connected autoencoder for image data because
it preserves \emph{spatial locality} and uses \emph{weight sharing}, which reduces parameters and exploits image structure.

\item \textbf{Decoder intuition (best statement)}\\
Which statement best describes the decoder $g(\cdot)$ in an autoencoder?

\begin{enumerate}[label=(\Alph*), itemsep=2pt]
\item It is a guaranteed mathematical inverse of the encoder.
\item It learns a mapping from the latent code back to the input space to approximate reconstruction.
\item It only works if $\dim(\mathbf{z})=\dim(\mathbf{x})$.
\item It computes labels for classification.
\end{enumerate}

\end{enumerate}

\end{document}
