\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{enumitem}

\begin{document}

\begin{center}
{\Large \textbf{In-Class Exercise --- Solutions}}\\[6pt]
{\large \textbf{Lecture 5: Introduction to Neural Networks (Single-Layer Perceptron)}}
\end{center}

% \vspace{8pt}
% \noindent \textbf{Question 1}
% \noindent We use a single-layer perceptron with sigmoid activation:
% \[
% h = \mathbf{w}^T \mathbf{x} + b,
% \qquad
% \hat{y} = \sigma(h) = \frac{1}{1+\exp(-h)}
% \]
% Perceptron weight update rule:
% \[
% \mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} + \alpha \mathbf{x} \,\delta,
% \qquad
% b^{(t+1)} = b^{(t)} + \alpha \delta,
% \qquad
% \delta = y - \hat{y}
% \]

% \vspace{6pt}
% \noindent
% \textbf{Given:}
% \[
% \mathbf{w}^{(0)} = \begin{bmatrix} 0.50 \\ -0.25 \end{bmatrix},
% \qquad
% b^{(0)} = 0.10,
% \qquad
% \alpha = 0.20
% \]
% and the following training sample:
% \[
% \mathbf{x}= \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \qquad y = 1
% \]

% \vspace{8pt}


% \begin{enumerate}[label=(\arabic*), itemsep=6pt]
%     \item Compute the neuron input: \quad $h = \mathbf{w}^T \mathbf{x} + b$.
%     \item Compute the predicted output: \quad $\hat{y} = \sigma(h)$.
%     \item Compute the error: \quad $\delta = y - \hat{y}$.
%     \item Perform \textbf{one} update step and compute:
%     \[
%     \mathbf{w}^{(1)}, \quad b^{(1)}
%     \]
% \end{enumerate}

% \vspace{6pt}
% \noindent
% \textbf{Round all final answers to 2 decimals.}

% \vspace{8pt}
% \noindent
% \textbf{Solutions:}
% \begin{enumerate}[label=(\arabic*), itemsep=10pt]

% \item \textbf{Compute $h = \mathbf{w}^T \mathbf{x} + b$:}
% \[
% \mathbf{w}^T \mathbf{x} = 0.50(1) + (-0.25)(0) = 0.50
% \]
% \[
% h = 0.50 + 0.10 = 0.60
% \]
% \[
% \boxed{h = 0.60}
% \]

% \item \textbf{Compute $\hat{y} = \sigma(h)$:}
% \[
% \hat{y} = \sigma(0.60) = \frac{1}{1+\exp(-0.60)} \approx 0.6457
% \]
% \[
% \boxed{\hat{y} = 0.65}
% \]

% \item \textbf{Compute error $\delta = y-\hat{y}$:}
% \[
% \delta = 1 - 0.6457 = 0.3543
% \]
% \[
% \boxed{\delta = 0.35}
% \]

% \item \textbf{Update $w$ and $b$:}

% Weight update:
% \[
% \mathbf{w}^{(1)} = \mathbf{w}^{(0)} + \alpha x \delta
% \]
% \[
% \alpha x \delta = 0.20 \cdot [1,0] \cdot 0.3543 = [0.07086,\,0]
% \]
% \[
% \mathbf{w}^{(1)} = [0.50,-0.25] + [0.07086,0] = [0.57086, -0.25]
% \]
% \[
% \boxed{\mathbf{w}^{(1)} = [0.57,\,-0.25]}
% \]

% Bias update:
% \[
% b^{(1)} = b^{(0)} + \alpha \delta 
% = 0.10 + 0.20(0.3543)= 0.17086
% \]
% \[
% \boxed{b^{(1)} = 0.17}
% \]

% \end{enumerate}

% \vspace{6pt}
% \noindent
% \textbf{Final answers (2 decimals):}
% \[
% \boxed{h=0.60,\;\hat{y}=0.65,\;\delta=0.35,\;\mathbf{w}^{(1)}=[0.57,-0.25],\;b^{(1)}=0.17}
% \]



\vspace{8pt}

\noindent \textbf{Question 1} 
\noindent Is the following statement True or False?  

\noindent \textit{``In an SLP, If $\mathbf{x} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}$ and both components of $\mathbf{w}$ are positive, then both weights will always increase during training.''}

\vspace{8pt}
\noindent
\textbf{Solutions:}

\noindent The statement is \textbf{False}.


\noindent Consider the following scenario:
True label: $y = 0$, 
Input: $\mathbf{x} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}$, 
Weights: $\mathbf{w} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}$ (both positive), and 
Bias: $b = 0$. 

\noindent The neuron input is:
    \[
    h = \mathbf{w}^T \mathbf{x} + b = (1)(1) + (1)(1) + 0 = 2
    \]
    
\noindent With $h = 2 > 0$, we get:
    \[
    \hat{y} = \sigma(2) \approx 0.88 > 0.5
    \]
    
\noindent The error becomes: 
    \[
    \delta = y - \hat{y} = 0 - 0.88 = -0.88 < 0
    \]
    
Finally, the weight update is:
    \[
    \Delta \mathbf{w} = \alpha \mathbf{x} \delta = \alpha \begin{bmatrix} 1 \\ 1 \end{bmatrix} \times (-0.88) = \begin{bmatrix} -0.88\alpha \\ -0.88\alpha \end{bmatrix} < \mathbf{0}
    \]


\vspace{6pt}
\noindent Both weights \textbf{decrease}, not increase, contradicting the statement.

\noindent In general, the direction of weight change depends on the \textbf{error} $\delta = y - \hat{y}$, not just the signs of $\mathbf{x}$ and $\mathbf{w}$. If the model \textbf{over-predicts} ($\hat{y} > y$), then $\delta < 0$, and weights move in the \textbf{opposite} direction of $\mathbf{x}$. But if the model \textbf{under-predicts} ($\hat{y} < y$), then $\delta > 0$, and weights move in the \textbf{same} direction of $\mathbf{x}$. Even with positive weights and positive inputs, the weights can decrease if the model over-predicts the target.


\vspace{8pt}

\noindent \textbf{Question 2} 
\noindent Consider two training samples for a binary classifier:

\textbf{Sample A:}
\[
\mathbf{x}_A = \begin{bmatrix} 0.1 \\ 0.1 \end{bmatrix}, \quad y_A = 1
\]

\textbf{Sample B:}
\[
\mathbf{x}_B = \begin{bmatrix} 10 \\ 10 \end{bmatrix}, \quad y_B = 1
\]

\noindent Assume both samples produce the same error $\delta = 0.5$ (same under-prediction). Is the following statement True or False? 

\noindent Since both samples have the same error, they will cause the same weight updates and contribute equally to learning. 


\vspace{8pt}
\noindent
\textbf{Solutions:}

\noindent The statement is \textbf{False}.

\noindent Sample B causes weight changes that are \textbf{100 times larger} than Sample A, even though both have identical errors.


\noindent In general, Gradient magnitude depends on input magnitude. In other words, weight updates scale with input feature values, not just the error.
Hence, feature scaling is critical. Features with large values dominate weight updates while features with small values barely influence learning. Therefore, without normalization, the model learns some features much faster than others. Similarly, large input values can cause large weight swings and it may lead to oscillating or divergent training. 
    



\end{document}
