\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{enumitem}

\begin{document}

\begin{center}
{\Large \textbf{In-Class Exercise (Canvas Quiz) --- 10 minutes}}\\[6pt]
{\large \textbf{Lecture 5: Introduction to Neural Networks (Single-Layer Perceptron)}}
\end{center}

\vspace{8pt}

% \noindent \textbf{Question 1}
% \noindent We use a single-layer perceptron with sigmoid activation:
% \[
% h = \mathbf{w}^T \mathbf{x} + b,
% \qquad
% \hat{y} = \sigma(h) = \frac{1}{1+\exp(-h)}
% \]
% Perceptron weight update rule:
% \[
% \mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} + \alpha \mathbf{x} \,\delta,
% \qquad
% b^{(t+1)} = b^{(t)} + \alpha \delta,
% \qquad
% \delta = y - \hat{y}
% \]

% \vspace{6pt}
% \noindent
% \textbf{Given:}
% \[
% \mathbf{w}^{(0)} = \begin{bmatrix} 0.50 \\ -0.25 \end{bmatrix},
% \qquad
% b^{(0)} = 0.10,
% \qquad
% \alpha = 0.20
% \]
% and the following training sample:
% \[
% \mathbf{x}= \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \qquad y = 1
% \]

% \vspace{8pt}


% \begin{enumerate}[label=(\arabic*), itemsep=6pt]
%     \item Compute the neuron input: \quad $h = \mathbf{w}^T \mathbf{x} + b$.
%     \item Compute the predicted output: \quad $\hat{y} = \sigma(h)$.
%     \item Compute the error: \quad $\delta = y - \hat{y}$.
%     \item Perform \textbf{one} update step and compute:
%     \[
%     \mathbf{w}^{(1)}, \quad b^{(1)}
%     \]
% \end{enumerate}

% \vspace{6pt}
% \noindent
% \textbf{Round all final answers to 2 decimals.}

\vspace{8pt}

\noindent \textbf{Question 1} 
\noindent Is the following statement True or False?  

\noindent \textit{``In an SLP, If $\mathbf{x} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}$ and both components of $\mathbf{w}$ are positive, then both weights will always increase during training.''}




\vspace{8pt}

\noindent \textbf{Question 2} 
\noindent Consider two training samples for a binary classifier:

\textbf{Sample A:}
\[
\mathbf{x}_A = \begin{bmatrix} 0.1 \\ 0.1 \end{bmatrix}, \quad y_A = 1
\]

\textbf{Sample B:}
\[
\mathbf{x}_B = \begin{bmatrix} 10 \\ 10 \end{bmatrix}, \quad y_B = 1
\]

\noindent Assume both samples produce the same error $\delta = 0.5$ (same under-prediction). Is the following statement True or False? 

\noindent Since both samples have the same error, they will cause the same weight updates and contribute equally to learning. 


\end{document}
