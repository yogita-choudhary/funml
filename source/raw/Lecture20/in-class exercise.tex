\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{enumitem}

\begin{document}

\begin{center}
{\Large \textbf{In-Class Exercise (Canvas Quiz) --- 10 minutes}}\\[6pt]
{\large \textbf{Lecture 20: Sequence Modeling (RNN + Causal Convolution)}}
\end{center}

\vspace{10pt}

\noindent
Throughout this exercise, use the RNN update equations:
\[
\mathbf{a}^{(t)}=\mathbf{b}+\mathbf{W}\mathbf{h}^{(t-1)}+\mathbf{U}\mathbf{x}^{(t)},
\qquad
\mathbf{h}^{(t)}=\tanh\!\left(\mathbf{a}^{(t)}\right).
\]

\vspace{8pt}

\begin{enumerate}[label=(\arabic*), itemsep=14pt]

\item \textbf{RNN memory (concept).}\\
Which statement best describes why an RNN can model temporal dependence?

\begin{enumerate}[label=(\Alph*), itemsep=3pt]
\item Each time step uses a different parameter set, so it learns a separate model per time.
\item The hidden state $\mathbf{h}^{(t)}$ summarizes past inputs and is reused at future time steps.
\item The model sees future inputs $\mathbf{x}^{(t+1)},\mathbf{x}^{(t+2)}$ when computing $\mathbf{h}^{(t)}$.
\item The nonlinearity $\tanh(\cdot)$ alone creates temporal memory even without $\mathbf{W}\mathbf{h}^{(t-1)}$.
\end{enumerate}

\item \textbf{Dependency tracing (concept).}\\
Assume $\mathbf{h}^{(0)}=\mathbf{0}$. For $t=2$, which inputs can influence $\mathbf{h}^{(2)}$?

\begin{enumerate}[label=(\Alph*), itemsep=3pt]
\item Only $\mathbf{x}^{(2)}$
\item Only $\mathbf{x}^{(1)}$
\item Both $\mathbf{x}^{(1)}$ and $\mathbf{x}^{(2)}$
\item Neither $\mathbf{x}^{(1)}$ nor $\mathbf{x}^{(2)}$
\end{enumerate}

\item \textbf{When does the RNN \emph{forget}? (concept).}\\
Consider the same update:
\[
\mathbf{a}^{(t)}=\mathbf{b}+\mathbf{W}\mathbf{h}^{(t-1)}+\mathbf{U}\mathbf{x}^{(t)}.
\]
Which condition most directly removes dependence on the past hidden state?

\begin{enumerate}[label=(\Alph*), itemsep=3pt]
\item $\mathbf{U}=\mathbf{0}$
\item $\mathbf{W}=\mathbf{0}$
\item $\mathbf{b}=\mathbf{0}$
\item Replacing $\tanh(\cdot)$ with $\mathrm{ReLU}(\cdot)$
\end{enumerate}

\item \textbf{Causal convolution (core idea).}\\
A 1D \emph{causal} convolution is designed so that the output at time $t$ depends only on present/past inputs (not future inputs).
For kernel length $k$, how many zeros must be padded on the \emph{left} to enforce causality?

\begin{enumerate}[label=(\Alph*), itemsep=3pt]
\item $k$
\item $k-1$
\item $k+1$
\item $2k-1$
\end{enumerate}

\item \textbf{Causal vs non-causal (concept check).}\\
Which statement is correct?

\begin{enumerate}[label=(\Alph*), itemsep=3pt]
\item A causal convolution can use future inputs, but only during training.
\item A non-causal convolution may use $\mathbf{x}^{(t+1)}$ to compute the output at time $t$.
\item Causality depends only on the activation function (e.g., $\tanh$ vs ReLU), not padding.
\item Causal convolutions require right-padding only.
\end{enumerate}

\end{enumerate}

\end{document}
