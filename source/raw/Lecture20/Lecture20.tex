%% Adapted from GaTech CS 7545 Machine Learning Theory given by Prof. Jacob Abernathy Fall 2018. Original latex file: https://github.com/mltheory/CS7545/blob/master/scribe/CS7545scribe_template.tex 



%%%%%PLEASE CONSIDER CORRECTIONS AT PLACES INDICATED%%%%%%%%
\documentclass{article}
%%%%%Packages Used, add more if necessary%%%%
\usepackage{amsmath,amsfonts,amssymb,graphicx,fullpage, float}
\setlength{\topmargin}{-0.6 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\usepackage{mdframed}
\usepackage{multicol}
\usepackage{microtype, hyperref}


%%%%% NO NEED TO EDIT THIS PREAMBLE %%%%%%%%
%%% PREAMBLE %%%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
      
      \hbox to 6.28in { 
      {\bf ECE 4252/6252 (FunML) Fundamentals of Machine Learning \hfill 2021--2028} }
      
      \vspace{2mm}
      \hbox to 6.28in { 
      {Courses developed by Professor Ghassan AlRegib 
      (\href{https://alregib.ece.gatech.edu/}{alregib.ece.gatech.edu}) 
      \hfill For inquiries: \href{mailto:alregib@gatech.edu}{alregib@gatech.edu}} }
      
      \vspace{3mm}
      \hbox to 6.28in { {\Large \hfill Lecture #1: #2 \hfill} }
      
      \vspace{2mm}
      \hbox to 6.28in { {\it Co-Instructors: #3 \hfill #4} }
      
      \vspace{2mm}}
   }
   \end{center}

   % -------- PEOPLE OUTSIDE BOX --------
   \vspace{2mm}
   \noindent{\bf Contributors:} Dr. Ahmad Mustafa, Dr. Motaz Alfarraj, Dr. Ashraf Alattar, Dr. Chen Zhou

   \vspace{1mm}
   \noindent{\bf Teaching Assistants} with remarkable contributions include: Kuo-Wei Lai, Wuyang Du, Shiva Mahato, Michael Zhou, Ninghan Zhong

   \markboth{Lecture #1: #2}{Lecture #1: #2}

   \vspace{2mm}
\noindent {\bf Disclaimer}: 
{All content of these notes are part of this course at Georgia Tech. Any re-use or distribution is not permitted without pre-approved permission. All these notes belong to, created by, and copyrighted for Ghassan AlRegib and Mohit Prabhushankar, Georgia Tech, 2021--2028.}

\vspace{2mm}
\noindent {\bf License}: 
{These lecture notes are licensed under the 
\href{https://creativecommons.org/licenses/by-nc-sa/4.0/}{Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License}.}

   \vspace{2mm}
   \noindent {\bf Errata}: 
   {\it Please submit any errata you find using the following form: 
   \href{https://forms.office.com/r/fbg9dMWPgY}{Errata Form for FunML Textbook} 
   or visit: \url{https://forms.office.com/r/fbg9dMWPgY}}

   \vspace*{4mm}
}
\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

\newcommand{\challenge}[2]{\noindent \textbf{(Challenge Problem)} \emph{#1}: #2 }

\newcommand{\exercise}[1]{\noindent \textbf{(Exercise)} #1 }

\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
%%% END_OF_PREAMBLE %%%


	
%%%%You may add more \newtheorem if necessary%%%%%%
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}


\newcommand{\dom}{\mathrm{dom}}
\begin{document}

%%%%%CHANGE HERE%%%%%%%
%%%%%\section{title of the section} similarly with the rest \section{} or \subsection{} or \subsubsection{} etc


%%%%%CHANGE HERE%%%%%%%%%
%%%%%\lecture{the ordinal number of the lecture}{lecture title}{Jacob Abernethy}{scriber's name}%%%%%%%%
\lecture{20}{Sequence Modeling}{Ghassan AlRegib and Mohit Prabhushankar}{}

%%%%%CHANGE HERE%%%%%%%
%%%%%\section{title of the section} similarly with the rest \section{} or \subsection{} or \subsubsection{} etc
\section{Learning Objectives}
This lecture provides an introduction to sequence modeling, which is necessary for tasks involving sequential data like time-series analysis and language processing. We will cover the limitations of traditional machine learning models, such as Multilayer Perceptrons (MLPs), which are not designed to handle dependencies over time. In order to address this, we introduce Recurrent Neural Networks (RNNs), which is much better at capturing sequential patterns. We will also explore the challenges in training RNNs, specfically the vanishing gradient problem. Finally, we will consider Temporal Convolutional Networks (TCNs) as an alternative to RNNs, due to their capability to model long-range dependencies in sequences.

\section{Introduction to Sequence Modeling}

\subsection*{Limitations of MLPs in Modeling Sequences}
\begin{itemize}
    \item \textbf{Fixed-Length Vector Input and Output:} Multilayer Perceptrons (MLPs) expect fixed-size inputs, which is limiting for variable-length sequences.
    \item \textbf{Lack of Temporal Awareness:} MLPs do not account for the ordering of inputs, meaning they fail to capture dependencies in time-series data, which is necessary for things like language modeling, stock prediction, and sensor data analysis.
    \item \textbf{Overfitting in Long Sequences:} For longer sequences, MLPs would require a very large number of weights, which often leads to overfitting when data is limited.
\end{itemize}

\subsection*{Types of Sequence Modeling Problems}
\begin{itemize}
    \item \textbf{Sequence Prediction and Classification:} Involves predicting a single output (e.g., a label or real number) from a sequence of inputs. Common examples include sentiment analysis (classifying the sentiment of a text) and stock price prediction.
    \item \textbf{Sequence Generation:} Generates an output sequence based on learned patterns, such as generating music or text. This requires capturing characteristics of sequences within the dataset.
    \item \textbf{Sequence-to-Sequence Prediction:} Maps an input sequence to an output sequence, essential for tasks like machine translation or video captioning, where a response sequence of arbitrary length needs to be produced based on an input sequence.
\end{itemize}

\clearpage
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{img/lecture20/h_graph.png}
    \caption{Parameter sharing with state history in sequence models}
    \label{fig:figure_label1}
\end{figure}

\subsection*{Variables in Figure 20.2}
\begin{itemize}
    \item \(\mathbf{x}(t)\): The input vector at time \(t\), providing new information to the network at each time step.
    \item \(\mathbf{h}(t)\): The hidden state vector at time \(t\), which captures the accumulated information from all previous inputs up to time \(t\). It serves as the memory of the network.
    \item \(\mathbf{o}(t)\): The output vector at time \(t\), generated based on the current hidden state \(\mathbf{h}(t)\).
    \item \(\mathbf{U}\): The weight matrix that maps the input \(\mathbf{x}(t)\) to the hidden state.
    \item \(\mathbf{W}\): The weight matrix that connects the hidden state at the previous time step, \(\mathbf{h}(t-1)\), to the hidden state at the current time, \(\mathbf{h}(t)\).
    \item \(\mathbf{V}\): The weight matrix that maps the hidden state \(\mathbf{h}(t)\) to the output \(\mathbf{o}(t)\).
\end{itemize}

\section{Recurrent Neural Networks (RNNs)}
Recurrent Neural Networks (RNNs) are a type of neural network designed for sequential data, where the order of inputs is significant. Figure 20.2 details the structure of an RNN in more depth. Unlike traditional neural networks, RNNs have a hidden state that serves as memory, allowing them to retain information across time steps. At each step, an RNN updates this hidden state based on both the current input and the previous hidden state, allowing it to have temporal dependencies. This makes RNNs ideal for tasks like language modeling, time-series forecasting, and any application where context from prior inputs influences future predictions.

\subsection{RNN Architecture}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{img/lecture20/bigH.png}
    \caption{The structure of a Recurrent Neural Network (RNN) and how the states and inputs are processed over time}
    \label{fig:figure_label2}
\end{figure}
\begin{itemize}
    \item \textbf{State Representation:} The core idea behind RNNs is that each output depends not only on the current input but also on a ``state" that captures past inputs. This allows RNNs to model temporal dependencies.
    \item \textbf{Parameter Sharing:} RNNs share parameters across time steps, allowing the model to handle varying input lengths and avoid the need for an excessive number of parameters.
    \item \textbf{State Update Equation:}
    \[
    \mathbf{h}(t) = f(\mathbf{h}(t-1), \mathbf{x}(t))
    \]
    where \( \mathbf{h}(t) \) is the hidden state at time \( t \), and \( f \) is typically a non-linear function like \(\tanh\) or \(\text{ReLU}\).
    \item \textbf{RNN Update Equations:}
    \begin{align*}
        \mathbf{a}(t) &= \mathbf{b} + W\mathbf{h}(t-1) + U\mathbf{x}(t) \quad (\text{Compute activations}) \\
        \mathbf{h}(t) &= \tanh(\mathbf{a}(t)) \quad (\text{Apply non-linear activation}) \\
        \mathbf{o}(t) &= \mathbf{c} + V\mathbf{h}(t) \quad (\text{Output calculation}) \\
        \mathbf{y}(t) &= \text{softmax}(\mathbf{o}(t)) \quad (\text{Convert to probabilities})
    \end{align*}
    \item \textbf{Interpretation:} These equations show how RNNs transform the input at each time step by maintaining a state, updating it with new information while preserving historical context.
\end{itemize}

\subsection{Loss Evaluation and Backpropagation}

In Recurrent Neural Networks (RNNs), loss evaluation is performed by calculating the discrepancy between the predicted output and the target output at each time step. The total loss across the sequence is typically the sum of individual losses at each time step, represented as:

\[
\text{Loss} = \sum_{t} L(t)
\]

where \( L(t) \) is the loss at time step \( t \). Common loss functions used include Mean Squared Error (MSE) for regression tasks and Cross-Entropy Loss for classification tasks.

\subsection*{Backpropagation Through Time (BPTT)}

To minimize this loss, RNNs are trained using a process called Backpropagation Through Time (BPTT), an extension of standard backpropagation adapted for sequential data. BPTT calculates gradients by unrolling the RNN over the sequence and then performing backpropagation across each time step. This allows the model to adjust weights by considering the cumulative impact of errors over multiple steps.

\subsection*{Gradient Calculation}

During BPTT, gradients of the loss with respect to the weights (e.g., \( \mathbf{U}, \mathbf{W}, \mathbf{V} \)) are calculated iteratively, considering dependencies across time. For a given weight matrix \( \mathbf{W} \), the gradient at time \( t \) is influenced by contributions from both the current and previous states, often resulting in a long chain of gradient products:

\[
\frac{\partial L}{\partial \mathbf{W}} = \sum_{t} \frac{\partial L(t)}{\partial \mathbf{W}}
\]

However, this chain can be computationally intensive and is prone to issues such as the vanishing and exploding gradient problems, where gradients either diminish or grow exponentially, impacting training stability.

\subsection*{Challenges and Solutions}

BPTT can cause gradients to either vanish or explode, especially in long sequences. This is due to repeated multiplication by weights at each time step, which may compress gradients to near zero or inflate them to very large values. Techniques like gradient clipping and advanced RNN architectures like Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs) are often used to mitigate these issues by managing the flow of gradients more effectively.

In summary, loss evaluation and BPTT are crucial components of training RNNs, enabling them to learn temporal dependencies by adjusting weights based on the cumulative error across sequences.

\subsection{The Vanishing Gradient Problem}

In training Recurrent Neural Networks (RNNs), the vanishing gradient problem is a common challenge, particularly when backpropagating over many time steps. This issue arises due to the nature of gradient propagation in recurrent structures, where repeated multiplication causes gradients to shrink exponentially.

\subsection*{Gradient Computation in RNNs}

The hidden state update for an RNN is given by:
\[
\mathbf{h}(t) = \sigma\left(\mathbf{b} + \mathbf{W} \mathbf{h}(t-1) + \mathbf{U} \mathbf{x}(t)\right)
\]
where \( \mathbf{h}(t) \) is the hidden state at time \( t \), \( \mathbf{b} \) is a bias vector, \( \mathbf{W} \) is the hidden-to-hidden weight matrix, \( \mathbf{U} \) maps the input \( \mathbf{x}(t) \), and \( \sigma \) is a non-linear activation function.

\subsection*{Gradient Backpropagation Through Time (BPTT)}

To compute the gradient of the loss \( L \) with respect to the hidden state \( \mathbf{h}(t) \), we must propagate through each time step. The gradient at time \( t \) with respect to an earlier time step \( 0 \) is given by:
\[
\frac{\partial \mathbf{h}(t)}{\partial \mathbf{h}(0)} = \frac{\partial \mathbf{h}(t)}{\partial \mathbf{h}(t)} \prod_{k=1}^{t} \frac{\partial \mathbf{h}(k)}{\partial \mathbf{h}(k-1)}
\]
where each term in the product represents the influence of one time step on the next. 

From the RNN update equation, we can express the partial derivative as:
\[
\frac{\partial \mathbf{h}(t)}{\partial \mathbf{h}(t-1)} = \mathbf{J}(t) \mathbf{W}^T
\]
where \( \mathbf{J}(t) \) is the Jacobian of \( \sigma \), the activation function applied to \( \mathbf{a}(t) = \mathbf{b} + \mathbf{W} \mathbf{h}(t-1) + \mathbf{U} \mathbf{x}(t) \).

When this expression is expanded over multiple time steps, it forms a long product of Jacobians and weight matrices:
\[
\frac{\partial \mathbf{h}(t)}{\partial \mathbf{h}(0)} = \mathbf{J}(t) \mathbf{W}^T \mathbf{J}(t-1) \mathbf{W}^T \cdots \mathbf{J}(1) \mathbf{W}^T
\]

\subsection*{Why Gradients Vanish}

As the sequence length \( t \) increases, each matrix product involving the Jacobian and weight matrix can cause the gradients to diminish. For small eigenvalues of \( \mathbf{W} \), each multiplication further reduces the gradient, resulting in a vanishing effect:
\[
\frac{\partial \mathbf{h}(t)}{\partial \mathbf{h}(0)} \approx 0 \quad \text{for large } t
\]

\subsection*{Additional Techniques to Mitigate the Vanishing Gradient Problem}

In addition to advanced architectures and gradient clipping, the following techniques can also help mitigate the vanishing gradient problem:

\begin{itemize}
    \item \textbf{ReLU Activation Functions:} Unlike sigmoid and tanh activations, which tend to saturate, ReLU (Rectified Linear Unit) activations do not have an upper bound and thus are less likely to cause vanishing gradients. By using ReLU or similar non-saturating functions, the network can retain larger gradients, making it easier to propagate information through many time steps.

    \item \textbf{Identity Matrix Initialization:} Initializing the RNN weight matrix \( \mathbf{W} \) as an identity matrix, instead of small random values, can help maintain the gradient flow over time. This approach stabilizes the gradient magnitude, reducing the likelihood of vanishing gradients, especially in long sequences.
\end{itemize}

These solutions enable RNNs to capture longer dependencies by preserving gradients across multiple time steps.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{img/lecture20/Gradient.png}
    \caption{Effects of initializing weights from a standard normal distribution versus identity matrix initialization}
    \label{fig:figure_label3}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{img/lecture20/ReLU.png}
    \caption{Activation functions (sigmoid, tanh, ReLU) across range of inputs}
    \label{fig:figure_label4}
\end{figure}

\section{Long Short-Term Memory (LSTM) Networks}

Long Short-Term Memory (LSTM) networks are a type of Recurrent Neural Network (RNN) specifically designed to address the vanishing gradient problem, enabling the model to learn long-term dependencies. LSTMs achieve this by introducing a unique cell structure that includes gating mechanisms, which regulate the flow of information and maintain gradients over time. Figure 20.5 details the structure of an individual LSTM cell.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{img/lecture20/LSTM.png}
    \caption{An LSTM cell for one time step}
    \label{fig:figure_label5}
\end{figure}

\subsection{LSTM Architecture and Gates}

The LSTM cell includes several key components that allow it to control the retention and update of information across time steps. Each LSTM cell consists of the following gates:

\begin{itemize}
    \item \textbf{Forget Gate} \( f^{(t)} \): This gate determines how much of the previous cell state should be retained or ``forgotten" for the current time step. It is calculated as:
    \[
    \mathbf{f}^{(t)} = \sigma(\mathbf{b}^f + \mathbf{U}^f \mathbf{x}^{(t)} + \mathbf{W}^f \mathbf{h}^{(t-1)})
    \]
    where \( \sigma \) is the sigmoid activation function, \( \mathbf{b}^f \) is the bias term, \( \mathbf{U}^f \) maps the input \( \mathbf{x}^{(t)} \), and \( \mathbf{W}^f \) maps the previous hidden state \( \mathbf{h}^{(t-1)} \) to the forget gate.

    \item \textbf{Input Gate} \( g^{(t)} \): This gate controls the extent to which new information (from the current input \( \mathbf{x}^{(t)} \)) should be added to the cell state. It is defined as:
    \[
    \mathbf{g}^{(t)} = \sigma(\mathbf{b}^g + \mathbf{U}^g \mathbf{x}^{(t)} + \mathbf{W}^g \mathbf{h}^{(t-1)})
    \]

    \item \textbf{Output Gate} \( \mathbf{o}^{(t)} \): This gate determines which parts of the cell state will contribute to the hidden state \( \mathbf{h}^{(t)} \), influencing the output for this time step. It is computed as:
    \[
    \mathbf{q}^{(t)} = \sigma(\mathbf{b}^o + \mathbf{U}^o \mathbf{x}^{(t)} + \mathbf{W}^o \mathbf{h}^{(t-1)})
    \]
\end{itemize}

\subsection{Updating the Cell and Hidden States}

With the gates calculated, the cell state \( \mathbf{s}^{(t)} \) and hidden state \( \mathbf{h}^{(t)} \) are updated as follows:

\begin{itemize}

    \item \textbf{Hidden State Update}: The hidden state \( \mathbf{h}(t) \) is derived from the updated cell state, modulated by the output gate \( o(t) \):
    \[
    \mathbf{h}^{(t)} = \tanh(\mathbf{s}^{(t)}) \circ \mathbf{q}^{(t)}
    \]
    
    \item \textbf{Cell State Update}: The cell state \( \mathbf{s}^{(t)} \) is updated by combining the previous cell state \( \mathbf{s}^{(t-1)} \), modulated by the forget gate \( \mathbf{f}^{(t)} \), with the new candidate scaled by the input gate \( \mathbf{g}^{(t)} \):
    \[
    \mathbf{h}^{(t)} = \mathbf{f}^{(t)} \circ \mathbf{s}^{(t-1)} + \mathbf{g}^{(t)} \circ \mathbf{\sigma}(\mathbf{b}^o + \mathbf{U}^o \mathbf{x}^{(t)} + \mathbf{W}^o \mathbf{h}^{(t-1)})
    \]
    where \( \circ \) denotes element-wise multiplication.
\end{itemize}

\subsection{Advantages of LSTMs}

LSTMs are effective for handling long sequences due to their ability to retain information over extended time steps. The gating mechanism allows LSTMs to ``remember" or ``forget" information as needed, enabling them to capture long-term dependencies without suffering from the vanishing gradient problem. This architecture has made LSTMs highly successful in tasks such as natural language processing, speech recognition, and time-series forecasting.

In summary, LSTM networks extend the capabilities of standard RNNs by providing a more robust memory structure, allowing for the learning of complex, long-term relationships in sequential data.

\section{Gated Recurrent Units (GRUs)}

Gated Recurrent Units (GRUs) are a type of recurrent neural network designed as a simpler alternative to Long Short-Term Memory (LSTM) networks. GRUs are effective for capturing dependencies in sequential data while using fewer parameters than LSTMs, making them computationally efficient. GRUs achieve this by combining the forget and input gates of an LSTM into a single update gate and removing the cell state, relying only on the hidden state.

\subsection{GRU Architecture and Gates}

Each GRU cell consists of two primary gates that control the flow of information:

\begin{itemize}
    \item \textbf{Update Gate} \( \mathbf{z}^{(t)} \): This gate determines how much of the previous hidden state \( \mathbf{h}^{(t-1)} \) should be retained in the current hidden state \( \mathbf{h}^{(t)} \). It is calculated as:
    \[
    \mathbf{u}^{(t)} = \sigma(\mathbf{b}^u + \mathbf{U}^u \mathbf{x}^{(t)} + \mathbf{W}^u \mathbf{h}^{(t-1)})
    \]
    where \( \sigma \) is the sigmoid activation function, \( \mathbf{b}^{(z)} \) is a bias term, \( \mathbf{U}^{(z)} \) maps the input \( \mathbf{x}^{(t)} \), and \( \mathbf{W}^{(z)} \) maps the previous hidden state \( \mathbf{h}^{(t-1)} \).

    \item \textbf{Reset Gate} \( \mathbf{r}^{(t)} \): This gate determines how much of the previous hidden state \( \mathbf{h}^{(t-1)} \) should be ``forgotten" when calculating the candidate hidden state. It is defined as:
    \[
    \mathbf{r}^{(t)} = \sigma(\mathbf{b}^r + \mathbf{U}^r \mathbf{x}^{(t)} + \mathbf{W}^r \mathbf{h}^{(t-1)})
    \]
\end{itemize}

\subsection{Updating the Hidden State}

With the gates calculated, the GRU updates its hidden state \( \mathbf{h}^{(t)} \) as follows:

\begin{itemize}
    \item \textbf{Hidden State Update}: The final hidden state \( \mathbf{h}^{(t)} \) is a combination of the previous hidden state \( \mathbf{h}^{(t-1)} \) and the hidden state, modulated by the update gate \( \mathbf{u}^{(t)} \):
    \[
    \mathbf{h}^{(t)} = \mathbf{u}^{(t)} \circ \mathbf{h}^{(t-1)} + (1 - \mathbf{u}^{(t)}) \circ \mathbf{\sigma}(\mathbf{b} + \mathbf{U} \mathbf{x}^{(t)} + \mathbf{W} \mathbf{h}^{(t-1)})
    \]
    where \( \circ \) denotes element-wise multiplication.
\end{itemize}

\subsection{Advantages of GRUs}

GRUs are computationally efficient due to their simpler architecture, making them faster to train and less prone to overfitting than LSTMs. The design of GRUs, with fewer gates and no cell state, allows them to capture long-term dependencies in sequences without the complexity of LSTMs, making GRUs suitable for tasks where simpler models can effectively capture temporal dependencies.

In summary, GRUs provide a streamlined approach to sequence modeling, allowing efficient learning of sequential patterns with reduced computational costs.

\subsection{Parameter Sharing in Sequence Modeling}

In sequence modeling, parameter sharing allows a model to scan through sequences of varying lengths to detect patterns or features without increasing the parameter count. This concept is commonly implemented in Convolutional Neural Networks (CNNs), where convolutional kernels, or filters, are applied to different portions of an input sequence.

A convolutional kernel can be viewed as a learned function applied across multiple positions in a sequence, looking for specific features. For example, a kernel can detect edges in an image or phrases in text by sliding across the input. This method results in:
\begin{itemize}
    \item \textbf{Spatial Invariance:} The model can recognize features regardless of their position in the sequence.
    \item \textbf{Efficiency for Variable Lengths:} By applying the same kernel across an entire sequence, the model achieves efficient parameterization, independent of sequence length.
\end{itemize}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{img/lecture20/RNN_Config.png}
    \caption{Various configurations for RNNs and other associated models. Blue boxes denote outputs, red boxes the inputs, and green boxes the hidden states}
    \label{fig:figure_label6}
\end{figure}

In textual data, for instance, the same convolutional kernel can identify relevant features (such as certain phrases) across different sentences regardless of the word order or sentence length.

\section{Temporal Convolutional Networks (TCNs)}

Temporal Convolutional Networks (TCNs) are a convolution-based architecture adapted to sequence modeling tasks, offering an alternative to Recurrent Neural Networks (RNNs) for learning temporal dependencies. By using convolutional layers, TCNs process the entire input sequence in parallel, enabling faster computation and sidestepping many limitations of RNNs, such as the vanishing gradient problem and limited receptive field.

\subsection{Motivation for TCNs}
RNNs and related models face challenges such as gradient instability over long sequences, high memory requirements, and computational bottlenecks due to sequential processing. TCNs address these issues by using one-dimensional (1D) convolutions, allowing them to process sequences in a more flexible and parallelizable way.

\subsection{Computation}
In TCNs, computation is based on one-dimensional (1D) convolutional layers applied across time. Each convolutional layer processes all time steps in parallel, making TCNs computationally efficient. The network consists of multiple stacked convolutional layers (Figure 20.7), where each layer has a growing receptive field that enables it to capture dependencies at increasing time steps. The output at each layer is a function of inputs from previous layers, achieving hierarchical feature extraction without requiring recurrent connections.

\subsection{Core Components of TCNs}

TCNs incorporate several key features to adapt convolutional networks for sequence modeling, including causal convolutions, dilated convolutions, and residual connections.

\subsection*{Causal Convolutions}
Causal convolutions ensure that each output at time \( t \) is derived only from current and past inputs, respecting the temporal order of data. This is achieved by padding the input so that convolutional layers do not access future data. For a kernel of length \( k \), the input sequence is padded with \( k-1 \) zeros on one side, producing outputs as:
\[
\mathbf{y}^{(t)} = f\left(\mathbf{x}^{(t)}, \mathbf{x}^{(t-1)}, \dots, \mathbf{x}^{(t-k+1)}\right)
\]
This constraint makes TCNs suitable for sequence prediction tasks where the model cannot access future data during training.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{img/lecture20/cc1.png}
    \caption{First step in causal convolution with kernel of length k}
    \label{fig:figure_label7}
\end{figure}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{img/lecture20/cc2.png}
    \caption{Next step in the convolution}
    \label{fig:figure_label8}
\end{figure}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{img/lecture20/cc3.png}
    \caption{Adjusted final sequence by removing the last k-1 samples}
    \label{fig:figure_label9}
\end{figure}

\clearpage
\subsection*{Dilated Convolutions}
To extend the receptive field without significantly increasing the network depth, TCNs utilize dilated convolutions. A dilated convolution introduces a dilation factor \( d \), spacing the elements in the convolutional filter, allowing TCNs to capture long-range dependencies, shown by Figure 20.13. The dilated convolution at time \( t \) with kernel size \( k \) and dilation \( d \) is defined as:
\[
\mathbf{y}^{(t)} = f\left(\mathbf{x}^{(t)}, \mathbf{x}^{(t-d)}, \dots, \mathbf{x}^{(t-d(k-1))}\right)
\]

The dilation factor typically grows exponentially with depth, such as \( d = 1, 2, 4, \dots \), expanding the receptive field while maintaining computational efficiency.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{img/lecture20/d4.png}
    \caption{Stacking multiple layers}
    \label{fig:figure_label10}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{img/lecture20/d1.png}
    \caption{Stacking multiple layers}
    \label{fig:figure_label11}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{img/lecture20/d2.png}
    \caption{Multiresolution kernels}
    \label{fig:figure_label12}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{img/lecture20/d3.png}
    \caption{Dilated Convolutions}
    \label{fig:figure_label13}
\end{figure}

\clearpage
\subsection{Computational Complexity}
One of the major advantages of TCNs over RNNs is their computational complexity. By leveraging convolutions, TCNs process all time steps in parallel, reducing training and inference time. The computational complexity per layer in a TCN depends primarily on the number of filters and the kernel size, rather than the sequence length, making TCNs highly scalable for long sequences which is shown by Figure 20.15 with the effect of multiresolution kernels. Figure 20.14 demonstrates how the concept of stacked dilated convolutions allows each layer to capture broader dependencies within the sequence by skipping elements based on dilation factor.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{img/lecture20/c1.png}
    \caption{Skipping elements based on dilation factor in a stacked dilated convolution}
    \label{fig:figure_label14}
\end{figure}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{img/lecture20/c2.png}
    \caption{Multiresolution kernels with convolutions of various kernel sizes and dilation factors}
    \label{fig:figure_label15}
\end{figure}

\subsection{Advantages of TCNs}

Temporal Convolutional Networks (TCNs) offer several distinct advantages over traditional sequence models, such as RNNs and LSTMs, due to their unique architectural design:

\begin{itemize}
    \item \textbf{Input Sequence Invariability}: TCNs can handle input sequences of varying lengths and produce outputs of the same length as the input sequence. This characteristic makes them comparable to RNNs in terms of sequence handling while retaining the efficiency of convolutional architectures.

    \item \textbf{Causal Sequence Modeling}: Each output in a TCN depends only on the current and past inputs, ensuring that the model respects the temporal order. This is achieved through specific settings for dilation and padding, making TCNs suitable for real-time and predictive tasks where future data cannot influence past outputs.

    \item \textbf{Residual Units for Optimized Learning}: TCNs use residual units similar to those in 2D CNNs, allowing for a deeper network without gradient issues. These residual connections simplify the learning process, making it easier to optimize deep TCNs while maintaining stable gradients.

    \item \textbf{Optimal Receptive Field Design}: Through the use of dilated causal convolutions across multiple layers, TCNs achieve a receptive field that covers the entire range of past inputs necessary for accurate predictions. This design enables TCNs to capture long-range dependencies more effectively than RNNs.

\end{itemize}

\subsection{Residual Units}
Residual units in TCNs add a direct path from the input to the output of each layer, which allows the network to learn residual mappings and aids in the backpropagation of gradients. A residual block is defined as:
\[
\mathbf{h}^{(t)} = \text{ReLU}\left(W_2 \cdot \text{ReLU}(W_1 \cdot \mathbf{x}^{(t)} + b_1) + b_2\right) + \mathbf{x}^{(t)}
\]
The residual connection helps prevent gradient degradation in deeper networks, enabling TCNs to train efficiently even with many layers. Figure 20.16 shows the structure of a single residual block in a TCN.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{img/lecture20/r.png}
    \caption{Structure of a single residual block in a TCN}
    \label{fig:figure_label16}
\end{figure}

\subsection{Weight Normalization}
Weight normalization is often applied in TCNs to stabilize training and control gradient magnitudes. Weight normalization reparameterizes the weights \( \mathbf{w} \) as:
\[
\mathbf{w} = \frac{\mathbf{g}\mathbf{v}}{\|\mathbf{v}\|}
\]
where \( \mathbf{g} \) is a learnable scaling factor, and \( \mathbf{v} \) is the original weight vector. Dividing \( \mathbf{v} \) by \( ||\mathbf{v}|| \) gives the direction of the weight vector. This normalization technique helps maintain consistent gradients across layers, leading to improved convergence and training stability.

\section{Appendix: Notations}
\begin{itemize}
    \item \( x_i \): Single feature
    \item \( \mathbf{x}_i \): Feature vector (data sample)
    \item \( \mathbf{X} \): Matrix of feature vectors (dataset)
    \item \( W \): Weight matrix
    \item \( P \): Number of features in a feature vector
    \item \( \alpha \): Learning rate
\end{itemize}

\end{document}