\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{enumitem}

\begin{document}

\begin{center}
{\Large \textbf{In-Class Exercise --- Questions + Solutions}}\\[6pt]
{\large \textbf{Lecture 20: Sequence Modeling (RNN + Causal Convolution)}}
\end{center}

\vspace{10pt}

\noindent
We use the RNN update equations:
\[
\mathbf{a}^{(t)}=\mathbf{b}+\mathbf{W}\mathbf{h}^{(t-1)}+\mathbf{U}\mathbf{x}^{(t)},
\qquad
\mathbf{h}^{(t)}=\tanh\!\left(\mathbf{a}^{(t)}\right).
\]

\vspace{8pt}


\noindent
\textbf{Question 1 (RNN memory --- concept).}\\
Which statement best describes why an RNN can model temporal dependence?

\begin{enumerate}[label=(\Alph*), itemsep=3pt]
\item Each time step uses a different parameter set, so it learns a separate model per time.
\item The hidden state $\mathbf{h}^{(t)}$ summarizes past inputs and is reused at future time steps.
\item The model sees future inputs $\mathbf{x}^{(t+1)},\mathbf{x}^{(t+2)}$ when computing $\mathbf{h}^{(t)}$.
\item The nonlinearity $\tanh(\cdot)$ alone creates temporal memory even without $\mathbf{W}\mathbf{h}^{(t-1)}$.
\end{enumerate}

\vspace{4pt}
\noindent\textbf{Solution 1:}\\
Temporal dependence comes from feeding the previous hidden state into the current update via $\mathbf{W}\mathbf{h}^{(t-1)}$. This makes $\mathbf{h}^{(t)}$ a running summary of the past.
\[
\boxed{\textbf{Answer: (B)}}
\]

\vspace{12pt}


\noindent
\textbf{Question 2 (Dependency tracing --- concept).}\\
Assume $\mathbf{h}^{(0)}=\mathbf{0}$. For $t=2$, which inputs can influence $\mathbf{h}^{(2)}$?

\begin{enumerate}[label=(\Alph*), itemsep=3pt]
\item Only $\mathbf{x}^{(2)}$
\item Only $\mathbf{x}^{(1)}$
\item Both $\mathbf{x}^{(1)}$ and $\mathbf{x}^{(2)}$
\item Neither $\mathbf{x}^{(1)}$ nor $\mathbf{x}^{(2)}$
\end{enumerate}

\vspace{4pt}
\noindent\textbf{Solution 2:}\\
$\mathbf{h}^{(2)}$ depends on $\mathbf{h}^{(1)}$ and $\mathbf{x}^{(2)}$. But $\mathbf{h}^{(1)}$ depends on $\mathbf{x}^{(1)}$. Therefore both inputs can influence $\mathbf{h}^{(2)}$.
\[
\boxed{\textbf{Answer: (C)}}
\]

\vspace{12pt}


\noindent
\textbf{Question 3 (When does the RNN forget? --- concept).}\\
Which condition most directly removes dependence on the past hidden state?

\begin{enumerate}[label=(\Alph*), itemsep=3pt]
\item $\mathbf{U}=\mathbf{0}$
\item $\mathbf{W}=\mathbf{0}$
\item $\mathbf{b}=\mathbf{0}$
\item Replacing $\tanh(\cdot)$ with $\mathrm{ReLU}(\cdot)$
\end{enumerate}

\vspace{4pt}
\noindent\textbf{Solution 3:}\\
The only term that carries information from the past is $\mathbf{W}\mathbf{h}^{(t-1)}$. Setting $\mathbf{W}=\mathbf{0}$ removes that pathway, so the state no longer depends on earlier time steps.
\[
\boxed{\textbf{Answer: (B)}}
\]

\vspace{12pt}


\noindent
\textbf{Question 4 (Causal convolution --- core idea).}\\
For kernel length $k$, how many zeros must be padded on the \emph{left} to enforce causality?

\begin{enumerate}[label=(\Alph*), itemsep=3pt]
\item $k$
\item $k-1$
\item $k+1$
\item $2k-1$
\end{enumerate}

\vspace{4pt}
\noindent\textbf{Solution 4:}\\
A causal 1D convolution with kernel length $k$ must not access future inputs. Left-padding by $k-1$ ensures the output at time $t$ can depend on $\mathbf{x}^{(t)},\mathbf{x}^{(t-1)},\ldots,\mathbf{x}^{(t-k+1)}$ only.
\[
\boxed{\textbf{Answer: (B)}}
\]

\vspace{12pt}


\noindent
\textbf{Question 5 (Causal vs non-causal --- concept check).}\\
Which statement is correct?

\begin{enumerate}[label=(\Alph*), itemsep=3pt]
\item A causal convolution can use future inputs, but only during training.
\item A non-causal convolution may use $\mathbf{x}^{(t+1)}$ to compute the output at time $t$.
\item Causality depends only on the activation function (e.g., $\tanh$ vs ReLU), not padding.
\item Causal convolutions require right-padding only.
\end{enumerate}

\vspace{4pt}
\noindent\textbf{Solution 5:}\\
Non-causal convolutions (e.g., centered kernels) can incorporate future context; causal convolutions explicitly forbid that by design (via padding/alignment).
\[
\boxed{\textbf{Answer: (B)}}
\]

\end{document}
