\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{enumitem}

% Vector / matrix formatting
\newcommand{\vecb}[1]{\boldsymbol{#1}}
\newcommand{\matb}[1]{\boldsymbol{#1}}
\newcommand{\R}{\mathbb{R}}

\begin{document}

\begin{center}
{\Large \textbf{In-Class Exercise --- Questions + Solutions}}\\[6pt]
{\large \textbf{Lecture 26: Self-Supervised Learning}}
\end{center}

\vspace{10pt}

\begin{enumerate}[label=(\arabic*), itemsep=12pt]

\item \textbf{Core SSL idea (pseudo-labels).} \\
A pre-text task $P(\cdot)$ transforms an unlabeled input $\vecb{x}$ into a pseudo-label.
Which option best describes \emph{why} pseudo-labels are useful in SSL?
\begin{enumerate}[label=(\alph*), itemsep=2pt]
    \item They replace ground-truth labels and guarantee perfect downstream accuracy.
    \item They create a training signal that encourages learning features that transfer to downstream tasks.
    \item They are only used to compress data and are not used for representation learning.
    \item They remove the need for augmentations in contrastive learning.
\end{enumerate}

\textbf{Solution.} \boxed{\textbf{(b)}} \\
Pseudo-labels are not ``true'' labels; their purpose is to define a learnable objective that forces the network to extract structure from unlabeled data in a way that can transfer to downstream tasks.

\item \textbf{Representation vs embedding (SimCLR design choice).} \\
SimCLR uses $\vecb{h}=f_\theta(\vecb{x})$ and then $\vecb{z}=g_\phi(\vecb{h})$ for the contrastive loss.
Why is it beneficial to apply the contrastive objective on $\vecb{z}$ rather than directly on $\vecb{h}$?
\begin{enumerate}[label=(\alph*), itemsep=2pt]
    \item It prevents the encoder from learning any transferable information.
    \item It lets the encoder representation $\vecb{h}$ remain useful for downstream tasks while $\vecb{z}$ specializes for the contrastive objective.
    \item It is required because cosine similarity cannot be computed on $\vecb{h}$.
    \item It makes positive pairs become negatives during training.
\end{enumerate}

\textbf{Solution.} \boxed{\textbf{(b)}} \\
The projection head $g_\phi$ can absorb task-specific distortions needed to optimize contrastive separation, while the encoder output $\vecb{h}$ is kept as a more general representation for downstream use.

\item \textbf{What the loss is pushing (conceptual geometry).} \\
For an anchor view $i$, SimCLR treats its paired view $j$ as positive and all other views as negatives.
Which statement best matches the intended effect of the contrastive loss on the embeddings?
\begin{enumerate}[label=(\alph*), itemsep=2pt]
    \item Increase $\mathrm{sim}(\vecb{z}_i,\vecb{z}_j)$ and decrease $\mathrm{sim}(\vecb{z}_i,\vecb{z}_k)$ for negatives $k$.
    \item Decrease all similarities so embeddings become orthogonal to everything.
    \item Increase similarity to all views so all embeddings collapse to the same point.
    \item Maximize the norm $\|\vecb{z}_i\|$ while ignoring pairwise similarities.
\end{enumerate}

\textbf{Solution.} \boxed{\textbf{(a)}} \\
The core contrastive goal is \emph{relative}: pull positives closer while pushing negatives away, so the embedding space separates instances by identity under augmentation.

\item \textbf{Negatives and batch effects (reasoning).} \\
Consider a fixed augmentation pipeline and a fixed encoder $f_\theta$.
If you increase the batch size $N$ (so there are more views and thus more negatives per anchor), which outcome is the most accurate \emph{conceptual} expectation?
\begin{enumerate}[label=(\alph*), itemsep=2pt]
    \item The task becomes strictly easier because positives become more similar automatically.
    \item The contrastive task typically becomes harder/more demanding because each anchor must be distinguished from more negatives.
    \item The number of positives per anchor increases linearly with $N$.
    \item The model no longer needs the projection head $g_\phi$.
\end{enumerate}

\textbf{Solution.} \boxed{\textbf{(b)}} \\
More negatives means the anchor must remain close to its positive \emph{while staying separated from a larger set of alternatives}, which typically makes the discrimination problem more demanding.

\item \textbf{One quick sanity check (minimal counting).} \\
In SimCLR with $N=4$ original inputs, there are $2N=8$ total views. For a given anchor view, how many positives and how many negatives are used in the denominator of the contrastive objective?
\begin{enumerate}[label=(\alph*), itemsep=2pt]
    \item 1 positive, 5 negatives
    \item 1 positive, 6 negatives
    \item 2 positives, 5 negatives
    \item 2 positives, 6 negatives
\end{enumerate}

\textbf{Solution.} \boxed{\textbf{(b)}} \\
Each anchor has exactly 1 positive (the other view of the same instance). Excluding the anchor itself and its positive leaves $2N-2 = 6$ negatives.

\end{enumerate}

\end{document}
