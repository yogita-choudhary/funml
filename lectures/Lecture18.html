<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>ECE4252-8803_Notes_Template</title>
  <link rel="stylesheet" href="../assets/style.css"/>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav>
  <a href="../index.html">Home</a>
</nav>
<main>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>_Lecture18</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div class="center">

</div>
<p><strong>Contributors:</strong> Dr. Ahmad Mustafa, Dr. Motaz Alfarraj, Dr. Ashraf Alattar, Dr. Chen Zhou</p>
<p><strong>Teaching Assistants</strong> with remarkable contributions include: Kuo-Wei Lai, Wuyang Du, Shiva Mahato, Michael Zhou, Ninghan Zhong</p>
<p><strong>Disclaimer</strong>: <span>All content of these notes are part of this course at Georgia Tech. Any re-use or distribution is not permitted without pre-approved permission. All these notes belong to, created by, and copyrighted for Ghassan AlRegib and Mohit Prabhushankar, Georgia Tech, 2021–2028.</span></p>
<p><strong>License</strong>: <span>These lecture notes are licensed under the <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.</span></p>
<p><strong>Errata</strong>: <span><em>Please submit any errata you find using the following form: <a href="https://forms.office.com/r/fbg9dMWPgY">Errata Form for FunML Textbook</a> or visit: <a href="https://forms.office.com/r/fbg9dMWPgY">https://forms.office.com/r/fbg9dMWPgY</a></em></span></p>
<section id="lecture-objectives" data-number="0.1">
<h2 data-number="1.1"><span class="header-section-number">1.1</span> Lecture Objectives</h2>
<p>This lecture provides an introduction to autoencoders (AEs), exploring their role and motivation within unsupervised learning. The session covers the foundational concepts of autoencoders, detailing how they work and their applications. It then dives into two primary types of autoencoders: fully-connected autoencoders, which use dense neural layers, and convolutional autoencoders, which leverage convolutional layers to capture spatial information in data. Together, these topics provide a comprehensive overview of autoencoders and their relevance in machine learning tasks.</p>
</section>
<section id="autoencoders" data-number="0.2">
<h2 data-number="1.2"><span class="header-section-number">1.2</span> Autoencoders</h2>
<section id="types-of-learning" data-number="0.2.1">
<h3 data-number="1.2.1"><span class="header-section-number">1.2.1</span> Types of Learning</h3>
<p>Supervised and Unsupervide learning were discussed.</p>
<ul>
<li><p><strong>Supervised Learning</strong></p>
<ul>
<li><p>Requires labeled training data <span class="math inline">\((x_1, y_1), \dots, (x_N, y_N)\)</span></p></li>
<li><p>Goal: Learn a mapping function <span class="math inline">\(f_{\theta}: \mathcal{X} \rightarrow \mathcal{Y}\)</span></p></li>
<li><p>Examples:</p>
<ul>
<li><p>Classification, regression, etc.</p></li>
</ul></li>
</ul></li>
<li><p><strong>Unsupervised Learning</strong></p>
<ul>
<li><p>No labels required, only data <span class="math inline">\(x_1, \dots, x_N\)</span></p></li>
<li><p>Goal: Learn some underlying hidden <em>structure</em> of the data</p></li>
<li><p>Examples:</p>
<ul>
<li><p>Clustering, dimensionality reduction, density estimation, etc.</p></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="motivation" data-number="0.2.2">
<h3 data-number="1.2.2"><span class="header-section-number">1.2.2</span> Motivation</h3>
<p>In unsupervised learning, the goal is to learn a lower-dimensional representation <span class="math inline">\(Z\)</span> that captures the essential structure of the data <span class="math inline">\(\mathbf{X}\)</span>. This representation can then be used to reconstruct the data, providing insights into its underlying structure. The <span class="math inline">\(Z\)</span>-space, which often has fewer dimensions than <span class="math inline">\(\mathbf{X}\)</span>, serves various purposes, including data visualization by mapping <span class="math inline">\(\mathbf{X}\)</span> to a 2-dimensional space, dimensionality reduction for compressing images, documents, or text, and supporting subsequent supervised tasks by using <span class="math inline">\(Z\)</span> as a basis for further training or fine-tuning.</p>
<p>An autoencoder is a type of model used for unsupervised learning that aims to learn a lower-dimensional representation <span class="math inline">\(Z\)</span> from the unlabeled input data <span class="math inline">\(\mathbf{X}\)</span>. In the case of linear dimensionality reduction, principal component analysis (PCA) can be used to project <span class="math inline">\(\mathbf{X}\)</span> onto a <span class="math inline">\(K\)</span>-dimensional subspace<br />
</p>
<figure>
<img src="img/lecture18/Screenshot 2024-10-30 at 4.36.04 PM.png" id="fig:enter-label" alt="Autoencoder and Reconstruction" /><figcaption aria-hidden="true">Autoencoder and Reconstruction</figcaption>
</figure>
</section>
<section id="principal-component-analysis-pca" data-number="0.2.3">
<h3 data-number="1.2.3"><span class="header-section-number">1.2.3</span> Principal Component Analysis (PCA)</h3>
<p>The intuition behind Principal Component Analysis (PCA) is that it finds the simplest way to represent data by focusing on the directions with the most variation or “spread." Imagine you have a dataset with many variables that are related to each other. In most real-world data, some features (variables) are more influential than others in explaining the variability. PCA finds these key directions, called principal components, to represent the data in a reduced, yet meaningful way. Here’s the step-by-step intuition:</p>
<ul>
<li><p>Finding the Major Axes of Variation</p>
<ul>
<li><p>PCA first looks for the direction along which data varies the most — this is the first principal component. This component captures the maximum amount of information (variance) possible with a single line.</p></li>
<li><p>Then, PCA finds the next major direction of variation that is perpendicular (uncorrelated) to the first one. This becomes the second principal component.</p></li>
<li><p>This process continues, finding orthogonal directions of decreasing variance.</p></li>
</ul></li>
<li><p>Reducing Dimensions While Retaining Variance</p>
<ul>
<li><p>In many datasets, most of the meaningful variation happens along just a few of these principal components, so you can ignore the rest with minimal loss of information. For instance, if you started with 10 dimensions, maybe only the first 2 or 3 principal components explain most of the variance.</p></li>
<li><p>By projecting the data onto these few components, you reduce the number of dimensions, simplifying the dataset but still retaining the essence of its structure.</p></li>
</ul></li>
</ul>
<figure>
<img src="img/lecture18/output.png" id="fig:enter-label" alt="PCA exmaple" /><figcaption aria-hidden="true">PCA exmaple</figcaption>
</figure>
<p>Here is a figure showing data before and after PCA:</p>
<ul>
<li><p>Left (3D): The original data in a three-dimensional space, where the first two dimensions are highly correlated.</p></li>
<li><p>Right (2D): The result after applying PCA, where the data is projected onto a two-dimensional space along the first two principal components. This 2D representation retains the most significant variation of the original data while simplifying its structure.</p></li>
</ul>
</section>
<section id="linear-autoencoder" data-number="0.2.4">
<h3 data-number="1.2.4"><span class="header-section-number">1.2.4</span> Linear Autoencoder</h3>
<p>A linear autoencoder is a type of neural network used for dimensionality reduction, which can be seen as learning a lower-dimensional representation <span class="math inline">\(Z\)</span> of the input data <span class="math inline">\(\mathbf{X}\)</span> similar to principal component analysis (PCA). In this setup, the encoding layer uses a fully connected (fc) layer with weights <span class="math inline">\(V_K\)</span> and linear activation to map <span class="math inline">\(\mathbf{X}\)</span> to <span class="math inline">\(Z\)</span>. The decoding layer also consists of a fully connected layer with weights <span class="math inline">\(V_K^T\)</span> and linear activation, reconstructing <span class="math inline">\(\mathbf{X}\)</span> as <span class="math inline">\(\hat{\mathbf{X}} = \mathbf{X} V_K V_K^T\)</span>. The autoencoder minimizes the reconstruction loss, defined as <span class="math inline">\(L(\mathbf{X}, \hat{\mathbf{X}}) = \|\mathbf{X} - \hat{\mathbf{X}}\|_F\)</span>. When the weight matrix <span class="math inline">\(V_K\)</span> contains the top <span class="math inline">\(K\)</span> eigenvectors of <span class="math inline">\(\mathbf{X}^T \mathbf{X}\)</span>, the linear autoencoder learns a subspace similar to PCA, effectively projecting <span class="math inline">\(\mathbf{X}\)</span> onto a linear subspace that captures the most significant variance.<br />
</p>
<figure>
<img src="img/lecture18/Screenshot 2024-10-30 at 4.38.07 PM.png" id="fig:enter-label" alt="Linear AutoEncoders" /><figcaption aria-hidden="true">Linear AutoEncoders</figcaption>
</figure>
</section>
<section id="nonlinear-autoencoders" data-number="0.2.5">
<h3 data-number="1.2.5"><span class="header-section-number">1.2.5</span> Nonlinear Autoencoders</h3>
<p>Unlike linear autoencoders, which learn to project data <span class="math inline">\(\mathbf{X}\)</span> onto a linear subspace, nonlinear autoencoders aim to map <span class="math inline">\(\mathbf{X}\)</span> onto a nonlinear manifold. This nonlinear manifold captures the intrinsic structure of the data in a way that is not restricted to linear relationships, allowing the autoencoder to better represent complex patterns within the data. By learning this manifold, the nonlinear autoencoder can capture more detailed variations, which are often lost in linear projections. This makes nonlinear autoencoders especially useful for datasets with intricate structures, as they can effectively reduce dimensionality while preserving essential, nonlinear relationships in the data.</p>
<figure>
<img src="img/lecture18/Screenshot 2024-10-30 at 4.38.15 PM.png" id="fig:enter-label" alt="Nonlinear Autoencoders" /><figcaption aria-hidden="true">Nonlinear Autoencoders</figcaption>
</figure>
</section>
</section>
<section id="fully-connected-autoencoders" data-number="0.3">
<h2 data-number="1.3"><span class="header-section-number">1.3</span> Fully-connected Autoencoders</h2>
<p>Encoder and decoder are fully-connected layers.</p>
<figure>
<img src="img/lecture18/Screenshot 2024-10-30 at 4.56.26 PM.png" id="fig:enter-label" alt="AutoEncoder and Decoder" /><figcaption aria-hidden="true">AutoEncoder and Decoder</figcaption>
</figure>
<p>Fully-connected autoencoders can be effectively applied to the MNIST dataset for dimensionality reduction and feature extraction. The MNIST dataset consists of 28x28 grayscale images of handwritten digits, which are first flattened into 784-dimensional vectors as input. The autoencoder architecture includes multiple fully connected layers with ReLU activation functions in the intermediate layers and a Sigmoid activation function in the output layer.</p>
<figure>
<img src="img/lecture18/Screenshot 2024-11-02 at 8.54.57 PM.png" id="fig:enter-label" alt="Architecture on MNIST Dataset" /><figcaption aria-hidden="true">Architecture on MNIST Dataset</figcaption>
</figure>
<p><strong>Mathematical Notation:</strong></p>
<ul>
<li><p><span class="math inline">\(\mathbf{X} \in \mathbb{R}^{N \times P}\)</span>: <span class="math inline">\(P\)</span>-dimensional input vectors <span class="math inline">\(\mathbf{x}_i \in \mathbb{R}^P\)</span>, where <span class="math inline">\(i = 1 \ldots N\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{W}^{(k)} \in \mathbb{R}^{P^{(k)} \times P^{(k-1)}}\)</span>: weights matrix of neurons in layer <span class="math inline">\(k\)</span></p></li>
<li><p><span class="math inline">\(P^{(k)}\)</span>: the number of neurons in layer <span class="math inline">\(k\)</span></p></li>
<li><p><span class="math inline">\(P^{(k-1)}\)</span>: the number of neurons in layer <span class="math inline">\(k-1\)</span></p></li>
</ul>
<p><strong>Visualization</strong></p>
<ul>
<li><p>Extract the weights <span class="math inline">\(\mathbf{W}^{(k)}\)</span> (without bias) in layer <span class="math inline">\(k\)</span> and reshape into an array of shape <span class="math inline">\((P^{(k)}, h, w, c)\)</span>, where <span class="math inline">\(h \times w \times c = P^{(k-1)}\)</span></p></li>
</ul>
<p>For example, what the network sees the images as is similar to <span class="math inline">\(\begin{bmatrix}
    56 \\
    231 \\
    24 \\
    188 \\
    75 \\
    \vdots \\
    32
\end{bmatrix}_{784}\)</span><br />
The numbers in this matrix represent the pixel intensities of an image after it has been flattened into a single-dimensional vector. In many image-processing tasks, especially in neural networks, images are converted from their 2D form (height x width) into a 1D vector form so that they can be fed into the network as input.Each pixel has an intensity value between 0 and 255, where 0 typically represents black, 255 represents white, and values in between represent varying shades of gray.<br />
The encoder compresses the input into a lower-dimensional latent space (e.g., 32 dimensions), capturing essential features of the data. The decoder then reconstructs the input from this latent representation. When trained, nonlinear autoencoders (using ReLU and Sigmoid activations) produce sharper reconstructions with lower mean squared error (MSE) compared to linear autoencoders, as they learn a more complex latent representation that better captures the nonlinear structures in the digit images.</p>
<figure>
<img src="img/lecture18/Screenshot 2024-10-30 at 6.37.17 PM.png" id="fig:enter-label" alt="NMIST dataset using Linear and Nonlinear FC AE" /><figcaption aria-hidden="true">NMIST dataset using Linear and Nonlinear FC AE</figcaption>
</figure>
<p>AE with nonlinearity learns more complex nonlinear latent representation to generate sharper reconstructions<br />
</p>
<section id="visualization-of-learned-filters-second-and-third-layers" data-number="0.3.1">
<h3 data-number="1.3.1"><span class="header-section-number">1.3.1</span> Visualization of Learned Filters: Second and Third Layers</h3>
<p>In the second layer, weights <span class="math inline">\(W^{(2)}\)</span> are reshaped into 64 filters of size <span class="math inline">\(8 \times 16\)</span>, while in the third layer, weights <span class="math inline">\(W^{(3)}\)</span> are reshaped into 32 filters of size <span class="math inline">\(8 \times 8\)</span>. For the nonlinear autoencoder, fewer neurons are effectively utilized compared to the linear version, indicating that nonlinear activations lead to more selective usage of neurons. The main takeaway is that as the network goes deeper, more neurons are used, but the patterns learned become less distinct, suggesting diminishing returns in interesting features with depth.</p>
<figure>
<img src="img/lecture18/Screenshot 2024-10-30 at 6.55.06 PM.png" id="fig:enter-label" alt="2rd layer" /><figcaption aria-hidden="true">2rd layer</figcaption>
</figure>
<figure>
<img src="img/lecture18/Screenshot 2024-10-30 at 6.55.09 PM.png" id="fig:enter-label" alt="3rd layer" /><figcaption aria-hidden="true">3rd layer</figcaption>
</figure>
</section>
</section>
<section id="visualization-of-2d-latent-space" data-number="0.4">
<h2 data-number="1.4"><span class="header-section-number">1.4</span> Visualization of 2D Latent Space</h2>
<p>A 2-dimensional latent representation is used to visualize how different input data maps in feature space, color-coded by their classes.</p>
<figure>
<img src="img/lecture18/Screenshot 2024-11-02 at 9.31.18 AM.png" id="fig:2d-latent" alt="2D latent space visualization" /><figcaption aria-hidden="true">2D latent space visualization</figcaption>
</figure>
<p>This image shows a visualization of the 2D latent space of a fully-connected autoencoder trained on the MNIST dataset. By compressing images into a two-dimensional latent representation, the model allows us to observe how different digit classes are organized. Color-coded by class, the data points reveal clusters that represent different digits, but with significant overlap. This overlap indicates weak discriminability, as the autoencoder, being unsupervised, is not explicitly trained to separate classes but to capture overall data structure. Additionally, the latent space exhibits discontinuities, where images generated from certain regions appear unrealistic or “digit-like" but do not resemble clear numbers. These gaps in the latent space highlight limitations in fully-connected autoencoders, especially in their ability to learn smooth, continuous representations that are essential for realistic reconstructions.</p>
</section>
<section id="convolutional-autoencoders" data-number="0.5">
<h2 data-number="1.5"><span class="header-section-number">1.5</span> Convolutional Autoencoders</h2>
<p>Fully-connected autoencoders do not leverage spatial structure, treating each input feature independently, which leads to redundancy in the parameters and inefficiency in learning spatial patterns. In contrast, convolutional autoencoders utilize convolutional layers that preserve spatial locality by sharing weights across locations. This allows them to capture spatial dependencies and local features more effectively, making them particularly suited for image data. Convolutional autoencoders use these localized latent representations to reconstruct the input with spatial consistency, resulting in more accurate and efficient reconstructions that retain essential structural information.</p>
<figure>
<img src="img/lecture18/Screenshot 2024-11-02 at 9.31.42 AM.png" id="fig:conv-autoencoder" alt="Convolutional Autoencoders" /><figcaption aria-hidden="true">Convolutional Autoencoders</figcaption>
</figure>
<ul>
<li><p>Encoder: Uses convolution and pooling to reduce dimensions.</p></li>
<li><p>Decoder: Uses unpooling and transposed convolution to reconstruct the input.</p></li>
</ul>
<figure>
<img src="img/lecture18/Screenshot 2024-11-02 at 9.31.52 AM.png" id="fig:encoder-decoder" alt="Encoder and decoder in convolutional autoencoders" /><figcaption aria-hidden="true">Encoder and decoder in convolutional autoencoders</figcaption>
</figure>
<p>In autoencoders, the decoder is not a strict inverse of the encoder but reconstructs the input approximately from a compressed latent representation. While the encoder downsampling loses some information, the decoder upsampling (using transposed convolutions or unpooling) fills in details based on learned patterns rather than exact inversion. By approximating the original input structure, the decoder mirrors the encoder’s architecture to restore spatial information without needing an exact inverse transformation.</p>
<section id="unpooling" data-number="0.5.1">
<h3 data-number="1.5.1"><span class="header-section-number">1.5.1</span> Unpooling</h3>
<p>Unpooling in convolutional autoencoders is a process used to expand feature maps back to their original dimensions. There are different strategies for unpooling:</p>
<figure>
<img src="img/lecture18/Screenshot 2024-11-02 at 9.41.54 PM.png" id="fig:unpooling" alt="Unpooling" /><figcaption aria-hidden="true">Unpooling</figcaption>
</figure>
<ul>
<li><p><strong>Nearest Neighbor</strong>: This method repeats the nearest pixel value, leading to blocky upsampled outputs. For example, a 2x2 input is expanded to a 4x4 output where each value is duplicated in its respective block.</p></li>
<li><p><strong>Bed of Nails</strong>: This method inserts zeros between the original pixels, resulting in a sparse output with fixed pixel locations.</p></li>
</ul>
<p>In the provided example, the nearest neighbor method duplicates values from a 2x2 input to create a 4x4 output, causing blocky patterns, while the bed of nails method creates a 4x4 grid with inserted zeros, maintaining a consistent structure but with empty spaces.</p>
</section>
<section id="max-unpooling" data-number="0.5.2">
<h3 data-number="1.5.2"><span class="header-section-number">1.5.2</span> Max-unpooling</h3>
<figure>
<img src="img/lecture18/Screenshot 2024-11-02 at 9.24.28 PM.png" id="fig:max-unpooling" alt="Max-unpooling with remembered indices" /><figcaption aria-hidden="true">Max-unpooling with remembered indices</figcaption>
</figure>
<p>Max-unpooling is an upsampling technique that restores spatial resolution by reusing indices from max-pooling. During max-pooling, the indices of maximum values in each region are recorded. Max-unpooling places these maximum values back in their original locations, filling other positions with zeros. This method preserves spatial consistency, allowing features to be upsampled in alignment with the original input, which is essential for tasks like image reconstruction and segmentation.</p>
</section>
<section id="learnable-upsampling-transposed-convolution-with-stride" data-number="0.5.3">
<h3 data-number="1.5.3"><span class="header-section-number">1.5.3</span> Learnable Upsampling: Transposed Convolution with Stride </h3>
<figure>
<img src="img/lecture18/Screenshot 2024-11-02 at 8.32.21 PM.png" id="fig:learnable-upsampling" alt="Learnable upsampling through transposed convolution" /><figcaption aria-hidden="true">Learnable upsampling through transposed convolution</figcaption>
</figure>
<p>Transposed convolution enables learning of upsampling, often called upconvolution or fractionally-strided convolution. Transposed convolution, sometimes called deconvolution, involves distributing weights across output pixels based on the input structure. It is conceptually similar to “unpooling + convolution,” but it is not an exact inverse of a standard convolution.</p>
<p>For example: <span class="math display">\[\text{Convolution structure: } \text{Conv 4} = 1 \times 1 + 2 \times 1 + 1 \times 1\]</span> Convolutional structure implies that each pixel contributes a weighted sum to overlapping regions. However, <span class="math display">\[\text{Deconvolution structure: } \text{Deconv 4} = 2 \times 2 + 0 \times 1 + 0 \times 1\]</span> Deconvolution interpretation highlights how upsampling operations map inputs to outputs, maintaining spatial relationships and learning filter effects.</p>
<p>Using unsampling the output size for a 3x3 transposed convolution with stride 2 and padding 1 is calculated as: <span class="math display">\[\text{out size} = \text{stride} \times (\text{input size} - 1) + \text{filter size} - 2 \times \text{padding}\]</span></p>
<p>Substituting the given values: <span class="math display">\[5 = 2 \times (3 - 1) + 3 - 2 \times 1\]</span></p>
</section>
<section id="training-on-mnist" data-number="0.5.4">
<h3 data-number="1.5.4"><span class="header-section-number">1.5.4</span> Training on MNIST</h3>
<p>The structure of the convolutional autoencoder for training on the MNIST dataset is detailed as follows:</p>
<figure>
<img src="img/lecture18/Screenshot 2024-11-02 at 9.20.33 PM.png" id="fig:conv-autoencoder-mnist" alt="Convolutional autoencoder structure for training on MNIST" /><figcaption aria-hidden="true">Convolutional autoencoder structure for training on MNIST</figcaption>
</figure>
<p><strong>Encoder:</strong></p>
<ul>
<li><p><strong>Conv 1:</strong> 16 kernels of size 3x3 with padding 1 and ReLU activation, producing a feature map of size 16x28x28.</p></li>
<li><p><strong>Maxpooling:</strong> Applied with a stride of 2, reducing the feature map to 16x14x14.</p></li>
<li><p><strong>Conv 2:</strong> 8 kernels of size 3x3 with padding 1 and ReLU activation, resulting in a feature map of size 8x14x14.</p></li>
<li><p><strong>Maxpooling:</strong> Applied with a stride of 2, further reducing the feature map to 8x7x7 (latent representation <span class="math inline">\(Z\)</span>).</p></li>
</ul>
<p><strong>Decoder:</strong></p>
<ul>
<li><p><strong>Transposed Conv 3:</strong> 16 kernels of size 2x2 with a stride of 2 and ReLU activation, expanding the feature map to 16x14x14.</p></li>
<li><p><strong>Transposed Conv 4:</strong> 1 kernel of size 2x2 with a stride of 2 and Sigmoid activation, reconstructing the output to size 1x28x28.</p></li>
</ul>
<p>Convolutional AEs achieve lower mean squared error (MSE), demonstrating their capability for clearer reconstructions compared to linear models.</p>
<figure>
<img src="img/lecture18/Screenshot 2024-11-02 at 8.28.20 PM.png" id="fig:input-reconstruction" alt="Input and reconstruction in convolutional autoencoders" /><figcaption aria-hidden="true">Input and reconstruction in convolutional autoencoders</figcaption>
</figure>
</section>
</section>
<section id="common-notations" data-number="0.6">
<h2 data-number="1.6"><span class="header-section-number">1.6</span> Common Notations</h2>
<div class="multicols">
<p><span>2</span></p>
<ul>
<li><p><span class="math inline">\(\mathbf{b}\)</span>: Bias vector</p></li>
<li><p><span class="math inline">\(C_k\)</span>: K-th cluster</p></li>
<li><p><span class="math inline">\(d(\mathbf{x_j, x_k})\)</span>: Dissimilarity between <span class="math inline">\(\mathbf{x_j, x_k}\)</span></p></li>
<li><p><span class="math inline">\(E_\theta\)</span>: Encoding function</p></li>
<li><p><span class="math inline">\(f(\cdot)\)</span>: Trained neural network</p></li>
<li><p><span class="math inline">\(\mathbf{G}(t)\)</span>: Second moment at time t</p></li>
<li><p><span class="math inline">\(G_\Phi\)</span>: Decoding function</p></li>
<li><p><span class="math inline">\(\mathbf{H(\theta)}\)</span>: Hessian matrix</p></li>
<li><p><span class="math inline">\(h_i, h_j\)</span>: Representation space vectors</p></li>
<li><p><span class="math inline">\(k^{(i)}\)</span>: Number of neurons in the <span class="math inline">\(i^{th}\)</span> layer</p></li>
<li><p><span class="math inline">\(M\)</span>: Number of features in a feature vector</p></li>
<li><p><span class="math inline">\(m\)</span>: Degree of polynomial</p></li>
<li><p><span class="math inline">\(m_j\)</span>: J-th centroid</p></li>
<li><p><span class="math inline">\(N\)</span>: Number of data samples</p></li>
<li><p><span class="math inline">\(P\)</span>: Predicted class</p></li>
<li><p><span class="math inline">\(P^{(k)}\)</span>: The number of neurons in layer k</p></li>
<li><p><span class="math inline">\(Q\)</span>: Contrast class</p></li>
<li><p><span class="math inline">\(Q_k\)</span>: Computed clustering for k-th cluster</p></li>
<li><p><span class="math inline">\(R_k\)</span>: Ground truth clustering for k-th cluster</p></li>
<li><p><span class="math inline">\(s(\mathbf{x_j, x_k})\)</span>: Similarity between <span class="math inline">\(\mathbf{x_j, x_k}\)</span></p></li>
<li><p><span class="math inline">\(v(t)\)</span>: First moment at time t</p></li>
<li><p><span class="math inline">\(\mathbf{W}\)</span>: Weight matrix</p></li>
<li><p><span class="math inline">\(w_{ij}\)</span>: Degree of membership of <span class="math inline">\(\mathbf{x_i}\)</span> in <span class="math inline">\(C_j\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{X}\)</span>: Matrix of feature vectors (dataset)</p></li>
<li><p><span class="math inline">\(\mathbf{\hat{X}}\)</span>: Reconstruction of data</p></li>
<li><p><span class="math inline">\(\widetilde{\mathbf{X}}\)</span>: Corrupted input</p></li>
<li><p><span class="math inline">\(\mathbf{x_i}\)</span>: Feature vector (a data sample)</p></li>
<li><p><span class="math inline">\(\mathbf{x_{:,i}}\)</span>: Feature vector of all data samples</p></li>
<li><p><span class="math inline">\(x_i\)</span>: A single feature</p></li>
<li><p><span class="math inline">\(\mathbf{Y}\)</span>: Output matrix</p></li>
<li><p><span class="math inline">\(y_i\)</span>: Target class</p></li>
<li><p><span class="math inline">\(y^{c}\)</span>: Predicted logit for class P</p></li>
<li><p><span class="math inline">\(y^{i}\)</span>: Logit for any class i</p></li>
<li><p><span class="math inline">\(\mathbf{Z}\)</span>: Latent representation</p></li>
<li></li>
<li><p><span class="math inline">\(\alpha\)</span>: Learning rate</p></li>
<li><p><span class="math inline">\(\gamma\)</span>: Bias factor</p></li>
<li><p><span class="math inline">\(\gamma_i^j\)</span>: Posterior of <span class="math inline">\(\mathbf{x_i}\)</span> coming from cluster j</p></li>
<li><p><span class="math inline">\(\epsilon\)</span>: Error margin</p></li>
<li><p><span class="math inline">\(\tilde{\lambda_j}\)</span>: Average activation of neuron <span class="math inline">\(z_{ij}\)</span></p></li>
<li><p><span class="math inline">\(\boldsymbol{\theta}\)</span>: Coefficient vector</p></li>
<li><p><span class="math inline">\(\theta_i\)</span>: A single model coefficient (parameter)</p></li>
<li><p><span class="math inline">\(\hat{\rho_j}\)</span>: Average activation of neuron <span class="math inline">\(z_{ij}\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{\Omega(Z)}\)</span>: Sparsity constraint</p></li>
</ul>
</div>
</section>
</body>
</html>

</main>
</body>
</html>
