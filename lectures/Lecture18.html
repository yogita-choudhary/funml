<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>ECE4252-8803_Notes_Template</title>
  <link rel="stylesheet" href="../assets/style.css"/>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav>
  <a href="../index.html">Home</a>
</nav>
<main>
<div class="center">

</div>
<p><strong>Contributors:</strong> Dr. Ahmad Mustafa, Dr. Motaz Alfarraj, Dr. Ashraf Alattar, Dr. Chen Zhou</p>
<p><strong>Teaching Assistants</strong> with remarkable contributions include: Kuo-Wei Lai, Wuyang Du, Shiva Mahato, Michael Zhou, Ninghan Zhong</p>
<p><strong>Disclaimer</strong>: <span>All content of these notes are part of this course at Georgia Tech. Any re-use or distribution is not permitted without pre-approved permission. All these notes belong to, created by, and copyrighted for Ghassan AlRegib and Mohit Prabhushankar, Georgia Tech, 2021–2028.</span></p>
<p><strong>License</strong>: <span>These lecture notes are licensed under the <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.</span></p>
<p><strong>Errata</strong>: <span><em>Please submit any errata you find using the following form: <a href="https://forms.office.com/r/fbg9dMWPgY">Errata Form for FunML Textbook</a> or visit: <a href="https://forms.office.com/r/fbg9dMWPgY">https://forms.office.com/r/fbg9dMWPgY</a></em></span></p>
<section id="lecture-objectives" data-number="0.1">
<h2 data-number="1.1"><span class="header-section-number">1.1</span> Lecture Objectives</h2>
<p>This lecture introduces autoencoders as an unsupervised learning approach for learning compact latent representations that preserve the essential structure of the data while enabling reconstruction of the original inputs. We begin by motivating representation learning through dimensionality reduction and briefly connecting autoencoders to principal component analysis as a linear baseline for capturing dominant variance. We then develop the core encoder–decoder framework and compare linear autoencoders with nonlinear autoencoders, emphasizing how nonlinearities allow the model to represent more complex, non-linear structure beyond a single subspace. Next, we study practical architectures on MNIST, including fully-connected and convolutional autoencoders, and examine how their learned latent spaces and reconstructions differ in quality. Finally, we discuss how convolutional decoders restore spatial resolution through upsampling mechanisms such as unpooling, max-unpooling with indices, and transposed convolution (including how to compute output sizes), and we conclude with qualitative reconstruction examples and latent-space visualizations.</p>
</section>
<section id="autoencoders" data-number="0.2">
<h2 data-number="1.2"><span class="header-section-number">1.2</span> Autoencoders</h2>
<section id="motivation" data-number="0.2.1">
<h3 data-number="1.2.1"><span class="header-section-number">1.2.1</span> Motivation</h3>
<p>In unsupervised learning, the goal is to learn a lower-dimensional representation <span class="math inline">\(Z\)</span> that captures the essential structure of the data <span class="math inline">\(\mathbf{X}\)</span>. This representation can then be used to reconstruct the data, providing insights into its underlying structure. The <span class="math inline">\(Z\)</span>-space, which often has fewer dimensions than <span class="math inline">\(\mathbf{X}\)</span>, serves various purposes, including data visualization by mapping <span class="math inline">\(\mathbf{X}\)</span> to a 2-dimensional space, dimensionality reduction for compressing images, documents, or text, and supporting subsequent supervised tasks by using <span class="math inline">\(Z\)</span> as a basis for further training or fine-tuning.</p>
<p>An autoencoder is a type of model used for unsupervised learning that aims to learn a lower-dimensional representation <span class="math inline">\(Z\)</span> from the unlabeled input data <span class="math inline">\(\mathbf{X}\)</span>. In the case of linear dimensionality reduction, principal component analysis (PCA) can be used to project <span class="math inline">\(\mathbf{X}\)</span> onto a <span class="math inline">\(K\)</span>-dimensional subspace<br />
</p>
<figure>
<img src="img/lecture18/Screenshot 2024-10-30 at 4.36.04 PM.png" id="fig:enter-label" alt="Autoencoder and Reconstruction" /><figcaption aria-hidden="true">Autoencoder and Reconstruction</figcaption>
</figure>
</section>
<section id="principal-component-analysis-pca" data-number="0.2.2">
<h3 data-number="1.2.2"><span class="header-section-number">1.2.2</span> Principal Component Analysis (PCA)</h3>
<p>The intuition behind Principal Component Analysis (PCA) is that it finds the simplest way to represent data by focusing on the directions with the most variation, or “spread.” Imagine you have a dataset with many variables that are related to each other. In most real-world datasets, some features are more influential than others in explaining the variability. PCA identifies these key directions, called principal components, to represent the data in a reduced yet meaningful way.</p>
<p>PCA begins by finding the direction along which the data varies the most; this becomes the first principal component and captures the maximum possible variance with a single line. Next, PCA identifies the direction of the next largest variation that is perpendicular (and therefore uncorrelated) to the first component. This becomes the second principal component. The process continues, producing a sequence of orthogonal directions that capture decreasing amounts of variance.</p>
<p>In many datasets, most of the meaningful variation lies along just a few of these principal components. As a result, the remaining components can often be ignored with minimal loss of information. For example, a dataset that originally has ten dimensions may be well represented by only two or three principal components. By projecting the data onto these few components, PCA reduces the dimensionality of the dataset while preserving its essential structure.</p>
<figure>
<img src="img/lecture18/output.png" id="fig:pca_example" alt="Illustration of PCA for dimensionality reduction. Left: The original dataset in three dimensions. The points lie roughly near a tilted plane, indicating that much of the variation occurs along only a few directions. Right: The same dataset after applying PCA and projecting onto the first two principal components. The projection preserves the dominant variance while reducing the dimensionality from 3D to 2D." /><figcaption aria-hidden="true"><strong>Illustration of PCA for dimensionality reduction.</strong> <strong>Left:</strong> The original dataset in three dimensions. The points lie roughly near a tilted plane, indicating that much of the variation occurs along only a few directions. <strong>Right:</strong> The same dataset after applying PCA and projecting onto the first two principal components. The projection preserves the dominant variance while reducing the dimensionality from 3D to 2D.</figcaption>
</figure>
<p>Figure <a href="#fig:pca_example" data-reference-type="ref" data-reference="fig:pca_example">2</a> illustrates how PCA reduces dimensionality while preserving the most important structure in the data. The original three-dimensional dataset lies close to a lower-dimensional subspace, which allows PCA to project the data onto two principal components with minimal information loss.</p>
</section>
<section id="linear-autoencoder" data-number="0.2.3">
<h3 data-number="1.2.3"><span class="header-section-number">1.2.3</span> Linear Autoencoder</h3>
<p>A linear autoencoder is a neural network for dimensionality reduction that learns a lower-dimensional representation <span class="math inline">\(Z\)</span> of the input data <span class="math inline">\(\mathbf{X}\)</span> using only linear transformations. When trained using squared reconstruction loss and without nonlinear activation functions, a linear autoencoder is closely related to principal component analysis (PCA).</p>
<p>In this setup, the encoder consists of a fully connected layer with weight matrix <span class="math inline">\(V_K \in \mathbb{R}^{P \times K}\)</span>, mapping the input to a latent representation <span class="math display">\[Z = \mathbf{X} V_K.\]</span> The decoder reconstructs the input using another fully connected layer with weights <span class="math inline">\(V_K^T\)</span>, producing <span class="math display">\[\hat{\mathbf{X}} = Z V_K^T = \mathbf{X} V_K V_K^T.\]</span></p>
<p>The model is trained by minimizing the squared reconstruction loss <span class="math display">\[L(\mathbf{X}, \hat{\mathbf{X}}) = \|\mathbf{X} - \hat{\mathbf{X}}\|_F^2.\]</span></p>
<p>When the columns of <span class="math inline">\(V_K\)</span> are orthonormal and correspond to the top <span class="math inline">\(K\)</span> eigenvectors of <span class="math inline">\(\mathbf{X}^T \mathbf{X}\)</span>, the linear autoencoder projects the data onto the same <span class="math inline">\(K\)</span>-dimensional subspace found by PCA. In this case, the learned representation spans the principal subspace that captures the largest variance in the data.</p>
<figure>
<img src="img/lecture18/Screenshot 2024-10-30 at 4.38.07 PM.png" id="fig:enter-label" alt="Linear AutoEncoders" /><figcaption aria-hidden="true">Linear AutoEncoders</figcaption>
</figure>
</section>
<section id="nonlinear-autoencoders" data-number="0.2.4">
<h3 data-number="1.2.4"><span class="header-section-number">1.2.4</span> Nonlinear Autoencoders</h3>
<p>Unlike linear autoencoders, which restrict the model to linear projections, nonlinear autoencoders introduce nonlinear activation functions (such as ReLU, sigmoid, or tanh) in the encoder and decoder networks. This allows the model to learn nonlinear transformations of the input data <span class="math inline">\(\mathbf{X}\)</span> and map it to a latent representation that lies on a nonlinear manifold.</p>
<p>Formally, the encoder and decoder become nonlinear functions, <span class="math display">\[Z = E_\theta(\mathbf{X}), \qquad \hat{\mathbf{X}} = G_\Phi(Z),\]</span> and the model is trained by minimizing the same reconstruction loss <span class="math inline">\(\|\mathbf{X}-\hat{\mathbf{X}}\|_F^2\)</span>. Because of the nonlinear activations, the equivalence with PCA no longer holds, and the autoencoder is no longer limited to learning a linear subspace.</p>
<p>This added flexibility enables nonlinear autoencoders to capture complex structures and variations in data that cannot be represented by linear dimensionality reduction. As a result, they are particularly effective for high-dimensional datasets such as images, where the data often lies on curved, lower-dimensional manifolds embedded in high-dimensional space.</p>
<figure>
<img src="img/lecture18/Screenshot 2024-10-30 at 4.38.15 PM.png" id="fig:enter-label" alt="Nonlinear Autoencoders" /><figcaption aria-hidden="true">Nonlinear Autoencoders</figcaption>
</figure>
</section>
</section>
<section id="fully-connected-autoencoders" data-number="0.3">
<h2 data-number="1.3"><span class="header-section-number">1.3</span> Fully-connected Autoencoders</h2>
<p>In fully-connected autoencoders, both the encoder and decoder are implemented using dense (fully-connected) neural network layers. Figure <a href="#fig:fc-ae-structure" data-reference-type="ref" data-reference="fig:fc-ae-structure">5</a> illustrates the basic encoder–decoder structure.</p>
<figure>
<img src="img/lecture18/Screenshot 2024-10-30 at 4.56.26 PM.png" id="fig:fc-ae-structure" alt="Fully-connected autoencoder encoder–decoder structure." /><figcaption aria-hidden="true">Fully-connected autoencoder encoder–decoder structure.</figcaption>
</figure>
<p>Fully-connected autoencoders can be applied to the MNIST handwritten digit dataset for dimensionality reduction and feature extraction. Each MNIST image has size <span class="math inline">\(28\times28\)</span> and is flattened into a 784-dimensional vector before being fed into the network. The architecture typically consists of several fully-connected layers with ReLU activations in the hidden layers and a sigmoid activation in the output layer to reconstruct pixel intensities.</p>
<figure>
<img src="img/lecture18/Screenshot 2024-11-02 at 8.54.57 PM.png" id="fig:fc-ae-mnist" alt="Example fully-connected autoencoder architecture for MNIST." /><figcaption aria-hidden="true">Example fully-connected autoencoder architecture for MNIST.</figcaption>
</figure>
<p>Let the dataset be <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{N \times P}\)</span>, where each input vector <span class="math inline">\(\mathbf{x}_i \in \mathbb{R}^P\)</span> and the weight matrix of layer <span class="math inline">\(k\)</span> is <span class="math inline">\(\mathbf{W}^{(k)} \in \mathbb{R}^{P^{(k)} \times P^{(k-1)}}\)</span>. During training, the encoder compresses the input into a low-dimensional latent representation (for example, 32 dimensions), and the decoder reconstructs the image from this representation by minimizing reconstruction error.</p>
<p>Because MNIST images are flattened into vectors, the network treats each pixel independently and does not explicitly exploit spatial structure. Nevertheless, nonlinear fully-connected autoencoders learn more expressive latent representations than linear autoencoders, producing sharper reconstructions and lower mean squared reconstruction error.</p>
<figure>
<img src="img/lecture18/Screenshot 2024-10-30 at 6.37.17 PM.png" id="fig:fc-ae-results" alt="MNIST reconstructions using linear and nonlinear fully-connected autoencoders. Nonlinear models produce sharper reconstructions." /><figcaption aria-hidden="true">MNIST reconstructions using linear and nonlinear fully-connected autoencoders. Nonlinear models produce sharper reconstructions.</figcaption>
</figure>
<section id="visualization-of-learned-filters-second-and-third-layers" data-number="0.3.1">
<h3 data-number="1.3.1"><span class="header-section-number">1.3.1</span> Visualization of Learned Filters: Second and Third Layers</h3>
<p>To better understand what the autoencoder learns internally, we can visualize the learned weight matrices by reshaping them into image-like filters. For the second layer, the weights <span class="math inline">\(\mathbf{W}^{(2)}\)</span> can be reshaped into 64 filters of size <span class="math inline">\(8 \times 16\)</span>, while the third-layer weights <span class="math inline">\(\mathbf{W}^{(3)}\)</span> can be reshaped into 32 filters of size <span class="math inline">\(8 \times 8\)</span>. These visualizations provide insight into how representations evolve across network depth.</p>
<p>Compared to the linear autoencoder, the nonlinear autoencoder tends to use neurons more selectively, leading to sparser and more specialized feature representations. As the network becomes deeper, the learned filters become increasingly abstract and less visually interpretable. Early layers tend to capture simple structures, while deeper layers encode more complex and distributed patterns, illustrating the hierarchical nature of representation learning.</p>
<figure>
<img src="img/lecture18/Screenshot 2024-10-30 at 6.55.06 PM.png" id="fig:enter-label" alt="2rd layer" /><figcaption aria-hidden="true">2rd layer</figcaption>
</figure>
<figure>
<img src="img/lecture18/Screenshot 2024-10-30 at 6.55.09 PM.png" id="fig:enter-label" alt="3rd layer" /><figcaption aria-hidden="true">3rd layer</figcaption>
</figure>
</section>
</section>
<section id="visualization-of-2d-latent-space" data-number="0.4">
<h2 data-number="1.4"><span class="header-section-number">1.4</span> Visualization of 2D Latent Space</h2>
<p>To better understand how autoencoders organize data internally, we can restrict the latent space to two dimensions and visualize how the input samples are embedded. Figure <a href="#fig:2d-latent" data-reference-type="ref" data-reference="fig:2d-latent">10</a> shows the 2D latent representation of a fully-connected autoencoder trained on the MNIST dataset, where each point corresponds to an input image and is color-coded by its true digit label.</p>
<figure>
<img src="img/lecture18/Screenshot 2024-11-02 at 9.31.18 AM.png" id="fig:2d-latent" alt="Two-dimensional latent space learned by a fully-connected autoencoder." /><figcaption aria-hidden="true">Two-dimensional latent space learned by a fully-connected autoencoder.</figcaption>
</figure>
<p>By compressing images into a two-dimensional representation, the autoencoder enables direct visualization of how the model organizes the data. Although clusters corresponding to different digits begin to emerge, there is still significant overlap between classes. This limited separability occurs because the autoencoder is trained in an unsupervised manner and is optimized only for reconstruction rather than classification.</p>
<p>The latent space also exhibits regions that do not correspond to realistic digits. Samples generated from these areas appear distorted or ambiguous, revealing discontinuities in the learned representation. These gaps highlight an important limitation of fully-connected autoencoders: they may learn latent spaces that are not smooth or well-structured, which can lead to unrealistic reconstructions when sampling from arbitrary latent locations.</p>
</section>
<section id="convolutional-autoencoders" data-number="0.5">
<h2 data-number="1.5"><span class="header-section-number">1.5</span> Convolutional Autoencoders</h2>
<p>Fully-connected autoencoders treat each input feature independently and therefore do not exploit the spatial structure present in images. This leads to redundant parameters and inefficient learning of spatial patterns. Convolutional autoencoders address this limitation by replacing dense layers with convolutional layers that share weights across spatial locations. This parameter sharing introduces a strong spatial inductive bias, enabling the model to capture local patterns such as edges, textures, and shapes more effectively.</p>
<p>Figure <a href="#fig:conv-autoencoder" data-reference-type="ref" data-reference="fig:conv-autoencoder">11</a> illustrates the overall architecture of a convolutional autoencoder. The encoder uses convolution and pooling operations to gradually reduce spatial resolution and compress the input into a latent representation, while the decoder reconstructs the image by progressively restoring spatial resolution.</p>
<figure>
<img src="img/lecture18/Screenshot 2024-11-02 at 9.31.42 AM.png" id="fig:conv-autoencoder" alt="Overview of convolutional autoencoder architecture." /><figcaption aria-hidden="true">Overview of convolutional autoencoder architecture.</figcaption>
</figure>
<p>A more detailed view of the encoder–decoder structure is shown in Figure <a href="#fig:encoder-decoder" data-reference-type="ref" data-reference="fig:encoder-decoder">12</a>. The encoder performs downsampling through convolution and pooling, whereas the decoder performs upsampling using unpooling and transposed convolution to reconstruct the image.</p>
<figure>
<img src="img/lecture18/Screenshot 2024-11-02 at 9.31.52 AM.png" id="fig:encoder-decoder" alt="Encoder and decoder structure in convolutional autoencoders." /><figcaption aria-hidden="true">Encoder and decoder structure in convolutional autoencoders.</figcaption>
</figure>
<p>It is important to note that the decoder is not a strict mathematical inverse of the encoder. Downsampling operations inevitably discard some information, and the decoder reconstructs the input only approximately from the compressed latent representation. During upsampling, operations such as transposed convolution and unpooling learn to restore spatial structure by filling in missing details based on patterns learned during training rather than by exactly reversing the encoder.</p>
<section id="unpooling" data-number="0.5.1">
<h3 data-number="1.5.1"><span class="header-section-number">1.5.1</span> Unpooling</h3>
<p>In convolutional autoencoders, <strong>unpooling</strong> is used in the decoder to increase the spatial resolution of feature maps that were previously reduced by pooling operations. While pooling compresses information by downsampling, unpooling attempts to reverse this process by expanding feature maps back to larger spatial dimensions. Unlike pooling, however, unpooling does not uniquely recover the original activations; instead, it relies on heuristic upsampling strategies.</p>
<p>Two common unpooling strategies are illustrated in Figure <a href="#fig:unpooling" data-reference-type="ref" data-reference="fig:unpooling">13</a>.</p>
<figure>
<img src="img/lecture18/Screenshot 2024-11-02 at 9.41.54 PM.png" id="fig:unpooling" alt="Unpooling strategies. Left: Nearest-neighbor upsampling duplicates values to increase resolution. Right: Bed-of-nails upsampling inserts zeros between original activations." /><figcaption aria-hidden="true"><strong>Unpooling strategies.</strong> Left: Nearest-neighbor upsampling duplicates values to increase resolution. Right: Bed-of-nails upsampling inserts zeros between original activations.</figcaption>
</figure>
<p><strong>Nearest-neighbor unpooling</strong> expands the feature map by repeating each value into a larger block. For example, a <span class="math inline">\(2 \times 2\)</span> input can be expanded into a <span class="math inline">\(4 \times 4\)</span> output by duplicating each value into a <span class="math inline">\(2 \times 2\)</span> region. This produces a piecewise-constant, block-like structure.</p>
<p><strong>Bed-of-nails unpooling</strong> increases spatial resolution by inserting zeros between the original pixel values. The original activations remain in fixed locations, while newly created positions are set to zero, resulting in a sparse intermediate representation. Subsequent convolutional layers then learn how to fill in these missing values.</p>
<p>In practice, unpooling is often combined with learned convolutional layers to reconstruct higher-resolution feature maps in the decoder.</p>
</section>
<section id="max-unpooling" data-number="0.5.2">
<h3 data-number="1.5.2"><span class="header-section-number">1.5.2</span> Max-unpooling</h3>
<figure>
<img src="img/lecture18/Screenshot 2024-11-02 at 9.24.28 PM.png" id="fig:max-unpooling" alt="Max-unpooling using stored pooling indices." /><figcaption aria-hidden="true">Max-unpooling using stored pooling indices.</figcaption>
</figure>
<p>Max-unpooling is an upsampling technique designed to partially reverse the spatial information loss introduced by max-pooling. During max-pooling, the indices of the maximum values within each pooling window are recorded. Max-unpooling reuses these stored indices to place the pooled activations back into their original spatial locations, while the remaining positions are filled with zeros.</p>
<p>Unlike nearest-neighbor or bed-of-nails unpooling, which apply fixed heuristics, max-unpooling leverages information from the forward pass of the encoder. By restoring activations to the locations where the strongest responses originally occurred, the decoder preserves spatial alignment with the input. This spatial consistency is especially important for image reconstruction and dense prediction tasks such as segmentation.</p>
</section>
<section id="learnable-upsampling-transposed-convolution-with-stride" data-number="0.5.3">
<h3 data-number="1.5.3"><span class="header-section-number">1.5.3</span> Learnable Upsampling: Transposed Convolution with Stride</h3>
<figure>
<img src="img/lecture18/Screenshot 2024-11-02 at 8.32.21 PM.png" id="fig:learnable-upsampling" alt="Learnable upsampling using transposed convolution." /><figcaption aria-hidden="true">Learnable upsampling using transposed convolution.</figcaption>
</figure>
<p>Transposed convolution is a learnable upsampling operation commonly used in the decoder of convolutional autoencoders. It is sometimes called <em>fractionally-strided convolution</em> or <em>deconvolution</em> (although it is not a true inverse of convolution). Conceptually, transposed convolution can be viewed as performing unpooling followed by a convolution, but in a single learnable operation.</p>
<p>In standard convolution, multiple input pixels contribute to a single output pixel through a weighted sum. In contrast, transposed convolution reverses this mapping: each input pixel contributes to multiple output pixels. This allows the spatial resolution of feature maps to increase while learning how to distribute information across the larger output grid.</p>
<p>The output spatial size of a transposed convolution (per dimension) is given by <span class="math display">\[\text{output size} = \text{stride}\,(\text{input size} - 1) + \text{kernel size} - 2 \times \text{padding}.\]</span></p>
<p>For example, for a <span class="math inline">\(3 \times 3\)</span> transposed convolution with stride 2 and padding 1 applied to an input of size 3, the output size becomes <span class="math display">\[5 = 2(3-1) + 3 - 2(1).\]</span></p>
<p>This learnable upsampling allows the decoder to restore spatial resolution while learning how to generate realistic high-resolution feature maps.</p>
</section>
<section id="training-on-mnist" data-number="0.5.4">
<h3 data-number="1.5.4"><span class="header-section-number">1.5.4</span> Training on MNIST</h3>
<p>We now consider a convolutional autoencoder trained on the MNIST handwritten digit dataset. Figure <a href="#fig:conv-autoencoder-mnist" data-reference-type="ref" data-reference="fig:conv-autoencoder-mnist">16</a> illustrates the overall architecture. The network follows a typical encoder–decoder design, where the encoder compresses the input image into a compact latent representation and the decoder reconstructs the image from this representation.</p>
<figure>
<img src="img/lecture18/Screenshot 2024-11-02 at 9.20.33 PM.png" id="fig:conv-autoencoder-mnist" alt="Convolutional autoencoder architecture for MNIST. The encoder progressively reduces spatial resolution to form a compact latent representation, while the decoder reconstructs the image using transposed convolutions." /><figcaption aria-hidden="true"><strong>Convolutional autoencoder architecture for MNIST.</strong> The encoder progressively reduces spatial resolution to form a compact latent representation, while the decoder reconstructs the image using transposed convolutions.</figcaption>
</figure>
<p><strong>Encoder.</strong> The encoder begins with a convolutional layer containing 16 kernels of size <span class="math inline">\(3\times3\)</span> with padding 1 and ReLU activation, producing feature maps of size <span class="math inline">\(16\times28\times28\)</span>. Max pooling with stride 2 then reduces the spatial resolution to <span class="math inline">\(16\times14\times14\)</span>. A second convolutional layer with 8 kernels (again <span class="math inline">\(3\times3\)</span>, padding 1, ReLU) produces feature maps of size <span class="math inline">\(8\times14\times14\)</span>, followed by another pooling layer that reduces the representation to <span class="math inline">\(8\times7\times7\)</span>. This compressed feature map serves as the latent representation <span class="math inline">\(Z\)</span>.</p>
<p><strong>Decoder.</strong> The decoder mirrors the encoder using transposed convolutions to increase spatial resolution. A transposed convolution with 16 kernels of size <span class="math inline">\(2\times2\)</span> and stride 2 upsamples the feature maps to <span class="math inline">\(16\times14\times14\)</span>. A final transposed convolution with a single <span class="math inline">\(2\times2\)</span> kernel and sigmoid activation reconstructs the output image of size <span class="math inline">\(1\times28\times28\)</span>.</p>
<p>Convolutional autoencoders trained on MNIST typically achieve lower reconstruction error (measured using mean squared error) compared to linear autoencoders, demonstrating their ability to capture spatial structure in images. Figure <a href="#fig:input-reconstruction" data-reference-type="ref" data-reference="fig:input-reconstruction">17</a> shows examples of original images and their reconstructions.</p>
<figure>
<img src="img/lecture18/Screenshot 2024-11-02 at 8.28.20 PM.png" id="fig:input-reconstruction" alt="Original vs reconstructed MNIST images. The autoencoder learns a compact representation that preserves the key structure of the digits." /><figcaption aria-hidden="true"><strong>Original vs reconstructed MNIST images.</strong> The autoencoder learns a compact representation that preserves the key structure of the digits.</figcaption>
</figure>
</section>
</section>
<section id="qa-section" data-number="0.6">
<h2 data-number="1.6"><span class="header-section-number">1.6</span> Q&amp;A Section</h2>
<ol>
<li><p><strong>Question:</strong> What is the main objective of an autoencoder, and how does it differ from supervised learning?</p>
<p><strong>Solution:</strong> An autoencoder learns a compact latent representation of the input and reconstructs the original data from this representation. Unlike supervised learning, it does not use labels and is trained by minimizing reconstruction error. Its goal is to capture the underlying structure of the data rather than predict target outputs.</p></li>
<li><p><strong>Question:</strong> Under what conditions is a linear autoencoder equivalent to PCA?</p>
<p><strong>Solution:</strong> When the network has no nonlinear activations and is trained using squared reconstruction loss, a linear autoencoder learns the same principal subspace as PCA. In this case, <span class="math display">\[\hat{\mathbf{X}}=\mathbf{X}V_KV_K^T,\]</span> which corresponds to projection onto the top <span class="math inline">\(K\)</span> principal components.</p></li>
<li><p><strong>Question:</strong> Why do nonlinear autoencoders often achieve lower reconstruction error than linear autoencoders on image data?</p>
<p><strong>Solution:</strong> Image data typically lies on nonlinear manifolds. Nonlinear activation functions allow the encoder and decoder to learn nonlinear transformations, enabling the model to capture more complex patterns than linear subspace methods.</p></li>
<li><p><strong>Question:</strong> When training a fully-connected autoencoder on MNIST, what is the dimensionality of each input sample?</p>
<p><strong>Solution:</strong> Each MNIST image is <span class="math inline">\(28\times28\)</span>. Fully-connected networks require vector inputs, so the image is flattened into a vector of length <span class="math display">\[28\times28=784.\]</span></p></li>
<li><p><strong>Question:</strong> What is a key limitation of fully-connected autoencoders for image data?</p>
<p><strong>Solution:</strong> They treat pixels independently and ignore spatial structure, leading to many parameters and inefficient learning of spatial patterns.</p></li>
<li><p><strong>Question:</strong> Why are convolutional autoencoders better suited for image reconstruction?</p>
<p><strong>Solution:</strong> Convolutional layers share weights across spatial locations and preserve locality, allowing the network to learn spatial features efficiently with far fewer parameters.</p></li>
<li><p><strong>Question:</strong> Compute the output size of a transposed convolution with input size <span class="math inline">\(4\)</span>, kernel size <span class="math inline">\(3\)</span>, stride <span class="math inline">\(2\)</span>, and padding <span class="math inline">\(1\)</span>.</p>
<p><strong>Solution:</strong> <span class="math display">\[\text{output size}=s(n-1)+k-2p\]</span> <span class="math display">\[=2(4-1)+3-2(1)=7.\]</span></p></li>
<li><p><strong>Question:</strong> Why do digit clusters overlap in the 2D latent space learned by a fully-connected autoencoder?</p>
<p><strong>Solution:</strong> The autoencoder is trained only to reconstruct inputs, not to separate classes. Therefore, the latent space reflects visual similarity rather than class separability, leading to overlapping clusters.</p></li>
</ol>
</section>

</main>
</body>
</html>
