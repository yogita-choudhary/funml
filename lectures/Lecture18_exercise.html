<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>Lecture18 In-class Exercise</title>
  <link rel="stylesheet" href="../assets/style.css"/>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav>
  <a href="../index.html">Home</a>
</nav>
<main>
<div class="center">
<p><span><strong>In-Class Exercise (Canvas Quiz) — 10 minutes</strong></span><br />
<span><strong>Lecture 18: Autoencoders</strong></span></p>
</div>
<p>We study an <strong>autoencoder</strong> with encoder and decoder <span class="math display">\[\mathbf{z} = f(\mathbf{x}), \qquad \hat{\mathbf{x}} = g(\mathbf{z}),\]</span> trained using <strong>MSE reconstruction loss</strong> <span class="math display">\[L(\mathbf{x},\hat{\mathbf{x}})=\frac{1}{n}\sum_{i=1}^{n}(x_i-\hat{x}_i)^2.\]</span></p>
<ol>
<li><p><strong>What does the bottleneck enforce?</strong><br />
Suppose the latent dimension is smaller than the input dimension: <span class="math inline">\(\dim(\mathbf{z}) \ll \dim(\mathbf{x})\)</span>. What is the <strong>most accurate</strong> interpretation of what this “bottleneck” forces the model to do?</p>
<ol>
<li><p>Memorize <span class="math inline">\(\mathbf{x}\)</span> exactly, since reconstruction is always perfect.</p></li>
<li><p>Learn a compressed representation that preserves information useful for reconstructing <span class="math inline">\(\mathbf{x}\)</span>.</p></li>
<li><p>Perform supervised classification of <span class="math inline">\(\mathbf{x}\)</span> into labels.</p></li>
<li><p>Guarantee the learned features are invariant to all transformations of the input.</p></li>
</ol></li>
<li><p><strong>Effect of increasing latent dimension</strong><br />
If we <em>increase</em> <span class="math inline">\(\dim(\mathbf{z})\)</span> while keeping the model class flexible enough, which outcome is <strong>most likely</strong>?</p>
<ol>
<li><p>Reconstruction error on the training set tends to decrease (or stay the same).</p></li>
<li><p>Reconstruction error on the training set must increase.</p></li>
<li><p>The encoder and decoder become exact inverses of each other.</p></li>
<li><p>The model becomes supervised.</p></li>
</ol></li>
<li><p><strong>What does MSE encourage?</strong><br />
For image reconstruction, MSE tends to produce reconstructions that are:</p>
<ol>
<li><p>Sharper, since it strongly rewards high-frequency details.</p></li>
<li><p>Often blurrier, because it averages over plausible pixel values.</p></li>
<li><p>Always identical to the input, regardless of model capacity.</p></li>
<li><p>Invariant to translation by design.</p></li>
</ol></li>
<li><p><strong>Why convolutional autoencoders for images? (True/False)</strong><br />
A convolutional autoencoder is usually preferred over a fully-connected autoencoder for image data because it preserves <em>spatial locality</em> and uses <em>weight sharing</em>, which reduces parameters and exploits image structure.</p></li>
<li><p><strong>Decoder intuition (best statement)</strong><br />
Which statement best describes the decoder <span class="math inline">\(g(\cdot)\)</span> in an autoencoder?</p>
<ol>
<li><p>It is a guaranteed mathematical inverse of the encoder.</p></li>
<li><p>It learns a mapping from the latent code back to the input space to approximate reconstruction.</p></li>
<li><p>It only works if <span class="math inline">\(\dim(\mathbf{z})=\dim(\mathbf{x})\)</span>.</p></li>
<li><p>It computes labels for classification.</p></li>
</ol></li>
</ol>

</main>
</body>
</html>
