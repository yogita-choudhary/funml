<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>ECE4252-8803_Notes_Template</title>
  <link rel="stylesheet" href="../assets/style.css"/>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav>
  <a href="../index.html">Home</a>
</nav>
<main>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>_Lecture26</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div class="center">

</div>
<p><strong>Contributors:</strong> Dr. Ahmad Mustafa, Dr. Motaz Alfarraj, Dr. Ashraf Alattar, Dr. Chen Zhou</p>
<p><strong>Teaching Assistants</strong> with remarkable contributions include: Kuo-Wei Lai, Wuyang Du, Shiva Mahato, Michael Zhou, Ninghan Zhong</p>
<p><strong>Disclaimer</strong>: <span>All content of these notes are part of this course at Georgia Tech. Any re-use or distribution is not permitted without pre-approved permission. All these notes belong to, created by, and copyrighted for Ghassan AlRegib and Mohit Prabhushankar, Georgia Tech, 2021–2028.</span></p>
<p><strong>License</strong>: <span>These lecture notes are licensed under the <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.</span></p>
<p><strong>Errata</strong>: <span><em>Please submit any errata you find using the following form: <a href="https://forms.office.com/r/fbg9dMWPgY">Errata Form for FunML Textbook</a> or visit: <a href="https://forms.office.com/r/fbg9dMWPgY">https://forms.office.com/r/fbg9dMWPgY</a></em></span></p>
<section id="recap-of-superviesd-learning" data-number="0.1">
<h2 data-number="1.1"><span class="header-section-number">1.1</span> Recap of Superviesd Learning</h2>
<p>Supervised learning relies on labeled data to train models. It faces several challenges, leading to the exploration of alternative learning paradigms such as unsupervised and self-supervised learning.</p>
<ul>
<li><p>High cost and time required for labeling large datasets.</p></li>
<li><p>Dependence on trained experts for specialized labeling tasks (e.g., medical or seismic data).</p></li>
<li><p>Data privacy regulations and restrictions, such as HIPAA, that limit access to labeled data.</p></li>
<li><p>Unreleased datasets and limited availability of high-quality labeled data.</p></li>
</ul>
</section>
<section id="comparison-of-data-requirements-and-structures-summary-supervised-unsupervised-self-supervised" data-number="0.2">
<h2 data-number="1.2"><span class="header-section-number">1.2</span> Comparison of Data Requirements and Structures Summary (supervised, unsupervised, Self-supervised):</h2>
<p><img src="img/lecture26/S US SSL.png" id="fig:comparison" style="width:50.0%" alt="image" /> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p>
<h3 class="unnumbered" id="supervised-learning">Supervised Learning</h3>
<ul>
<li><p>Requires labeled datasets <span class="math inline">\((x, y)\)</span>, where <span class="math inline">\(x\)</span> represents input data and <span class="math inline">\(y\)</span> represents ground-truth labels.</p></li>
<li><p>The model directly learns a mapping <span class="math inline">\(f(x) \rightarrow y\)</span> to minimize the loss function <span class="math inline">\(L\)</span>.</p></li>
</ul>
<h3 class="unnumbered" id="unsupervised-learning">Unsupervised Learning</h3>
<ul>
<li><p>Requires only unlabeled data <span class="math inline">\(x\)</span>.</p></li>
<li><p>The model <span class="math inline">\(f\)</span> learns the internal structure or distribution of the data and maps the input <span class="math inline">\(x\)</span> to a reconstructed output <span class="math inline">\(\hat{x}\)</span>: <span class="math inline">\(f(x) \to \hat{x}\)</span>.</p></li>
<li><p>The loss <span class="math inline">\(L(\hat{x}, x)\)</span> is computed based on reconstruction error or other objectives, guiding the model to better represent the characteristics of the data.</p></li>
</ul>
<h3 class="unnumbered" id="self-supervised-learning">Self-Supervised Learning</h3>
<ul>
<li><p>Relies data <span class="math inline">\(x\)</span> and on partially labeled data <span class="math inline">\(y\)</span>, and introduces <em>pre-text tasks</em> to generate pseudo-labels <span class="math inline">\(z\)</span>.</p></li>
<li><p>The model first learns a pre-text representation using pseudo-labels and then adapts the learned features for downstream tasks with limited data <span class="math inline">\(y\)</span> (e.g., classification).</p></li>
<li><p>The "Pre-text Tasks" are what unique for Self-Supervised Learning than Unsupervised Learning</p></li>
</ul>
</section>
<section id="self-supervised-learning-ssl-structure" data-number="0.3">
<h2 data-number="1.3"><span class="header-section-number">1.3</span> Self-Supervised Learning (SSL) Structure</h2>
<p>Self-supervised learning (SSL) utilizes both labeled and unlabeled data to generate pseudo-labels and learn representations through pre-text tasks. Its structure can be summarized in three key steps:</p>
<h3 class="unnumbered" id="generate-pseudo-labels">Generate Pseudo-Labels</h3>
<p><img src="img/lecture26/gen pseudo lab.png" id="fig:comparison" style="width:70.0%" alt="image" /> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p>
<ul>
<li><p><strong>Input:</strong></p>
<ul>
<li><p>Unlabeled data <span class="math inline">\((x_1, \dots, x_N)\)</span>.</p></li>
<li><p>Optionally, a small amount of labeled data <span class="math inline">\((x_1, \dots, x_M), (y_1, \dots, y_M)\)</span>.</p></li>
</ul></li>
<li><p><strong>Process:</strong></p>
<ul>
<li><p>Use a pre-text task <span class="math inline">\(P\)</span> (e.g., rotation prediction, contrastive learning) to generate pseudo-labels <span class="math inline">\((z_1, \dots, z_N)\)</span> for the unlabeled data.</p></li>
</ul></li>
<li><p><strong>Output:</strong></p>
<ul>
<li><p>Pseudo-labeled data: <span class="math inline">\((x_1, \dots, x_N), (z_1, \dots, z_N)\)</span>.</p></li>
</ul></li>
</ul>
<h3 class="unnumbered" id="pre-training-on-pseudo-labels">Pre-Training on Pseudo-Labels</h3>
<p><img src="img/lecture26/pre train on pse.png" id="fig:comparison" style="width:70.0%" alt="image" /> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p>
<ul>
<li><p><strong>Input:</strong></p>
<ul>
<li><p>Pseudo-labeled data <span class="math inline">\((x_1, \dots, x_N), (z_1, \dots, z_N)\)</span>.</p></li>
</ul></li>
<li><p><strong>Process:</strong></p>
<ul>
<li><p>Train a neural network <span class="math inline">\(h_\theta\)</span> using the pseudo-labels.</p></li>
<li><p>Minimize the loss <span class="math inline">\(L(z, \hat{z})\)</span>, where <span class="math inline">\(z\)</span> is the pseudo-label and <span class="math inline">\(\hat{z}\)</span> is the model prediction.</p></li>
</ul></li>
<li><p><strong>Output:</strong></p>
<ul>
<li><p>A pre-trained representation <span class="math inline">\(h_\theta\)</span> capturing patterns in the unlabeled data.</p></li>
</ul></li>
</ul>
<h3 class="unnumbered" id="fine-tuning-for-downstream-tasks">Fine-Tuning for Downstream Tasks</h3>
<p><img src="img/lecture26/utilize learn model.png" id="fig:comparison" style="width:70.0%" alt="image" /> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p>
<ul>
<li><p><strong>Input:</strong></p>
<ul>
<li><p>Pre-trained network <span class="math inline">\(h_{\theta^*}\)</span>.</p></li>
<li><p>Labeled data <span class="math inline">\((x_1, \dots, x_M), (y_1, \dots, y_M)\)</span>.</p></li>
</ul></li>
<li><p><strong>Process:</strong></p>
<ul>
<li><p>Use <span class="math inline">\(h_{\theta^*}\)</span> as a feature extractor or initialize the weights for fine-tuning.</p></li>
<li><p>Attach a task-specific head (e.g., <span class="math inline">\(g_\phi\)</span> for classification) and train on the labeled data to minimize <span class="math inline">\(L(\hat{y}, y)\)</span>, where <span class="math inline">\(\hat{y}\)</span> is the prediction and <span class="math inline">\(y\)</span> is the ground-truth label.</p></li>
</ul></li>
<li><p><strong>Output:</strong></p>
<ul>
<li><p>A fine-tuned model for the downstream task (original task like predicting the animal category).</p></li>
</ul></li>
</ul>
</section>
<section id="different-pre-text-tasks-in-self-supervised-learning" data-number="0.4">
<h2 data-number="1.4"><span class="header-section-number">1.4</span> Different Pre-text Tasks in Self-Supervised Learning</h2>
<p>Different pre-text tasks enable the model to first learn various features of the images beyond the primary task, such as rotation, masking, brightness, noise, etc., to prepare for downstream prediction tasks. Unlike unsupervised learning, this approach avoids blind feature extraction by leveraging targeted auxiliary tasks.</p>
<section id="transformation-prediction" data-number="0.4.1">
<h3 data-number="1.4.1"><span class="header-section-number">1.4.1</span> Transformation Prediction</h3>
<p><img src="img/lecture26/tran.png" id="fig:comparison" style="width:70.0%" alt="image" /> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p>
<ul>
<li><p>Pre-text task performs some transformation on data and tasks the model with trying to learn the nature of the transformation.</p></li>
<li><p>Example: Predict image rotation angles (e.g., 90°, 180°, 270°).</p></li>
</ul>
</section>
<section id="masked-prediction" data-number="0.4.2">
<h3 data-number="1.4.2"><span class="header-section-number">1.4.2</span> Masked Prediction</h3>
<p><img src="img/lecture26/mask.png" id="fig:comparison" style="width:70.0%" alt="image" /> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p>
<ul>
<li><p>Pre-text task removes some part of the data, and the model is tasked with trying to predict what was removed.</p></li>
<li><p>Example: Mask a specific region in an image and predict the missing region using an encoder-decoder structure.</p></li>
</ul>
</section>
<section id="deep-clustering" data-number="0.4.3">
<h3 data-number="1.4.3"><span class="header-section-number">1.4.3</span> Deep Clustering</h3>
<p><img src="img/lecture26/cluster.png" id="fig:comparison" style="width:70.0%" alt="image" /> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p>
<ul>
<li><p>Identify clusters of features and iteratively assign pseudo-labels to train the model.</p></li>
<li><p>Process:</p>
<ul>
<li><p>Extract features from unlabeled data.</p></li>
<li><p>Cluster the extracted features.</p></li>
<li><p>Use cluster assignments as pseudo-labels for supervised training.</p></li>
</ul></li>
</ul>
</section>
<section id="contrastive-learning" data-number="0.4.4">
<h3 data-number="1.4.4"><span class="header-section-number">1.4.4</span> Contrastive Learning</h3>
<p><img src="img/lecture26/cont.png" id="fig:comparison" style="width:70.0%" alt="image" /> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p>
<ul>
<li><p>Pre-text task identifies positive and negative pairs of data, and the model is tasked with learning similarities to discriminate between positive and negative pairs.</p></li>
<li><p>Process:</p>
<ul>
<li><p>Positive pairs: Different augmentations of the same data point.</p></li>
<li><p>Negative pairs: Augmentations from different data points.</p></li>
<li><p>Optimize a contrastive loss to maximize similarity within positive pairs and minimize it for negative pairs.</p></li>
<li><p>This Contrastive Learning is so important, and will be detailly talked about in following part.</p></li>
</ul></li>
</ul>
</section>
</section>
<section id="contrastive-learning-frameworks" data-number="0.5">
<h2 data-number="1.5"><span class="header-section-number">1.5</span> Contrastive Learning Frameworks</h2>
<section id="simclr-framework" data-number="0.5.1">
<h3 data-number="1.5.1"><span class="header-section-number">1.5.1</span> SimCLR Framework</h3>
<p><img src="img/lecture26/SimCLR_Framework.png" id="fig:comparison" style="width:70.0%" alt="image" /> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p>
<p>SimCLR is a framework for contrastive learning that works by creating and learning from positive-negative pairs within batches. The process involves:</p>
<ol>
<li><p><strong>Image Augmentation</strong>: Generate similar pairs from the initial batch by applying augmentations to each image.</p></li>
<li><p><strong>Encoding</strong>: Pass both original and augmented images through an encoder to obtain lower-dimensional representations (<span class="math inline">\(h_i\)</span> and <span class="math inline">\(h_j\)</span>).</p></li>
<li><p><strong>Projection</strong>: Further compress these representations using a projection head to create embeddings (<span class="math inline">\(z_i\)</span> and <span class="math inline">\(z_j\)</span>).</p></li>
<li><p><strong>Similarity Calculation</strong>: Compute cosine similarity between the generated embeddings.</p></li>
<li><p><strong>Loss Computation</strong>: Calculate noise contrastive estimation loss for image pairs.</p></li>
<li><p><strong>Batch Processing</strong>: Average the loss across all pairs in the batch.</p></li>
</ol>
</section>
<section id="performance-comparison-contrastive-vs.-supervised-learning" data-number="0.5.2">
<h3 data-number="1.5.2"><span class="header-section-number">1.5.2</span> Performance Comparison: Contrastive vs. Supervised Learning</h3>
<p><img src="img/lecture26/performance_comparison.png" id="fig:comparison" style="width:70.0%" alt="image" /> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p>
<ul>
<li><p>Recent developments show that contrastive learning algorithms like SimCLR have, in some cases, surpassed supervised learning performance.</p></li>
<li><p>Self-supervision at scale has become increasingly important.</p></li>
<li><p><strong>Modern Approach</strong>: Utilize self-supervision for feature understanding, followed by supervised learning.</p></li>
<li><p>Large language models typically follow this pattern: initial self-supervised training followed by supervised fine-tuning.</p></li>
</ul>
</section>
</section>
<section id="variations-in-contrastive-learning-methods-and-applications" data-number="0.6">
<h2 data-number="1.6"><span class="header-section-number">1.6</span> Variations in Contrastive Learning Methods and Applications</h2>
<section id="core-concept" data-number="0.6.1">
<h3 data-number="1.6.1"><span class="header-section-number">1.6.1</span> Core Concept</h3>
<p>The main differentiating factor between various contrastive learning methods is how they generate positive and negative pairs. Several creative approaches have emerged for constructing these pairs based on different contexts and requirements.</p>
</section>
<section id="example-applications" data-number="0.6.2">
<h3 data-number="1.6.2"><span class="header-section-number">1.6.2</span> Example Applications</h3>
<ol>
<li><p><strong>Fisheye Images</strong></p>
<ul>
<li><p><strong>Approach</strong>: Treats regions within fisheye images as distinct classes.</p>
<p><img src="img/lecture26/fisheye_region.png" id="fig:comparison" style="width:70.0%" alt="image" /> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p></li>
<li><p><strong>Loss Components</strong>:</p>
<ul>
<li><p><span class="math inline">\(L_{\text{class}}\)</span>: Objects of the same class are positives regardless of position.</p></li>
<li><p><span class="math inline">\(L_{\text{region class}}\)</span>: Objects in the same region are positives regardless of class.</p></li>
</ul></li>
<li><p><strong>Combined Loss</strong>: <span class="math inline">\(\alpha L_{\text{class}} + (1-\alpha) L_{\text{region class}}\)</span></p></li>
<li><p><strong>Alternative Partitioning Methods</strong>:</p>
<ul>
<li><p>Square-based divisions</p></li>
<li><p>Radial partitioning</p></li>
<li><p>Grid-wise (9-class) segmentation</p></li>
</ul>
<p><img src="img/lecture26/fisheye_alternative.png" id="fig:comparison" style="width:70.0%" alt="image" /> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p></li>
</ul></li>
<li><p><strong>Seismic Images</strong></p>
<p><img src="img/lecture26/seismic.png" id="fig:comparison" style="width:70.0%" alt="image" /> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p>
<ul>
<li><p><strong>Pre-text Task</strong>: Volume label classification.</p></li>
<li><p><strong>Process</strong>:</p>
<ol>
<li><p>Partition the full seismic volume (generally <span class="math inline">\(100 \text{ km} \times 100 \text{ km} \times 25\)</span> km).</p></li>
<li><p>Divide into <span class="math inline">\(N\)</span> equal sub-volumes.</p></li>
<li><p>Assign volume labels to slice groups.</p></li>
</ol></li>
</ul></li>
<li><p><strong>Medical Images</strong></p>
<p><img src="img/lecture26/medical.png" id="fig:comparison" style="width:70.0%" alt="image" /> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p>
<ul>
<li><p><strong>Pre-text Task</strong>: Classification based on patient ID or clinical labels.</p></li>
<li><p><strong>Positive-Negative Pairs</strong>: Determined by patient identity or clinical characteristics.</p></li>
</ul></li>
</ol>
</section>
<section id="contrastive-learning-in-other-modalities" data-number="0.6.3">
<h3 data-number="1.6.3"><span class="header-section-number">1.6.3</span> Contrastive Learning in Other Modalities</h3>
<ul>
<li><p>Beyond computer vision, contrastive learning has been successfully applied to:</p>
<ul>
<li><p>Textual models (e.g., NLP)</p></li>
<li><p>Audio processing models</p></li>
</ul>
<p><img src="img/lecture26/other_modalities.png" id="fig:comparison" style="width:70.0%" alt="image" /> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p></li>
<li><p>This demonstrates the versatility of contrastive learning across different data types and applications.</p></li>
</ul>
</section>
</section>
<section id="foundation-models" data-number="0.7">
<h2 data-number="1.7"><span class="header-section-number">1.7</span> Foundation Models</h2>
<section id="recent-developments" data-number="0.7.1">
<h3 data-number="1.7.1"><span class="header-section-number">1.7.1</span> Recent Developments</h3>
<ul>
<li><p><strong>Segment Anything Model (SAM)</strong>:</p>
<p><img src="img/lecture26/sam.png" id="fig:comparison" style="width:70.0%" alt="image" /> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p>
<ul>
<li><p>Released by Meta (April 2023).</p></li>
<li><p>Trained on 1.1 billion segmentation masks from 11 million images.</p></li>
</ul></li>
</ul>
</section>
<section id="historical-evolution" data-number="0.7.2">
<h3 data-number="1.7.2"><span class="header-section-number">1.7.2</span> Historical Evolution</h3>
<ul>
<li><p><strong>Pre-2019</strong>:</p>
<ul>
<li><p>Primary architectures: ResNets, VGG.</p></li>
</ul></li>
<li><p><strong>Post-2019</strong>:</p>
<ul>
<li><p>New architectures: BERT, DALL-E, GPT, Flamingo.</p></li>
<li><p>Key changes: Introduction of transformer architectures and self-supervision.</p></li>
</ul></li>
</ul>
</section>
<section id="applications-and-impact" data-number="0.7.3">
<h3 data-number="1.7.3"><span class="header-section-number">1.7.3</span> Applications and Impact</h3>
<p><img src="img/lecture26/foundation_model.png" id="fig:comparison" style="width:70.0%" alt="image" /> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p>
<ul>
<li><p>Foundation models leverage transfer learning at scale.</p></li>
<li><p>Scale enables emergence of common properties across tasks.</p></li>
<li><p><strong>Potential Applications</strong>:</p>
<ul>
<li><p>Healthcare</p></li>
<li><p>Embodied interactive perception</p></li>
<li><p>Visual knowledge distillation</p></li>
<li><p>Temporal and commonsense reasoning</p></li>
</ul></li>
</ul>
</section>
</section>
<section id="key-insights-and-conclusions" data-number="0.8">
<h2 data-number="1.8"><span class="header-section-number">1.8</span> Key Insights and Conclusions</h2>
<section id="pseudo-label-generation" data-number="0.8.1">
<h3 data-number="1.8.1"><span class="header-section-number">1.8.1</span> Pseudo-Label Generation</h3>
<ul>
<li><p>Relies heavily on creativity.</p></li>
<li><p>Must maintain a relationship with original labels.</p></li>
<li><p>Feature extraction between pseudo and original labels should yield similar or related features.</p></li>
</ul>
</section>
<section id="future-directions" data-number="0.8.2">
<h3 data-number="1.8.2"><span class="header-section-number">1.8.2</span> Future Directions</h3>
<ul>
<li><p>Self-supervision at scale has become a crucial component in modern AI systems.</p></li>
<li><p>Continued evolution of foundation models and their applications.</p></li>
</ul>
</section>
</section>
</body>
</html>

</main>
</body>
</html>
