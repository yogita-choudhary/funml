<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>ECE4252-8803_Notes_Template</title>
  <link rel="stylesheet" href="../assets/style.css"/>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav>
  <a href="../index.html">Home</a>
</nav>
<main>
<div class="center">
</div>





<section data-number="0.1" id="recap-of-superviesd-learning">
<h2 data-number="1.1"><span class="header-section-number">1.1</span> Recap of Superviesd Learning</h2>
<p>Supervised learning relies on labeled data to train models. It faces several challenges, leading to the exploration of alternative learning paradigms such as unsupervised and self-supervised learning.</p>
<ul>
<li><p>High cost and time required for labeling large datasets.</p></li>
<li><p>Dependence on trained experts for specialized labeling tasks (e.g., medical or seismic data).</p></li>
<li><p>Data privacy regulations and restrictions, such as HIPAA, that limit access to labeled data.</p></li>
<li><p>Unreleased datasets and limited availability of high-quality labeled data.</p></li>
</ul>
</section>
<section data-number="0.2" id="comparison-of-data-requirements-and-structures-summary-supervised-unsupervised-self-supervised">
<h2 data-number="1.2"><span class="header-section-number">1.2</span> Comparison of Data Requirements and Structures Summary (supervised, unsupervised, Self-supervised):</h2>
<p><img alt="image" id="fig:comparison" src="img/lecture26/S US SSL.png" style="width:50.0%"/> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p>
<h3 class="unnumbered" id="supervised-learning">Supervised Learning</h3>
<ul>
<li><p>Requires labeled datasets <span class="math inline">\((x, y)\)</span>, where <span class="math inline">\(x\)</span> represents input data and <span class="math inline">\(y\)</span> represents ground-truth labels.</p></li>
<li><p>The model directly learns a mapping <span class="math inline">\(f(x) \rightarrow y\)</span> to minimize the loss function <span class="math inline">\(L\)</span>.</p></li>
</ul>
<h3 class="unnumbered" id="unsupervised-learning">Unsupervised Learning</h3>
<ul>
<li><p>Requires only unlabeled data <span class="math inline">\(x\)</span>.</p></li>
<li><p>The model <span class="math inline">\(f\)</span> learns the internal structure or distribution of the data and maps the input <span class="math inline">\(x\)</span> to a reconstructed output <span class="math inline">\(\hat{x}\)</span>: <span class="math inline">\(f(x) \to \hat{x}\)</span>.</p></li>
<li><p>The loss <span class="math inline">\(L(\hat{x}, x)\)</span> is computed based on reconstruction error or other objectives, guiding the model to better represent the characteristics of the data.</p></li>
</ul>
<h3 class="unnumbered" id="self-supervised-learning">Self-Supervised Learning</h3>
<ul>
<li><p>Relies data <span class="math inline">\(x\)</span> and on partially labeled data <span class="math inline">\(y\)</span>, and introduces <em>pre-text tasks</em> to generate pseudo-labels <span class="math inline">\(z\)</span>.</p></li>
<li><p>The model first learns a pre-text representation using pseudo-labels and then adapts the learned features for downstream tasks with limited data <span class="math inline">\(y\)</span> (e.g., classification).</p></li>
<li><p>The "Pre-text Tasks" are what unique for Self-Supervised Learning than Unsupervised Learning</p></li>
</ul>
</section>
<section data-number="0.3" id="self-supervised-learning-ssl-structure">
<h2 data-number="1.3"><span class="header-section-number">1.3</span> Self-Supervised Learning (SSL) Structure</h2>
<p>Self-supervised learning (SSL) utilizes both labeled and unlabeled data to generate pseudo-labels and learn representations through pre-text tasks. Its structure can be summarized in three key steps:</p>
<h3 class="unnumbered" id="generate-pseudo-labels">Generate Pseudo-Labels</h3>
<p><img alt="image" id="fig:comparison" src="img/lecture26/gen pseudo lab.png" style="width:70.0%"/> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p>
<ul>
<li><p><strong>Input:</strong></p>
<ul>
<li><p>Unlabeled data <span class="math inline">\((x_1, \dots, x_N)\)</span>.</p></li>
<li><p>Optionally, a small amount of labeled data <span class="math inline">\((x_1, \dots, x_M), (y_1, \dots, y_M)\)</span>.</p></li>
</ul></li>
<li><p><strong>Process:</strong></p>
<ul>
<li><p>Use a pre-text task <span class="math inline">\(P\)</span> (e.g., rotation prediction, contrastive learning) to generate pseudo-labels <span class="math inline">\((z_1, \dots, z_N)\)</span> for the unlabeled data.</p></li>
</ul></li>
<li><p><strong>Output:</strong></p>
<ul>
<li><p>Pseudo-labeled data: <span class="math inline">\((x_1, \dots, x_N), (z_1, \dots, z_N)\)</span>.</p></li>
</ul></li>
</ul>
<h3 class="unnumbered" id="pre-training-on-pseudo-labels">Pre-Training on Pseudo-Labels</h3>
<p><img alt="image" id="fig:comparison" src="img/lecture26/pre train on pse.png" style="width:70.0%"/> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p>
<ul>
<li><p><strong>Input:</strong></p>
<ul>
<li><p>Pseudo-labeled data <span class="math inline">\((x_1, \dots, x_N), (z_1, \dots, z_N)\)</span>.</p></li>
</ul></li>
<li><p><strong>Process:</strong></p>
<ul>
<li><p>Train a neural network <span class="math inline">\(h_\theta\)</span> using the pseudo-labels.</p></li>
<li><p>Minimize the loss <span class="math inline">\(L(z, \hat{z})\)</span>, where <span class="math inline">\(z\)</span> is the pseudo-label and <span class="math inline">\(\hat{z}\)</span> is the model prediction.</p></li>
</ul></li>
<li><p><strong>Output:</strong></p>
<ul>
<li><p>A pre-trained representation <span class="math inline">\(h_\theta\)</span> capturing patterns in the unlabeled data.</p></li>
</ul></li>
</ul>
<h3 class="unnumbered" id="fine-tuning-for-downstream-tasks">Fine-Tuning for Downstream Tasks</h3>
<p><img alt="image" id="fig:comparison" src="img/lecture26/utilize learn model.png" style="width:70.0%"/> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p>
<ul>
<li><p><strong>Input:</strong></p>
<ul>
<li><p>Pre-trained network <span class="math inline">\(h_{\theta^*}\)</span>.</p></li>
<li><p>Labeled data <span class="math inline">\((x_1, \dots, x_M), (y_1, \dots, y_M)\)</span>.</p></li>
</ul></li>
<li><p><strong>Process:</strong></p>
<ul>
<li><p>Use <span class="math inline">\(h_{\theta^*}\)</span> as a feature extractor or initialize the weights for fine-tuning.</p></li>
<li><p>Attach a task-specific head (e.g., <span class="math inline">\(g_\phi\)</span> for classification) and train on the labeled data to minimize <span class="math inline">\(L(\hat{y}, y)\)</span>, where <span class="math inline">\(\hat{y}\)</span> is the prediction and <span class="math inline">\(y\)</span> is the ground-truth label.</p></li>
</ul></li>
<li><p><strong>Output:</strong></p>
<ul>
<li><p>A fine-tuned model for the downstream task (original task like predicting the animal category).</p></li>
</ul></li>
</ul>
</section>
<section data-number="0.4" id="different-pre-text-tasks-in-self-supervised-learning">
<h2 data-number="1.4"><span class="header-section-number">1.4</span> Different Pre-text Tasks in Self-Supervised Learning</h2>
<p>Different pre-text tasks enable the model to first learn various features of the images beyond the primary task, such as rotation, masking, brightness, noise, etc., to prepare for downstream prediction tasks. Unlike unsupervised learning, this approach avoids blind feature extraction by leveraging targeted auxiliary tasks.</p>
<section data-number="0.4.1" id="transformation-prediction">
<h3 data-number="1.4.1"><span class="header-section-number">1.4.1</span> Transformation Prediction</h3>
<p><img alt="image" id="fig:comparison" src="img/lecture26/tran.png" style="width:70.0%"/> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p>
<ul>
<li><p>Pre-text task performs some transformation on data and tasks the model with trying to learn the nature of the transformation.</p></li>
<li><p>Example: Predict image rotation angles (e.g., 90°, 180°, 270°).</p></li>
</ul>
</section>
<section data-number="0.4.2" id="masked-prediction">
<h3 data-number="1.4.2"><span class="header-section-number">1.4.2</span> Masked Prediction</h3>
<p><img alt="image" id="fig:comparison" src="img/lecture26/mask.png" style="width:70.0%"/> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p>
<ul>
<li><p>Pre-text task removes some part of the data, and the model is tasked with trying to predict what was removed.</p></li>
<li><p>Example: Mask a specific region in an image and predict the missing region using an encoder-decoder structure.</p></li>
</ul>
</section>
<section data-number="0.4.3" id="deep-clustering">
<h3 data-number="1.4.3"><span class="header-section-number">1.4.3</span> Deep Clustering</h3>
<p><img alt="image" id="fig:comparison" src="img/lecture26/cluster.png" style="width:70.0%"/> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p>
<ul>
<li><p>Identify clusters of features and iteratively assign pseudo-labels to train the model.</p></li>
<li><p>Process:</p>
<ul>
<li><p>Extract features from unlabeled data.</p></li>
<li><p>Cluster the extracted features.</p></li>
<li><p>Use cluster assignments as pseudo-labels for supervised training.</p></li>
</ul></li>
</ul>
</section>
<section data-number="0.4.4" id="contrastive-learning">
<h3 data-number="1.4.4"><span class="header-section-number">1.4.4</span> Contrastive Learning</h3>
<p><img alt="image" id="fig:comparison" src="img/lecture26/cont.png" style="width:70.0%"/> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p>
<ul>
<li><p>Pre-text task identifies positive and negative pairs of data, and the model is tasked with learning similarities to discriminate between positive and negative pairs.</p></li>
<li><p>Process:</p>
<ul>
<li><p>Positive pairs: Different augmentations of the same data point.</p></li>
<li><p>Negative pairs: Augmentations from different data points.</p></li>
<li><p>Optimize a contrastive loss to maximize similarity within positive pairs and minimize it for negative pairs.</p></li>
<li><p>This Contrastive Learning is so important, and will be detailly talked about in following part.</p></li>
</ul></li>
</ul>
</section>
</section>
<section data-number="0.5" id="contrastive-learning-frameworks">
<h2 data-number="1.5"><span class="header-section-number">1.5</span> Contrastive Learning Frameworks</h2>
<section data-number="0.5.1" id="simclr-framework">
<h3 data-number="1.5.1"><span class="header-section-number">1.5.1</span> SimCLR Framework</h3>
<p><img alt="image" id="fig:comparison" src="img/lecture26/SimCLR_Framework.png" style="width:70.0%"/> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p>
<p>SimCLR is a framework for contrastive learning that works by creating and learning from positive-negative pairs within batches. The process involves:</p>
<ol>
<li><p><strong>Image Augmentation</strong>: Generate similar pairs from the initial batch by applying augmentations to each image.</p></li>
<li><p><strong>Encoding</strong>: Pass both original and augmented images through an encoder to obtain lower-dimensional representations (<span class="math inline">\(h_i\)</span> and <span class="math inline">\(h_j\)</span>).</p></li>
<li><p><strong>Projection</strong>: Further compress these representations using a projection head to create embeddings (<span class="math inline">\(z_i\)</span> and <span class="math inline">\(z_j\)</span>).</p></li>
<li><p><strong>Similarity Calculation</strong>: Compute cosine similarity between the generated embeddings.</p></li>
<li><p><strong>Loss Computation</strong>: Calculate noise contrastive estimation loss for image pairs.</p></li>
<li><p><strong>Batch Processing</strong>: Average the loss across all pairs in the batch.</p></li>
</ol>
</section>
<section data-number="0.5.2" id="performance-comparison-contrastive-vs.-supervised-learning">
<h3 data-number="1.5.2"><span class="header-section-number">1.5.2</span> Performance Comparison: Contrastive vs. Supervised Learning</h3>
<p><img alt="image" id="fig:comparison" src="img/lecture26/performance_comparison.png" style="width:70.0%"/> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p>
<ul>
<li><p>Recent developments show that contrastive learning algorithms like SimCLR have, in some cases, surpassed supervised learning performance.</p></li>
<li><p>Self-supervision at scale has become increasingly important.</p></li>
<li><p><strong>Modern Approach</strong>: Utilize self-supervision for feature understanding, followed by supervised learning.</p></li>
<li><p>Large language models typically follow this pattern: initial self-supervised training followed by supervised fine-tuning.</p></li>
</ul>
</section>
</section>
<section data-number="0.6" id="variations-in-contrastive-learning-methods-and-applications">
<h2 data-number="1.6"><span class="header-section-number">1.6</span> Variations in Contrastive Learning Methods and Applications</h2>
<section data-number="0.6.1" id="core-concept">
<h3 data-number="1.6.1"><span class="header-section-number">1.6.1</span> Core Concept</h3>
<p>The main differentiating factor between various contrastive learning methods is how they generate positive and negative pairs. Several creative approaches have emerged for constructing these pairs based on different contexts and requirements.</p>
</section>
<section data-number="0.6.2" id="example-applications">
<h3 data-number="1.6.2"><span class="header-section-number">1.6.2</span> Example Applications</h3>
<ol>
<li><p><strong>Fisheye Images</strong></p>
<ul>
<li><p><strong>Approach</strong>: Treats regions within fisheye images as distinct classes.</p>
<p><img alt="image" id="fig:comparison" src="img/lecture26/fisheye_region.png" style="width:70.0%"/> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p></li>
<li><p><strong>Loss Components</strong>:</p>
<ul>
<li><p><span class="math inline">\(L_{\text{class}}\)</span>: Objects of the same class are positives regardless of position.</p></li>
<li><p><span class="math inline">\(L_{\text{region class}}\)</span>: Objects in the same region are positives regardless of class.</p></li>
</ul></li>
<li><p><strong>Combined Loss</strong>: <span class="math inline">\(\alpha L_{\text{class}} + (1-\alpha) L_{\text{region class}}\)</span></p></li>
<li><p><strong>Alternative Partitioning Methods</strong>:</p>
<ul>
<li><p>Square-based divisions</p></li>
<li><p>Radial partitioning</p></li>
<li><p>Grid-wise (9-class) segmentation</p></li>
</ul>
<p><img alt="image" id="fig:comparison" src="img/lecture26/fisheye_alternative.png" style="width:70.0%"/> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p></li>
</ul></li>
<li><p><strong>Seismic Images</strong></p>
<p><img alt="image" id="fig:comparison" src="img/lecture26/seismic.png" style="width:70.0%"/> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p>
<ul>
<li><p><strong>Pre-text Task</strong>: Volume label classification.</p></li>
<li><p><strong>Process</strong>:</p>
<ol>
<li><p>Partition the full seismic volume (generally <span class="math inline">\(100 \text{ km} \times 100 \text{ km} \times 25\)</span> km).</p></li>
<li><p>Divide into <span class="math inline">\(N\)</span> equal sub-volumes.</p></li>
<li><p>Assign volume labels to slice groups.</p></li>
</ol></li>
</ul></li>
<li><p><strong>Medical Images</strong></p>
<p><img alt="image" id="fig:comparison" src="img/lecture26/medical.png" style="width:70.0%"/> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p>
<ul>
<li><p><strong>Pre-text Task</strong>: Classification based on patient ID or clinical labels.</p></li>
<li><p><strong>Positive-Negative Pairs</strong>: Determined by patient identity or clinical characteristics.</p></li>
</ul></li>
</ol>
</section>
<section data-number="0.6.3" id="contrastive-learning-in-other-modalities">
<h3 data-number="1.6.3"><span class="header-section-number">1.6.3</span> Contrastive Learning in Other Modalities</h3>
<ul>
<li><p>Beyond computer vision, contrastive learning has been successfully applied to:</p>
<ul>
<li><p>Textual models (e.g., NLP)</p></li>
<li><p>Audio processing models</p></li>
</ul>
<p><img alt="image" id="fig:comparison" src="img/lecture26/other_modalities.png" style="width:70.0%"/> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p></li>
<li><p>This demonstrates the versatility of contrastive learning across different data types and applications.</p></li>
</ul>
</section>
</section>
<section data-number="0.7" id="foundation-models">
<h2 data-number="1.7"><span class="header-section-number">1.7</span> Foundation Models</h2>
<section data-number="0.7.1" id="recent-developments">
<h3 data-number="1.7.1"><span class="header-section-number">1.7.1</span> Recent Developments</h3>
<ul>
<li><p><strong>Segment Anything Model (SAM)</strong>:</p>
<p><img alt="image" id="fig:comparison" src="img/lecture26/sam.png" style="width:70.0%"/> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p>
<ul>
<li><p>Released by Meta (April 2023).</p></li>
<li><p>Trained on 1.1 billion segmentation masks from 11 million images.</p></li>
</ul></li>
</ul>
</section>
<section data-number="0.7.2" id="historical-evolution">
<h3 data-number="1.7.2"><span class="header-section-number">1.7.2</span> Historical Evolution</h3>
<ul>
<li><p><strong>Pre-2019</strong>:</p>
<ul>
<li><p>Primary architectures: ResNets, VGG.</p></li>
</ul></li>
<li><p><strong>Post-2019</strong>:</p>
<ul>
<li><p>New architectures: BERT, DALL-E, GPT, Flamingo.</p></li>
<li><p>Key changes: Introduction of transformer architectures and self-supervision.</p></li>
</ul></li>
</ul>
</section>
<section data-number="0.7.3" id="applications-and-impact">
<h3 data-number="1.7.3"><span class="header-section-number">1.7.3</span> Applications and Impact</h3>
<p><img alt="image" id="fig:comparison" src="img/lecture26/foundation_model.png" style="width:70.0%"/> <span id="fig:comparison" label="fig:comparison">[fig:comparison]</span></p>
<ul>
<li><p>Foundation models leverage transfer learning at scale.</p></li>
<li><p>Scale enables emergence of common properties across tasks.</p></li>
<li><p><strong>Potential Applications</strong>:</p>
<ul>
<li><p>Healthcare</p></li>
<li><p>Embodied interactive perception</p></li>
<li><p>Visual knowledge distillation</p></li>
<li><p>Temporal and commonsense reasoning</p></li>
</ul></li>
</ul>
</section>
</section>
<section data-number="0.8" id="key-insights-and-conclusions">
<h2 data-number="1.8"><span class="header-section-number">1.8</span> Key Insights and Conclusions</h2>
<section data-number="0.8.1" id="pseudo-label-generation">
<h3 data-number="1.8.1"><span class="header-section-number">1.8.1</span> Pseudo-Label Generation</h3>
<ul>
<li><p>Relies heavily on creativity.</p></li>
<li><p>Must maintain a relationship with original labels.</p></li>
<li><p>Feature extraction between pseudo and original labels should yield similar or related features.</p></li>
</ul>
</section>
<section data-number="0.8.2" id="future-directions">
<h3 data-number="1.8.2"><span class="header-section-number">1.8.2</span> Future Directions</h3>
<ul>
<li><p>Self-supervision at scale has become a crucial component in modern AI systems.</p></li>
<li><p>Continued evolution of foundation models and their applications.</p></li>
</ul>
</section>
</section>

</main>
</body>
</html>
