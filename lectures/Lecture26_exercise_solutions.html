<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>Lecture26 In-class Exercise Solutions</title>
  <link rel="stylesheet" href="../assets/style.css"/>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav>
  <a href="../index.html">Home</a>
</nav>
<main>
<div class="center">
<p><span><strong>In-Class Exercise — Questions + Solutions</strong></span><br />
<span><strong>Lecture 26: Self-Supervised Learning</strong></span></p>
</div>
<ol>
<li><p><strong>Core SSL idea (pseudo-labels).</strong><br />
A pre-text task <span class="math inline">\(P(\cdot)\)</span> transforms an unlabeled input <span class="math inline">\(\boldsymbol{x}\)</span> into a pseudo-label. Which option best describes <em>why</em> pseudo-labels are useful in SSL?</p>
<ol>
<li><p>They replace ground-truth labels and guarantee perfect downstream accuracy.</p></li>
<li><p>They create a training signal that encourages learning features that transfer to downstream tasks.</p></li>
<li><p>They are only used to compress data and are not used for representation learning.</p></li>
<li><p>They remove the need for augmentations in contrastive learning.</p></li>
</ol>
<p><strong>Solution.</strong><br />
Pseudo-labels are not “true” labels; their purpose is to define a learnable objective that forces the network to extract structure from unlabeled data in a way that can transfer to downstream tasks.</p></li>
<li><p><strong>Representation vs embedding (SimCLR design choice).</strong><br />
SimCLR uses <span class="math inline">\(\boldsymbol{h}=f_\theta(\boldsymbol{x})\)</span> and then <span class="math inline">\(\boldsymbol{z}=g_\phi(\boldsymbol{h})\)</span> for the contrastive loss. Why is it beneficial to apply the contrastive objective on <span class="math inline">\(\boldsymbol{z}\)</span> rather than directly on <span class="math inline">\(\boldsymbol{h}\)</span>?</p>
<ol>
<li><p>It prevents the encoder from learning any transferable information.</p></li>
<li><p>It lets the encoder representation <span class="math inline">\(\boldsymbol{h}\)</span> remain useful for downstream tasks while <span class="math inline">\(\boldsymbol{z}\)</span> specializes for the contrastive objective.</p></li>
<li><p>It is required because cosine similarity cannot be computed on <span class="math inline">\(\boldsymbol{h}\)</span>.</p></li>
<li><p>It makes positive pairs become negatives during training.</p></li>
</ol>
<p><strong>Solution.</strong><br />
The projection head <span class="math inline">\(g_\phi\)</span> can absorb task-specific distortions needed to optimize contrastive separation, while the encoder output <span class="math inline">\(\boldsymbol{h}\)</span> is kept as a more general representation for downstream use.</p></li>
<li><p><strong>What the loss is pushing (conceptual geometry).</strong><br />
For an anchor view <span class="math inline">\(i\)</span>, SimCLR treats its paired view <span class="math inline">\(j\)</span> as positive and all other views as negatives. Which statement best matches the intended effect of the contrastive loss on the embeddings?</p>
<ol>
<li><p>Increase <span class="math inline">\(\mathrm{sim}(\boldsymbol{z}_i,\boldsymbol{z}_j)\)</span> and decrease <span class="math inline">\(\mathrm{sim}(\boldsymbol{z}_i,\boldsymbol{z}_k)\)</span> for negatives <span class="math inline">\(k\)</span>.</p></li>
<li><p>Decrease all similarities so embeddings become orthogonal to everything.</p></li>
<li><p>Increase similarity to all views so all embeddings collapse to the same point.</p></li>
<li><p>Maximize the norm <span class="math inline">\(\|\boldsymbol{z}_i\|\)</span> while ignoring pairwise similarities.</p></li>
</ol>
<p><strong>Solution.</strong><br />
The core contrastive goal is <em>relative</em>: pull positives closer while pushing negatives away, so the embedding space separates instances by identity under augmentation.</p></li>
<li><p><strong>Negatives and batch effects (reasoning).</strong><br />
Consider a fixed augmentation pipeline and a fixed encoder <span class="math inline">\(f_\theta\)</span>. If you increase the batch size <span class="math inline">\(N\)</span> (so there are more views and thus more negatives per anchor), which outcome is the most accurate <em>conceptual</em> expectation?</p>
<ol>
<li><p>The task becomes strictly easier because positives become more similar automatically.</p></li>
<li><p>The contrastive task typically becomes harder/more demanding because each anchor must be distinguished from more negatives.</p></li>
<li><p>The number of positives per anchor increases linearly with <span class="math inline">\(N\)</span>.</p></li>
<li><p>The model no longer needs the projection head <span class="math inline">\(g_\phi\)</span>.</p></li>
</ol>
<p><strong>Solution.</strong><br />
More negatives means the anchor must remain close to its positive <em>while staying separated from a larger set of alternatives</em>, which typically makes the discrimination problem more demanding.</p></li>
<li><p><strong>One quick sanity check (minimal counting).</strong><br />
In SimCLR with <span class="math inline">\(N=4\)</span> original inputs, there are <span class="math inline">\(2N=8\)</span> total views. For a given anchor view, how many positives and how many negatives are used in the denominator of the contrastive objective?</p>
<ol>
<li><p>1 positive, 5 negatives</p></li>
<li><p>1 positive, 6 negatives</p></li>
<li><p>2 positives, 5 negatives</p></li>
<li><p>2 positives, 6 negatives</p></li>
</ol>
<p><strong>Solution.</strong><br />
Each anchor has exactly 1 positive (the other view of the same instance). Excluding the anchor itself and its positive leaves <span class="math inline">\(2N-2 = 6\)</span> negatives.</p></li>
</ol>

</main>
</body>
</html>
