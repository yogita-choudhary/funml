<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>Lecture20 In-class Exercise</title>
  <link rel="stylesheet" href="../assets/style.css"/>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav>
  <a href="../index.html">Home</a>
</nav>
<main>
<div class="center">
<p><span><strong>In-Class Exercise (Canvas Quiz) â€” 10 minutes</strong></span><br />
<span><strong>Lecture 20: Sequence Modeling (RNN + Causal Convolution)</strong></span></p>
</div>
<p>Throughout this exercise, use the RNN update equations: <span class="math display">\[\mathbf{a}^{(t)}=\mathbf{b}+\mathbf{W}\mathbf{h}^{(t-1)}+\mathbf{U}\mathbf{x}^{(t)},
\qquad
\mathbf{h}^{(t)}=\tanh\!\left(\mathbf{a}^{(t)}\right).\]</span></p>
<ol>
<li><p><strong>RNN memory (concept).</strong><br />
Which statement best describes why an RNN can model temporal dependence?</p>
<ol>
<li><p>Each time step uses a different parameter set, so it learns a separate model per time.</p></li>
<li><p>The hidden state <span class="math inline">\(\mathbf{h}^{(t)}\)</span> summarizes past inputs and is reused at future time steps.</p></li>
<li><p>The model sees future inputs <span class="math inline">\(\mathbf{x}^{(t+1)},\mathbf{x}^{(t+2)}\)</span> when computing <span class="math inline">\(\mathbf{h}^{(t)}\)</span>.</p></li>
<li><p>The nonlinearity <span class="math inline">\(\tanh(\cdot)\)</span> alone creates temporal memory even without <span class="math inline">\(\mathbf{W}\mathbf{h}^{(t-1)}\)</span>.</p></li>
</ol></li>
<li><p><strong>Dependency tracing (concept).</strong><br />
Assume <span class="math inline">\(\mathbf{h}^{(0)}=\mathbf{0}\)</span>. For <span class="math inline">\(t=2\)</span>, which inputs can influence <span class="math inline">\(\mathbf{h}^{(2)}\)</span>?</p>
<ol>
<li><p>Only <span class="math inline">\(\mathbf{x}^{(2)}\)</span></p></li>
<li><p>Only <span class="math inline">\(\mathbf{x}^{(1)}\)</span></p></li>
<li><p>Both <span class="math inline">\(\mathbf{x}^{(1)}\)</span> and <span class="math inline">\(\mathbf{x}^{(2)}\)</span></p></li>
<li><p>Neither <span class="math inline">\(\mathbf{x}^{(1)}\)</span> nor <span class="math inline">\(\mathbf{x}^{(2)}\)</span></p></li>
</ol></li>
<li><p><strong>When does the RNN <em>forget</em>? (concept).</strong><br />
Consider the same update: <span class="math display">\[\mathbf{a}^{(t)}=\mathbf{b}+\mathbf{W}\mathbf{h}^{(t-1)}+\mathbf{U}\mathbf{x}^{(t)}.\]</span> Which condition most directly removes dependence on the past hidden state?</p>
<ol>
<li><p><span class="math inline">\(\mathbf{U}=\mathbf{0}\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{W}=\mathbf{0}\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{b}=\mathbf{0}\)</span></p></li>
<li><p>Replacing <span class="math inline">\(\tanh(\cdot)\)</span> with <span class="math inline">\(\mathrm{ReLU}(\cdot)\)</span></p></li>
</ol></li>
<li><p><strong>Causal convolution (core idea).</strong><br />
A 1D <em>causal</em> convolution is designed so that the output at time <span class="math inline">\(t\)</span> depends only on present/past inputs (not future inputs). For kernel length <span class="math inline">\(k\)</span>, how many zeros must be padded on the <em>left</em> to enforce causality?</p>
<ol>
<li><p><span class="math inline">\(k\)</span></p></li>
<li><p><span class="math inline">\(k-1\)</span></p></li>
<li><p><span class="math inline">\(k+1\)</span></p></li>
<li><p><span class="math inline">\(2k-1\)</span></p></li>
</ol></li>
<li><p><strong>Causal vs non-causal (concept check).</strong><br />
Which statement is correct?</p>
<ol>
<li><p>A causal convolution can use future inputs, but only during training.</p></li>
<li><p>A non-causal convolution may use <span class="math inline">\(\mathbf{x}^{(t+1)}\)</span> to compute the output at time <span class="math inline">\(t\)</span>.</p></li>
<li><p>Causality depends only on the activation function (e.g., <span class="math inline">\(\tanh\)</span> vs ReLU), not padding.</p></li>
<li><p>Causal convolutions require right-padding only.</p></li>
</ol></li>
</ol>

</main>
</body>
</html>
