<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>Lecture22 In-class Exercise Solutions</title>
  <link rel="stylesheet" href="../assets/style.css"/>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav>
  <a href="../index.html">Home</a>
</nav>
<main>
<div class="center">
<p><span><strong>In-Class Exercise â€” Solutions</strong></span><br />
<span><strong>Lecture 22: Explainability in Neural Networks</strong></span></p>
</div>
<p><strong>Given:</strong> <span class="math inline">\(p=0.85\)</span>, <span class="math inline">\(p_1=0.60\)</span>, <span class="math inline">\(p_2=0.82\)</span>, <span class="math inline">\(p_3=0.30\)</span>, and <span class="math display">\[\mathbf{A}^1=
\begin{bmatrix}
1 &amp; 2\\
0 &amp; 1
\end{bmatrix},
\quad
\mathbf{A}^2=
\begin{bmatrix}
2 &amp; 1\\
1 &amp; 0
\end{bmatrix},
\quad
\alpha_1^c=0.50,\quad
\alpha_2^c=-0.25.\]</span></p>
<ol>
<li><p><strong>Occlusion Saliency (Most Important Patch)</strong><br />
Which patch is <strong>most important</strong> for predicting class <span class="math inline">\(c\)</span> (largest probability drop)?</p>
<ol>
<li><p>Patch 1</p></li>
<li><p>Patch 2</p></li>
<li><p>Patch 3</p></li>
<li><p>All patches are equally important</p></li>
</ol>
<p><strong>Solution:</strong> Compute drops <span class="math inline">\(\Delta p_i = p - p_i\)</span>: <span class="math display">\[\Delta p_1 = 0.85-0.60=0.25,\quad
\Delta p_2 = 0.85-0.82=0.03,\quad
\Delta p_3 = 0.85-0.30=0.55.\]</span> Largest drop is Patch 3. <span class="math display">\[\boxed{\textbf{Answer: (C)}}\]</span></p></li>
<li><p><strong>Faithfulness vs Cost (Occlusion)</strong><br />
Why is occlusion saliency considered <strong>computationally expensive</strong>?</p>
<ol>
<li><p>It requires retraining the model.</p></li>
<li><p>It requires many forward passes with masked inputs.</p></li>
<li><p>It needs gradients of all layers.</p></li>
<li><p>It increases the number of model parameters.</p></li>
</ol>
<p><strong>Solution:</strong> Occlusion saliency tests many masked versions of the input and re-runs inference each time, i.e., many forward passes. <span class="math display">\[\boxed{\textbf{Answer: (B)}}\]</span></p></li>
<li><p><strong>Gradient Saliency (Concept)</strong><br />
Why do input gradients approximate pixel importance?</p>
<ol>
<li><p>They linearize the model locally around the input.</p></li>
<li><p>They guarantee causal explanations.</p></li>
<li><p>They remove nonlinearities from the network.</p></li>
<li><p>They compute feature correlations.</p></li>
</ol>
<p><strong>Solution:</strong> <span class="math inline">\(\frac{\partial y_c}{\partial \mathbf{x}}\)</span> measures local sensitivity: a first-order (Taylor) approximation of how changing pixels changes the class score. <span class="math display">\[\boxed{\textbf{Answer: (A)}}\]</span></p></li>
<li><p><strong>Why the Last Convolutional Layer?</strong><br />
Grad-CAM uses the <strong>last convolutional layer</strong> because it:</p>
<ol>
<li><p>Has the largest number of parameters.</p></li>
<li><p>Preserves spatial structure and captures high-level semantics.</p></li>
<li><p>Produces the final class probabilities.</p></li>
<li><p>Eliminates the need for backpropagation.</p></li>
</ol>
<p><strong>Solution:</strong> The last conv layer keeps spatial layout (so we can localize) and is deep enough to encode class-relevant semantics. <span class="math display">\[\boxed{\textbf{Answer: (B)}}\]</span></p></li>
<li><p><strong>Grad-CAM (After ReLU)</strong><br />
Let <span class="math inline">\(S=\alpha_1^c\mathbf{A}^1+\alpha_2^c\mathbf{A}^2\)</span>. After applying <span class="math inline">\(L_{\text{Grad-CAM}}^{c}=\mathrm{ReLU}(S)\)</span>, what happens to <strong>negative</strong> entries of <span class="math inline">\(S\)</span>?</p>
<ol>
<li><p>They become positive.</p></li>
<li><p>They remain negative.</p></li>
<li><p>They become zero.</p></li>
<li><p>They are doubled in magnitude.</p></li>
</ol>
<p><strong>Solution:</strong> ReLU is applied elementwise: <span class="math inline">\(\mathrm{ReLU}(z)=\max(0,z)\)</span>. Therefore any negative entries become <span class="math inline">\(0\)</span>. <span class="math display">\[\boxed{\textbf{Answer: (C)}}\]</span></p></li>
<li><p><strong>Concept Check (True/False)</strong><br />
Guided Backpropagation can produce sharp visual explanations but may fail to be <strong>class-discriminative</strong>.</p>
<ol>
<li><p>True</p></li>
<li><p>False</p></li>
</ol>
<p><strong>Solution:</strong> Guided Backprop often looks visually sharp but can highlight similar regions for different classes (not reliably class-discriminative). <span class="math display">\[\boxed{\textbf{Answer: (A) True}}\]</span></p></li>
</ol>

</main>
</body>
</html>
