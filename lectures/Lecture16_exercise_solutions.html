<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>Lecture16 In-class Exercise Solutions</title>
  <link rel="stylesheet" href="../assets/style.css"/>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav>
  <a href="../index.html">Home</a>
</nav>
<main>
<div class="center">
<p><span><strong>In-Class Exercise (Canvas Quiz) â€” 10 minutes</strong></span><br />
<span><strong>Lecture 16: Convolutional Neural Networks (Training)</strong></span></p>
</div>
<ol>
<li><p><strong>Highest computation cost per epoch:</strong><br />
You have a dataset with <span class="math inline">\(N=1{,}000{,}000\)</span> datapoints. Which method generally has the <strong>highest memory/computation cost per epoch</strong>?</p>
<ol>
<li><p>SGD</p></li>
<li><p>Mini-batch GD (MBGD)</p></li>
<li><p>Batch GD (BGD)</p></li>
</ol>
<p><strong>Solution 1:</strong><br />
<strong>(C) BGD</strong>. Batch gradient descent computes gradients using the entire dataset each epoch. <span class="math display">\[\boxed{\textbf{Answer: (C)}}\]</span></p></li>
<li><p><strong>Noisiest / highest variance update path:</strong><br />
Which method tends to have the <strong>noisiest / highest-variance update path</strong>?</p>
<ol>
<li><p>SGD</p></li>
<li><p>Mini-batch GD (MBGD)</p></li>
<li><p>Batch GD (BGD)</p></li>
</ol>
<p><strong>Solution 2:</strong><br />
<strong>(A) SGD</strong>. SGD uses a single datapoint per update, which produces high variance updates. <span class="math display">\[\boxed{\textbf{Answer: (A)}}\]</span></p></li>
<li><p><strong>SGD noise can help escape shallow minima / saddle points:</strong><br />
<strong>True/False:</strong> The stochasticity (noise) of SGD can sometimes help escape shallow local minima and saddle points.</p>
<p><strong>Solution 3:</strong><br />
<strong>True</strong>. Noisy gradients can help avoid getting stuck in saddle points or shallow local minima. <span class="math display">\[\boxed{\textbf{Answer: True}}\]</span></p></li>
<li><p><strong>Optimizer with aggressive LR decay from accumulating squared gradients:</strong><br />
Which optimizer can suffer from <strong>learning rate decaying too aggressively over time</strong> due to accumulating squared gradients?</p>
<ol>
<li><p>RMSProp</p></li>
<li><p>AdaGrad</p></li>
<li><p>Adam</p></li>
</ol>
<p><strong>Solution 4:</strong><br />
<strong>(B) AdaGrad</strong>. The accumulated gradient sum grows over time, shrinking the effective learning rate. <span class="math display">\[\boxed{\textbf{Answer: (B)}}\]</span></p></li>
<li><p><strong>Momentum + adaptive LR (RMSProp-style) combination:</strong><br />
Which optimizer is best described as combining <strong>Momentum + RMSProp-style adaptive learning rates</strong>?</p>
<ol>
<li><p>RMSProp</p></li>
<li><p>AdaGrad</p></li>
<li><p>Adam</p></li>
</ol>
<p><strong>Solution 5:</strong><br />
<strong>(C) Adam</strong>. Adam combines momentum (first moment) with adaptive learning rate scaling (second moment). <span class="math display">\[\boxed{\textbf{Answer: (C)}}\]</span></p></li>
</ol>

</main>
</body>
</html>
