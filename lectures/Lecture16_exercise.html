<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>Lecture16 In-class Exercise</title>
  <link rel="stylesheet" href="../assets/style.css"/>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav>
  <a href="../index.html">Home</a>
</nav>
<main>
<div class="center">
<p><span><strong>In-Class Exercise (Canvas Quiz) â€” 10 minutes</strong></span><br />
<span><strong>Lecture 16: Convolutional Neural Networks (Training)</strong></span></p>
</div>
<p>This exercise is based on the lecture discussion of training optimization for CNNs, including: Gradient Descent variants (BGD, SGD, MBGD) and adaptive optimizers (AdaGrad, RMSProp, Adam).<br />
(Full questions are shown in class on projector. Canvas contains keywords only.)</p>
<p><strong>Tasks:</strong> (Answer ALL)</p>
<ol>
<li><p><strong>(Multiple choice)</strong> You have a dataset with <span class="math inline">\(N=1,000,000\)</span> datapoints. Which method generally has the <strong>highest memory/computation cost per epoch</strong>?</p>
<ol>
<li><p>SGD</p></li>
<li><p>Mini-batch GD (MBGD)</p></li>
<li><p>Batch GD (BGD)</p></li>
</ol></li>
<li><p><strong>(Multiple choice)</strong> Which method tends to have the <strong>noisiest / highest-variance update path</strong>?</p>
<ol>
<li><p>SGD</p></li>
<li><p>Mini-batch GD (MBGD)</p></li>
<li><p>Batch GD (BGD)</p></li>
</ol></li>
<li><p><strong>(True/False)</strong> The stochasticity (noise) of SGD can sometimes help escape shallow local minima and saddle points.</p></li>
<li><p><strong>(Multiple choice)</strong> Which optimizer can suffer from <strong>learning rate decaying too aggressively over time</strong> due to accumulating squared gradients?</p>
<ol>
<li><p>RMSProp</p></li>
<li><p>AdaGrad</p></li>
<li><p>Adam</p></li>
</ol></li>
<li><p><strong>(Multiple choice)</strong> Which optimizer is best described as combining <strong>Momentum + RMSProp-style adaptive learning rates</strong>?</p>
<ol>
<li><p>RMSProp</p></li>
<li><p>AdaGrad</p></li>
<li><p>Adam</p></li>
</ol></li>
</ol>

</main>
</body>
</html>
