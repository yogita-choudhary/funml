<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>Lecture26 In-class Exercise</title>
  <link rel="stylesheet" href="../assets/style.css"/>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav>
  <a href="../index.html">Home</a>
</nav>
<main>
<div class="center">
<p><span><strong>In-Class Exercise (Canvas Quiz) â€” 15 minutes</strong></span><br />
<span><strong>Lecture 26: Self-Supervised Learning</strong></span></p>
</div>
<p>In this lecture, we discussed self-supervised learning (SSL) via <strong>pre-text tasks</strong> that generate <strong>pseudo-labels</strong> and learn representations from unlabeled data. In <strong>SimCLR</strong>, two augmented views of the same instance form a <strong>positive pair</strong>, while views from different instances act as <strong>negatives</strong>.</p>
<p><strong>Notation (use in your answers):</strong></p>
<ul>
<li><p>Encoder: <span class="math inline">\(f_\theta(\cdot)\)</span> produces representation vectors <span class="math inline">\(\boldsymbol{h}\in\mathbb{R}^d\)</span>.</p></li>
<li><p>Projection head: <span class="math inline">\(g_\phi(\cdot)\)</span> produces embedding vectors <span class="math inline">\(\boldsymbol{z}\in\mathbb{R}^m\)</span>.</p></li>
<li><p>Mini-batch of <span class="math inline">\(N\)</span> original inputs: <span class="math inline">\(\{\boldsymbol{x}_1,\dots,\boldsymbol{x}_N\}\)</span>; two augmentations per input <span class="math inline">\(\Rightarrow 2N\)</span> views.</p></li>
<li><p>Similarity uses cosine similarity: <span class="math inline">\(\mathrm{sim}(\boldsymbol{u},\boldsymbol{v})=\frac{\boldsymbol{u}^\top\boldsymbol{v}}{\|\boldsymbol{u}\|\|\boldsymbol{v}\|}\)</span>.</p></li>
</ul>
<p><strong>Tasks (answer directly):</strong></p>
<ol>
<li><p><strong>Core SSL idea (pseudo-labels).</strong><br />
A pre-text task <span class="math inline">\(P(\cdot)\)</span> transforms an unlabeled input <span class="math inline">\(\boldsymbol{x}\)</span> into a pseudo-label. Which option best describes <em>why</em> pseudo-labels are useful in SSL?</p>
<ol>
<li><p>They replace ground-truth labels and guarantee perfect downstream accuracy.</p></li>
<li><p>They create a training signal that encourages learning features that transfer to downstream tasks.</p></li>
<li><p>They are only used to compress data and are not used for representation learning.</p></li>
<li><p>They remove the need for augmentations in contrastive learning.</p></li>
</ol></li>
<li><p><strong>Representation vs embedding (SimCLR design choice).</strong><br />
SimCLR uses <span class="math inline">\(\boldsymbol{h}=f_\theta(\boldsymbol{x})\)</span> and then <span class="math inline">\(\boldsymbol{z}=g_\phi(\boldsymbol{h})\)</span> for the contrastive loss. Why is it beneficial to apply the contrastive objective on <span class="math inline">\(\boldsymbol{z}\)</span> (after the projection head) rather than directly on <span class="math inline">\(\boldsymbol{h}\)</span>?</p>
<ol>
<li><p>It prevents the encoder from learning any transferable information.</p></li>
<li><p>It lets the encoder representation <span class="math inline">\(\boldsymbol{h}\)</span> remain useful for downstream tasks while <span class="math inline">\(\boldsymbol{z}\)</span> specializes for the contrastive objective.</p></li>
<li><p>It is required because cosine similarity cannot be computed on <span class="math inline">\(\boldsymbol{h}\)</span>.</p></li>
<li><p>It makes positive pairs become negatives during training.</p></li>
</ol></li>
<li><p><strong>What the loss is pushing (conceptual geometry).</strong><br />
For an anchor view <span class="math inline">\(i\)</span>, SimCLR treats its paired view <span class="math inline">\(j\)</span> as positive and all other views as negatives. Which statement best matches the intended effect of the contrastive loss on the embeddings?</p>
<ol>
<li><p>Increase <span class="math inline">\(\mathrm{sim}(\boldsymbol{z}_i,\boldsymbol{z}_j)\)</span> and decrease <span class="math inline">\(\mathrm{sim}(\boldsymbol{z}_i,\boldsymbol{z}_k)\)</span> for negatives <span class="math inline">\(k\)</span>.</p></li>
<li><p>Decrease all similarities so embeddings become orthogonal to everything.</p></li>
<li><p>Increase similarity to all views so all embeddings collapse to the same point.</p></li>
<li><p>Maximize the norm <span class="math inline">\(\|\boldsymbol{z}_i\|\)</span> while ignoring pairwise similarities.</p></li>
</ol></li>
<li><p><strong>Negatives and batch effects (reasoning).</strong><br />
Consider a fixed augmentation pipeline and a fixed encoder <span class="math inline">\(f_\theta\)</span>. If you increase the batch size <span class="math inline">\(N\)</span> (so there are more views and thus more negatives per anchor), which outcome is the most accurate <em>conceptual</em> expectation?</p>
<ol>
<li><p>The task becomes strictly easier because positives become more similar automatically.</p></li>
<li><p>The contrastive task typically becomes harder/more demanding because each anchor must be distinguished from more negatives.</p></li>
<li><p>The number of positives per anchor increases linearly with <span class="math inline">\(N\)</span>.</p></li>
<li><p>The model no longer needs the projection head <span class="math inline">\(g_\phi\)</span>.</p></li>
</ol></li>
<li><p><strong>One quick sanity check (minimal counting).</strong><br />
In SimCLR with <span class="math inline">\(N=4\)</span> original inputs, there are <span class="math inline">\(2N=8\)</span> total views. For a given anchor view, how many positives and how many negatives are used in the denominator of the contrastive objective?</p>
<ol>
<li><p>1 positive, 5 negatives</p></li>
<li><p>1 positive, 6 negatives</p></li>
<li><p>2 positives, 5 negatives</p></li>
<li><p>2 positives, 6 negatives</p></li>
</ol></li>
</ol>

</main>
</body>
</html>
