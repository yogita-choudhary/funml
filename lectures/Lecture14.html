<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>main</title>
  <link rel="stylesheet" href="../assets/style.css"/>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav>
  <a href="../index.html">Home</a>
</nav>
<main>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>_Lecture14</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div class="center">

</div>
<p><strong>Contributors:</strong> Dr. Ahmad Mustafa, Dr. Motaz Alfarraj, Dr. Ashraf Alattar, Dr. Chen Zhou</p>
<p><strong>Teaching Assistants</strong> with remarkable contributions include: Kuo-Wei Lai, Wuyang Du, Shiva Mahato, Michael Zhou, Ninghan Zhong</p>
<p><strong>Disclaimer</strong>: <span>All content of these notes are part of this course at Georgia Tech. Any re-use or distribution is not permitted without pre-approved permission. All these notes belong to, created by, and copyrighted for Ghassan AlRegib and Mohit Prabhushankar, Georgia Tech, 2021–2028.</span></p>
<p><strong>License</strong>: <span>These lecture notes are licensed under the <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.</span></p>
<p><strong>Errata</strong>: <span><em>Please submit any errata you find using the following form: <a href="https://forms.office.com/r/fbg9dMWPgY">Errata Form for FunML Textbook</a> or visit: <a href="https://forms.office.com/r/fbg9dMWPgY">https://forms.office.com/r/fbg9dMWPgY</a></em></span></p>
<section id="lecture-objectives" data-number="0.1">
<h2 data-number="1.1"><span class="header-section-number">1.1</span> Lecture Objectives</h2>
<p>In the last lecture, we extended our coverage of Artificial Neural Networks (ANNs) beyond our basic understandings of the perceptron algorithm and its multi-layer network ability. This lecture addresses the possible limitations of the ANN by introducing Convolutional Neural Networks (CNNs), which functions off the basis of locally connected layer analysis within the input data. We will cover the different methods that a CNN is capable of doing including pooling, full-connection layers, and 2D convolution.</p>
</section>
<section id="recap-of-last-lecture" data-number="0.2">
<h2 data-number="1.2"><span class="header-section-number">1.2</span> Recap of Last Lecture</h2>
<p>The last lecture focused on some aspects such as:</p>
<ul>
<li><p>Finding optimum weights via solving the <strong>Normal Equation</strong></p></li>
<li><p>Finding optimum weights via training by <strong>Gradient Descent</strong></p></li>
<li><p>Quick Review of Derivatives</p>
<ul>
<li><p>Chain Rule</p></li>
<li><p>Derivative of the sum is the sum of the derivatives</p></li>
<li><p>Derivative wrt one element of the sum collapses the sum</p></li>
</ul></li>
<li><p>Computing the backpropagation error (<span class="math inline">\(e=L(y,y^*)\)</span>) by carrying derivatives from later layers, thereby updating weights until the error <span class="math inline">\(e\)</span> converges</p></li>
</ul>
</section>
<section id="limitations-of-multi-layered-perceptrons-mlps" data-number="0.3">
<h2 data-number="1.3"><span class="header-section-number">1.3</span> Limitations of Multi-layered Perceptrons (MLPs)</h2>
<section id="ann-and-mlp-disadvantages" data-number="0.3.1">
<h3 data-number="1.3.1"><span class="header-section-number">1.3.1</span> ANN and MLP Disadvantages</h3>
<p>We have been using ANNs for very high-dimensional inputs, which requires a large amount of parameters which makes it tedious for analysis. For example, in a 200x200 pixel image (this is not large at all!), if each input is connected to a neuron, each individual neuron would need 40,000 weights. And for 100 neurons, that’s just 4 million parameters itself. An excessively high number of parameters might result in inefficiencies and difficulty, especially in the case of limited training data.</p>
<figure>
<img src="img/lecture14/image1.png" id="fig:enter-label" alt="Image depicting the process of a single neuron attached to every input pixel" /><figcaption aria-hidden="true">Image depicting the process of a single neuron attached to every input pixel</figcaption>
</figure>
</section>
<section id="neural-networks-through-local-connectivity" data-number="0.3.2">
<h3 data-number="1.3.2"><span class="header-section-number">1.3.2</span> Neural Networks through Local Connectivity</h3>
<p><strong>Locally connected layers</strong> are one of the solutions that full-connected layers might complicate. For the 200x200 pixel image, neurons are to be connected to smaller batches of pixels in the image. In this example, a neuron can be attached to a 3x3 patch inside the grid, which results in less weights. Here, the 3x3 batch results in 9 shared weights. So for 100 neurons, there would only be 900 parameters required, far fewer than the number of parameters needed to compute using MLPs. See figure 14.2 for reference.</p>
<figure>
<img src="img/lecture14/image2.png" id="fig:enter-label" alt="Process of neurons attached to specific pixels in local connectivity" /><figcaption aria-hidden="true">Process of neurons attached to specific pixels in local connectivity</figcaption>
</figure>
<p>Locally connected inputs are advantageous if the inputs are roughly registered (e.g. face recognition). Additionally, they are effective at recognizing tangible objects in images that can appear anywhere (e.g. cup on a table).</p>
</section>
<section id="stationarity" data-number="0.3.3">
<h3 data-number="1.3.3"><span class="header-section-number">1.3.3</span> Stationarity</h3>
<p>The implications of <strong>stationarity</strong> inside a locally connected network means that the statistics, or qualities/features, are similar across different locations pinpointed on the input image. Assuming stationarity, we can allow for sharing of parameters, which mean using the same parameters (or filters) throughout different parts of the image. This technique is known as convolution with learned kernels, which we will get to in the later part of the section. What this enables is basic pattern recognition in the whole image without the necessity of parameters at every single pixel location.</p>
</section>
<section id="anns-vs.-cnns" data-number="0.3.4">
<h3 data-number="1.3.4"><span class="header-section-number">1.3.4</span> ANNs vs. CNNs</h3>
<p>ANNs suffer from a scalability issue that causes parameters to grow quadratically alongside the size increase of the input. This makes ANNs inefficient for processing high-dimensional data, especially images, where the example we used for 200x200 is very much on the smaller scale. Common image sizes like 1080x1080 makes the algorithm difficult to leverage different properties of the input.</p>
<p>CNNs address the shortcomings by connecting each neuron to a local patch of the input through convolutional layers. Weight sharing is another attribute of a CNN, which means a similar set of weights are applied to various regions of the input across spatial locations. The reduction of parameters allows CNNs to capture spatial hierarchies like texture and edges in images, so they are very prominent for image processing tasks.</p>
</section>
</section>
<section id="mathematical-concept-of-convolution-1d-and-2d" data-number="0.4">
<h2 data-number="1.4"><span class="header-section-number">1.4</span> Mathematical Concept of Convolution (1D and 2D)</h2>
<section id="d-and-2d-convolutions" data-number="0.4.1">
<h3 data-number="1.4.1"><span class="header-section-number">1.4.1</span> 1D and 2D Convolutions</h3>
<ul>
<li><p>In 1D convolution, the process involves multiplying the input signal <span class="math inline">\(x(t)\)</span> and the kernel (filter) <span class="math inline">\(w(a)\)</span> and integrating it</p>
<ul>
<li><p>the <span class="math inline">\(y(t)\)</span> output is obtained by shifting the kernel over the input and getting the integral of the product over all possible shifts <span class="math inline">\(a\)</span></p></li>
</ul>
<p><span class="math display">\[y(t) = \int_{-\infty}^{+\infty} x(t - a) w(a) \, da\]</span></p></li>
</ul>
<ul>
<li><p>In 2D convolution</p>
<ul>
<li><p>both input and kernel are 2D, like images</p></li>
<li><p>the output <span class="math inline">\(y(t_1, t_2)\)</span> is found by convolving the input image <span class="math inline">\(x(t_1, t_2)\)</span> with the kernel <span class="math inline">\(w(a, b)\)</span>, and the process involves a double integration for both dimensions <span class="math display">\[y(t_1, t_2) = \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} x(t_1 - a, t_2 - b) w(a, b) \, da \, db\]</span></p></li>
</ul></li>
</ul>
</section>
<section id="d-discrete-convolution" data-number="0.4.2">
<h3 data-number="1.4.2"><span class="header-section-number">1.4.2</span> 2D Discrete Convolution</h3>
<p>The discrete form is the more specific method for digital images and neural networks. In this equation, the input <span class="math inline">\(X[t_1, t_2]\)</span> is convolved with the discrete kernel <span class="math inline">\(W[a, b]\)</span>. The output <span class="math inline">\(Y[t_1, t_2]\)</span> is the products added together over the size of the kernel <span class="math inline">\(k_1\times k_2\)</span>, during when the kernel slides over the input image. The formula is given below: <span class="math display">\[Y[t_1, t_2] = \sum_{a=0}^{k_1 - 1} \sum_{b=0}^{k_2 - 1} X[t_1 + a, t_2 + b] W[a, b]\]</span> Here, the summation over the discrete inputs and kernel dimensions gives the output result. Figure 14.3 above illustrates how kernel moves about the input area to produce the output, capturing local features or pattern recognition along the way.</p>
<figure>
<img src="img/lecture14/image3.png" id="fig:enter-label" alt="Process of 2D discrete convolution" /><figcaption aria-hidden="true">Process of 2D discrete convolution</figcaption>
</figure>
</section>
</section>
<section id="mechanics-of-2d-discrete-convolution" data-number="0.5">
<h2 data-number="1.5"><span class="header-section-number">1.5</span> Mechanics of 2D Discrete Convolution</h2>
<section id="example-using-3x3-kernel" data-number="0.5.1">
<h3 data-number="1.5.1"><span class="header-section-number">1.5.1</span> Example using 3x3 Kernel</h3>
<p><strong>(Exercise)</strong> We will witness how a 3x3 kernel interacts with an input matrix to produce the resulting output. The 3x3 kernel matrix slides over the input matrix <span class="math inline">\(X\)</span>, and the starting points are different for each row (e.g. <span class="math inline">\(X[0, 0], X[1, 0], X[2, 0], X[3, 0]\)</span>). As it slides, the kernel performs an element-wise multiplication with the corresponding values with the input matrix within the 3x3 area. The output then is a sum of those 9 values. The process is repeated for all of the sliding positions of the kernel to fill the output matrix <span class="math inline">\(Y\)</span>. Figures 14.4 through 14.6 depicts some random positions the kernel is at.</p>
<figure>
<img src="img/lecture14/image4.png" id="fig:enter-label" alt="Kernel position at X[0, 3] " /><figcaption aria-hidden="true">Kernel position at <span class="math inline">\(X[0, 3]\)</span> </figcaption>
</figure>
<figure>
<img src="img/lecture14/image5.png" id="fig:enter-label" alt="Kernel at position X[1, 3] " /><figcaption aria-hidden="true">Kernel at position <span class="math inline">\(X[1, 3]\)</span> </figcaption>
</figure>
<figure>
<img src="img/lecture14/image6.png" id="fig:enter-label" alt="Kernel at position X[3, 0] " /><figcaption aria-hidden="true">Kernel at position <span class="math inline">\(X[3, 0]\)</span> </figcaption>
</figure>
<p>The mathematical expression given at the bottom of each of the figures 14.4-14.6 sums the element-wise products of the input matrix <span class="math inline">\(X\)</span> and the kernel <span class="math inline">\(W\)</span>. The result is then passed through a non-linear activation function <span class="math inline">\(\sigma\)</span>, and introduces non-linearity into the model. The formula is: <span class="math display">\[Y[i, j] = \sigma \left( \sum_{a=0}^{N-1} \sum_{b=0}^{N-1} X[i + a, j + b] W[a, b] \right)\]</span> This formula represents the 2D convolution at any point/position <span class="math inline">\((i, j)\)</span>, where <span class="math inline">\(N\)</span> are the dimensions of the kernel.</p>
</section>
<section id="kernel-intuition" data-number="0.5.2">
<h3 data-number="1.5.2"><span class="header-section-number">1.5.2</span> Kernel Intuition</h3>
<p>Figure 14.7 details more in depth of how the kernel operates. In summary as the kernel slides across over various 3x3 patches on the input image, it performs the element-wise multiplication and sums up the products to produce a single output value. This is repeated on all positions where the kernel has room to slide over the input matrix, thus generating an entire output matrix. The output ends up being a "lower-dimensional representation" of the input, which captures the significant features from the input based on the learned weights (values of the kernel matrix).</p>
<figure>
<img src="img/lecture14/image7.png" id="fig:enter-label" alt="Summing all 9 products to give output matrix" /><figcaption aria-hidden="true">Summing all 9 products to give output matrix</figcaption>
</figure>
</section>
</section>
<section id="convolutional-layer" data-number="0.6">
<h2 data-number="1.6"><span class="header-section-number">1.6</span> Convolutional Layer</h2>
<p>One of the core components of the CNN is the convolutional layer, which is responsible for much of the computational work done. It operates using 3 properties:</p>
<ul>
<li><p><strong>local connectivity</strong> - the neurons are connected to only a local subset region of the input</p></li>
<li><p><strong>parameter sharing</strong> - the weights <span class="math inline">\(w\)</span> or filters are applied across the spatial location on input image</p></li>
<li><p><strong>spatial arrangement</strong> - the structural aspect of the input image are maintained, all through the operations</p></li>
</ul>
<section id="kernel-application-in-spatial-arrangement" data-number="0.6.1">
<h3 data-number="1.6.1"><span class="header-section-number">1.6.1</span> Kernel Application in Spatial Arrangement</h3>
<p>The following image in figure 14.8 depicts a 32x32x3 being processed with a 5x5 filter with same depth. The third dimension of 3 is the depth which represents the RGB color channel. A sequence of convolutional filters are to be applied to this multi-dimensional input while preserving the spatial arrangement of the data.</p>
<figure>
<img src="img/lecture14/image8.png" id="fig:enter-label" alt="32x32x3 image processed with 5x5x3 filter" /><figcaption aria-hidden="true">32x32x3 image processed with 5x5x3 filter</figcaption>
</figure>
<p>The filters as you can see must be covering the entire depth of the input volume. As depicted, the 5x5x3 filter is convolved with the 3D input image. Just like the 2D version, the filter slides over the image spatially and computes the dot products between each filter and each small patch of the image. The result of each dot product would a singular scalar value in the output, which would then be compiled into an activation maps which will be explained in the following.</p>
<p>We mentioned filter(s) in a form of plurality because it is possible to stack multiple filters. The process is shown in figure 14.9 and outputs an <strong>activation map</strong> (also called a feature map!). For each of the filters applied, a small 5x5x3 region is selected, and the dot product computed between the filter’s weights and the pixel’s values describes the region. This resulting dot product is what we have seen as our basic scalar notation:</p>
<p><span class="math display">\[w^Tx+b\]</span> where <span class="math inline">\(w\)</span> represents the weight of the filter, <span class="math inline">\(x\)</span> represents the image patch, and <span class="math inline">\(b\)</span> is the bias term.</p>
</section>
<section id="generating-feature-maps" data-number="0.6.2">
<h3 data-number="1.6.2"><span class="header-section-number">1.6.2</span> Generating Feature Maps</h3>
<p>To be more specific regarding how the output activation map is obtained, when the filter slides all over the spatial locations of the image, it first generates a 2D feature map. In this case, the output result dimension is 28x28x1, since the filter reduces the spatial dimensions. But multiple feature maps are generated if many filters are stacked; hence the 6 stacked filters produce a multi-dimensional output map of size 28x28x6.</p>
<figure>
<img src="img/lecture14/image9.png" id="fig:enter-label" alt="If 6 filters of size 5x5x3, the outputs result in 6 separate 28x28x6 activation maps" /><figcaption aria-hidden="true">If 6 filters of size 5x5x3, the outputs result in 6 separate 28x28x6 activation maps</figcaption>
</figure>
</section>
</section>
<section id="spatial-dimensions-inside-convolutional-layer" data-number="0.7">
<h2 data-number="1.7"><span class="header-section-number">1.7</span> Spatial Dimensions Inside Convolutional Layer</h2>
<p>This section will go through a primary example that can be used to replicate most convolutional operations. The goal is the demonstrate how the filters operate on the spatial dimensions of an input image, how the output is affected by stride, and how the input size consecutively reduces after applying convolution.</p>
<section id="filter-application-example" data-number="0.7.1">
<h3 data-number="1.7.1"><span class="header-section-number">1.7.1</span> Filter Application Example</h3>
<p>In this example, we will use a 7x7 input image and 3x3 filter(s). The filter slides across the input to compute the outputs; the resulting operation for the first row created 5 columns as seen in figure 14.11.</p>
<figure>
<img src="img/lecture14/image10.png" id="fig:enter-label" alt="Filter slides along the first row, all the way to end of row" /><figcaption aria-hidden="true">Filter slides along the first row, all the way to end of row</figcaption>
</figure>
<figure>
<img src="img/lecture14/11.png" id="fig:enter-label" alt="After the first row, the output created contains 5 columns" /><figcaption aria-hidden="true">After the first row, the output created contains 5 columns</figcaption>
</figure>
<p>After the first row, the filter would shift one block under into the next row. This image is a 7x7, so this process would continue until the bottom-left block of the filter would hit the bottom-left block of the image. This then would be the last row the filter traverses. Thus, the final resulting output dimensions are 5x5, as shown in figure 14.12.</p>
<figure>
<img src="img/lecture14/12.png" id="fig:enter-label" alt="The output (right grid) as soon as filter (left grid) reaches bottom-right" /><figcaption aria-hidden="true">The output (right grid) as soon as filter (left grid) reaches bottom-right</figcaption>
</figure>
</section>
<section id="impact-of-stride" data-number="0.7.2">
<h3 data-number="1.7.2"><span class="header-section-number">1.7.2</span> Impact of Stride</h3>
<p><strong>Stride</strong> can be described as the number of steps that the filter moves at an iteration inside the input image. Here, we set an example of when the stride is <span class="math inline">\(2\)</span>, which results in the output dimension being reduced quite further. The filter skips more input values, covering less of the image data, and in this case, the 7x7 input produces a 3x3 output. Figures 14.13 and 14.14 describes this process.</p>
<p>It is important to note that stride affects all dimensions; hence, for a stride of 2, it skips every other row as well.</p>
<figure>
<img src="img/lecture14/13.png" id="fig:enter-label" alt="Output produces 3 columns after the filter travels through first row; the filter skips two units at a time" /><figcaption aria-hidden="true">Output produces 3 columns after the filter travels through first row; the filter skips two units at a time</figcaption>
</figure>
<figure>
<img src="img/lecture14/14.png" id="fig:enter-label" alt="Each row is skipped by 2 units as well; results a 3x3 output" /><figcaption aria-hidden="true">Each row is skipped by 2 units as well; results a 3x3 output</figcaption>
</figure>
<p>What about strides that don’t work? With a stride of 1, as in figures 14.10-14.12, we see that the filter moves through every possible location of the input. When the stride is 2, the filter skips over locations, but still the entire canvas is covered in an incrementally different way. But when the stride is 3, you will see that the filter doesn’t fit the input dimensions anymore, leading to an issue that cannot be applied.</p>
<p>There is a way to calculate whether a certain stride value would be applicable and to obtain the output size as per chosen stride. The formula is: <span class="math display">\[Output size = \frac{P-k}{stride}+1\]</span> Using the formula above, the output sizes are calculated for the different strides we have chose (1, 2, and 3). Figure 14.15 presents the calculations. Notice how if the resulting scalar value isn’t a whole number, the corresponding stride value is nonfunctional.</p>
</section>
<section id="zero-padding" data-number="0.7.3">
<h3 data-number="1.7.3"><span class="header-section-number">1.7.3</span> Zero-Padding</h3>
<p><strong>Zero-padding</strong> is a technique where slack zeros are attached around the input, in order to adjust the input dimensions to meet the filter’s ability to slide over the boundaries of the input’s non-zero values. This allows the usual convolution to be computed while the spatial dimensions of the output are preserved, especially when using a stride of 1 in this new input environment.</p>
<figure>
<img src="img/lecture14/15.png" id="fig:enter-label" alt="Parameters used to calculate the output dimensions using input variable of stride value" /><figcaption aria-hidden="true">Parameters used to calculate the output dimensions using input variable of stride value</figcaption>
</figure>
<p>Let’s take the usual 7x7 input image with the 3x3 filter as an application. Figure 14.16 shows that using this same filter on the input, the output dimensions are a preserved 7x7 size due to 1-pixel padding of the border. This zero-padding with <span class="math inline">\((k-1)/2\)</span> ensures that both the input and output sizes are the same when stride 1 is used, as long as the kernel size <span class="math inline">\(k\)</span> is an odd number.</p>
<figure>
<img src="img/lecture14/16.png" id="fig:enter-label" alt="Based on kernel sizes, zero-pad with respective amount of units" /><figcaption aria-hidden="true">Based on kernel sizes, zero-pad with respective amount of units</figcaption>
</figure>
</section>
<section id="dimensions-and-parameters" data-number="0.7.4">
<h3 data-number="1.7.4"><span class="header-section-number">1.7.4</span> Dimensions and Parameters</h3>
<p>The output volume dimensions can now be updated to be able to include the stride value parameter. The formula used is shown below:</p>
<p><span class="math display">\[\frac{Input size + 2\times Padding - Filter size}{Stride} + 1\]</span> If, for example, an input size 32x32x3 is given with filters of size 5x5, stride value of 1, and padding 2, then the output volume size obtained would be 32x32x10.</p>
<p>Next, the number of parameters calculated gives the amount for each filter: <span class="math inline">\(5\times 5\times 3 + 1 = 76\)</span> parameters, which includes 1 for bias. And with 10 filters, the total number of parameters is <span class="math inline">\(76\times 10 = 760\)</span> parameters.</p>
<p>Finally, the general formulas for calculating the output dimensions of a convolutional layer can be found. Figure 14.17 depicts each dimensions for the input, kernel, and the output. The input is described by <span class="math inline">\((C_{in}, N, H_{in}, W_{in})\)</span> where <span class="math inline">\(N\)</span> is the number of samples, <span class="math inline">\(C_{in}\)</span> is the number of input channels (normally fixed for RGB color scheme), <span class="math inline">\(H_{in}\)</span> and <span class="math inline">\(W_{in}\)</span> are the height and width of the input, respectively. The kernel size is defined by <span class="math inline">\((C_{out}, C_{in}, k_0, k_1)\)</span> of which <span class="math inline">\(k_0\)</span> and <span class="math inline">\(k_1\)</span> are the spatial dimensions of the filter. The output volume has dimensions <span class="math inline">\((N, C_{out}, H_{out}, W_{out})\)</span>, where the height and width of the output are determined by the padding, filter size, and the stride value, following the respective formulas for the <span class="math inline">\(H_{out}\)</span> and <span class="math inline">\(W_{out}\)</span>:</p>
<p><span class="math display">\[H_{out} = \frac{H_{in} + 2 \times Padding_H - k_0}{Stride_H} + 1\]</span> <span class="math display">\[W_{out} =\frac{W_{in} + 2 \times Padding_W - k_1}{Stride_W} + 1\]</span></p>
<figure>
<img src="img/lecture14/17.png" id="fig:enter-label" alt="Dimension variables for input, kernel, and output" /><figcaption aria-hidden="true">Dimension variables for input, kernel, and output</figcaption>
</figure>
</section>
<section id="interceptions-with-activation-functions" data-number="0.7.5">
<h3 data-number="1.7.5"><span class="header-section-number">1.7.5</span> Interceptions with Activation Functions</h3>
<p>CNNs can come as a sequence of convolutional layers that are intertwined with various activition functions like ReLU. Each layers of convolution implements filters to the input data, which causes spatial dimensions to decrease the depth of feature maps increase. The number of filters applied determines the output depth.</p>
<p>As mentioned before, CNNs differ from fully-connected networks where neurons in the convolutional layers are focuing on local regions of the inputs. This is also called the <strong>receptive field</strong>. As the number of layers increase, the ability to capture more global information expands; i.e. in other words, the receptive field expands. It is important to note that as number of layers increase, the neural network becomes similar with fully-connected networks, but with less parameters are needed. Each layer stage extracts feature maps from the input, processes them through layers, and conglomerates and flattens the data for fully-connected layers. The last layer would finally apply a softmax function for classification tasks. Figure 14.19 highlights the gradual progression from raw input to the final form of output, as this is one of many methods of simply transforming images into critical features for powerful classification tasks.</p>
<figure>
<img src="img/lecture14/18.png" id="fig:enter-label" alt="Z_{1,1} has 3x3 receptive field; Z^{2,2} has 5x5, gradually expanding" /><figcaption aria-hidden="true"><span class="math inline">\(Z_{1,1}\)</span> has 3x3 receptive field; <span class="math inline">\(Z^{2,2}\)</span> has 5x5, gradually expanding</figcaption>
</figure>
<figure>
<img src="img/lecture14/19.png" id="fig:enter-label" alt="Process of how one local region in an image can undergo convolutional layers to classify the image" /><figcaption aria-hidden="true">Process of how one local region in an image can undergo convolutional layers to classify the image</figcaption>
</figure>
<figure>
<img src="img/lecture14/Im2.png" id="fig:enter-label" alt="Variation of the pooling layer" /><figcaption aria-hidden="true">Variation of the pooling layer</figcaption>
</figure>
<figure>
<img src="img/lecture14/Im3.png" id="fig:enter-label" alt="Three main types of layers to build ConvNet architectures" /><figcaption aria-hidden="true">Three main types of layers to build ConvNet architectures</figcaption>
</figure>
</section>
</section>
<section id="pooling-layer-and-fully-connected-layer" data-number="0.8">
<h2 data-number="1.8"><span class="header-section-number">1.8</span> Pooling Layer and Fully-connected Layer</h2>
<p>By using pooling filter, we are taking maximum value within the size of the filter so that we can gain robustness to the exact spatial location of features. Figure 14.20 shows the activation maps become smaller and more manageable due to the pooling filter. This layer does not have trainable values and could work as a reduced version of entire convolution layer. Since there is no parameters like weight or bias for pooling layers, iterative calculation for gradient descent is also not needed. On the other hand, Fully-connected layer is nothing more than a <span class="math inline">\(1\times1\)</span> formulation. For example, if we have <span class="math inline">\(32\times32\times3\)</span> image as an input, then we can stretch the image to <span class="math inline">\(3072\times1\)</span>, and perform 3072-dimensional dot product.</p>
</section>
<section id="terminology" data-number="0.9">
<h2 data-number="1.9"><span class="header-section-number">1.9</span> Terminology</h2>
<ul>
<li><p><strong><em>Distribution</em></strong>: (sample space) The set of all possible samples</p></li>
<li><p><strong><em>Dataset</em></strong>: A set of samples drawn from a distribution</p></li>
<li><p><strong><em>Batch</em></strong>: A subset of samples drawn from the dataset</p></li>
<li><p><strong><em>Sample</em></strong>: A single data object represented as a set of features</p></li>
<li><p><strong><em>Feature</em></strong>: Value of a single attribute, property, in a sample</p></li>
</ul>
</section>
<section id="qa-section" data-number="0.10">
<h2 data-number="1.10"><span class="header-section-number">1.10</span> Q&amp;A Section</h2>
<ol>
<li><p><strong>Question:</strong></p>
<p>Given an input image of size <span class="math inline">\(64 \times 64 \times 3\)</span>, apply a convolutional layer with a filter of size <span class="math inline">\(8 \times 8 \times 3\)</span>, stride <span class="math inline">\(4\)</span>, and no padding. Then, apply a <span class="math inline">\(4 \times 4\)</span> max pooling layer with stride <span class="math inline">\(1\)</span> and no padding. Compute the size of the final output after the pooling layer.</p>
<p><strong>Solution:</strong></p>
<p>After the convolutional layer (input size: <span class="math inline">\(H_{\text{in}} = 64\)</span>, <span class="math inline">\(W_{\text{in}} = 64\)</span>; filter size: <span class="math inline">\(k = 8\)</span>; stride: <span class="math inline">\(S = 4\)</span>; padding: <span class="math inline">\(P = 0\)</span>), the output dimensions are: <span class="math display">\[H_{\text{out}} = \frac{H_{\text{in}} - k + 2P}{S} + 1 = \frac{64 - 8 + 0}{4} + 1 = \frac{56}{4} + 1 = 14 + 1 = 15\]</span> <span class="math display">\[W_{\text{out}} = 15\]</span></p>
<p>So, the output size after the convolutional layer is <span class="math inline">\(15 \times 15\)</span>.</p>
<p>After the pooling layer (input size: <span class="math inline">\(H_{\text{in}} = 15\)</span>, <span class="math inline">\(W_{\text{in}} = 15\)</span>; filter size: <span class="math inline">\(k = 4\)</span>; stride: <span class="math inline">\(S = 1\)</span>; padding: <span class="math inline">\(P = 0\)</span>), the output dimensions are: <span class="math display">\[H_{\text{out}} = \frac{15 - 4}{1} + 1 = 11 + 1 = 12\]</span> <span class="math display">\[W_{\text{out}} = 12\]</span></p>
<p>Therefore, the final output size after the pooling layer is <span class="math inline">\(12 \times 12\)</span>.</p></li>
</ol>
</section>
</body>
</html>

</main>
</body>
</html>
