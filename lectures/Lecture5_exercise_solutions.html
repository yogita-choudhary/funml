<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>Lecture5 In-class Exercise Solutions</title>
  <link rel="stylesheet" href="../assets/style.css"/>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav>
  <a href="../index.html">Home</a>
</nav>
<main>
<div class="center">
<p><span><strong>In-Class Exercise — Solutions</strong></span><br />
<span><strong>Lecture 5: Introduction to Neural Networks (Single-Layer Perceptron)</strong></span></p>
</div>
<p><strong>Question 1</strong> Is the following statement True or False?</p>
<p><em>“In an SLP, If <span class="math inline">\(\mathbf{x} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}\)</span> and both components of <span class="math inline">\(\mathbf{w}\)</span> are positive, then both weights will always increase during training.”</em></p>
<p><strong>Solutions:</strong></p>
<p>The statement is <strong>False</strong>.</p>
<p>Consider the following scenario: True label: <span class="math inline">\(y = 0\)</span>, Input: <span class="math inline">\(\mathbf{x} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}\)</span>, Weights: <span class="math inline">\(\mathbf{w} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}\)</span> (both positive), and Bias: <span class="math inline">\(b = 0\)</span>.</p>
<p>The neuron input is: <span class="math display">\[h = \mathbf{w}^T \mathbf{x} + b = (1)(1) + (1)(1) + 0 = 2\]</span></p>
<p>With <span class="math inline">\(h = 2 &gt; 0\)</span>, we get: <span class="math display">\[\hat{y} = \sigma(2) \approx 0.88 &gt; 0.5\]</span></p>
<p>The error becomes: <span class="math display">\[\delta = y - \hat{y} = 0 - 0.88 = -0.88 &lt; 0\]</span></p>
<p>Finally, the weight update is: <span class="math display">\[\Delta \mathbf{w} = \alpha \mathbf{x} \delta = \alpha \begin{bmatrix} 1 \\ 1 \end{bmatrix} \times (-0.88) = \begin{bmatrix} -0.88\alpha \\ -0.88\alpha \end{bmatrix} &lt; \mathbf{0}\]</span></p>
<p>Both weights <strong>decrease</strong>, not increase, contradicting the statement.</p>
<p>In general, the direction of weight change depends on the <strong>error</strong> <span class="math inline">\(\delta = y - \hat{y}\)</span>, not just the signs of <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{w}\)</span>. If the model <strong>over-predicts</strong> (<span class="math inline">\(\hat{y} &gt; y\)</span>), then <span class="math inline">\(\delta &lt; 0\)</span>, and weights move in the <strong>opposite</strong> direction of <span class="math inline">\(\mathbf{x}\)</span>. But if the model <strong>under-predicts</strong> (<span class="math inline">\(\hat{y} &lt; y\)</span>), then <span class="math inline">\(\delta &gt; 0\)</span>, and weights move in the <strong>same</strong> direction of <span class="math inline">\(\mathbf{x}\)</span>. Even with positive weights and positive inputs, the weights can decrease if the model over-predicts the target.</p>
<p><strong>Question 2</strong> Consider two training samples for a binary classifier:</p>
<p><strong>Sample A:</strong> <span class="math display">\[\mathbf{x}_A = \begin{bmatrix} 0.1 \\ 0.1 \end{bmatrix}, \quad y_A = 1\]</span></p>
<p><strong>Sample B:</strong> <span class="math display">\[\mathbf{x}_B = \begin{bmatrix} 10 \\ 10 \end{bmatrix}, \quad y_B = 1\]</span></p>
<p>Assume both samples produce the same error <span class="math inline">\(\delta = 0.5\)</span> (same under-prediction). Is the following statement True or False?</p>
<p>Since both samples have the same error, they will cause the same weight updates and contribute equally to learning.</p>
<p><strong>Solutions:</strong></p>
<p>The statement is <strong>False</strong>.</p>
<p>Sample B causes weight changes that are <strong>100 times larger</strong> than Sample A, even though both have identical errors.</p>
<p>In general, Gradient magnitude depends on input magnitude. In other words, weight updates scale with input feature values, not just the error. Hence, feature scaling is critical. Features with large values dominate weight updates while features with small values barely influence learning. Therefore, without normalization, the model learns some features much faster than others. Similarly, large input values can cause large weight swings and it may lead to oscillating or divergent training.</p>

</main>
</body>
</html>
