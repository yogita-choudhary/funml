<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>Lecture23 In-class Exercise</title>
  <link rel="stylesheet" href="../assets/style.css"/>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav>
  <a href="../index.html">Home</a>
</nav>
<main>
<div class="center">
<p><span><strong>In-Class Exercise (Canvas Quiz) — 15 minutes</strong></span><br />
<span><strong>Lecture 23: Explainability Paradigms and Evaluation</strong></span></p>
</div>
<p><strong>Context (only what you need for this quiz).</strong> We consider explanation <em>types</em> (indirect vs. direct), <em>targeted questions</em> (correlational / counterfactual / contrastive), and <em>evaluation</em> (human / application / network). In network evaluation, <em>insertion</em> and <em>deletion</em> curves summarize how predictions change as we add or remove pixels according to an explanation.</p>
<p><strong>Scenario.</strong> A CNN classifies an input image as <strong>Bullmastiff (P)</strong>. A second plausible class is <strong>Boxer (Q)</strong>.</p>
<p><strong>Tasks (answer directly):</strong></p>
<ol>
<li><p><strong>Indirect vs Direct.</strong> For each method below, label it as a <strong>Direct</strong> or <strong>Indirect</strong> explanation:</p>
<ol>
<li><p>Activation visualizations of intermediate layers.</p></li>
<li><p>Saliency map produced by occluding patches of the image and measuring the change in confidence for class <span class="math inline">\(P\)</span>.</p></li>
</ol></li>
<li><p><strong>Targeted explanation type.</strong> For each question below, classify it as: <span class="math display">\[\text{Correlation (Why $P$?)} \quad\text{or}\quad \text{Counterfactual (What if?)} \quad\text{or}\quad \text{Contrastive (Why $P$ rather than $Q$?)}\]</span></p>
<ol>
<li><p>“Why Bullmastiff?”</p></li>
<li><p>“What if the dog’s head shape were narrower?”</p></li>
<li><p>“Why Bullmastiff rather than Boxer?”</p></li>
</ol></li>
<li><p><strong>Evaluation type.</strong> Match each method to the correct category: <span class="math display">\[\text{Human Evaluation} \quad\text{or}\quad \text{Application Evaluation} \quad\text{or}\quad \text{Network Evaluation}\]</span></p>
<ol>
<li><p>43 Mechanical Turk workers choose which explanation is “better.”</p></li>
<li><p>Gaze tracking is used as a reference of salient regions and compared to explanation maps.</p></li>
<li><p>Progressive pixel-wise deletion is performed and an AUC score is computed.</p></li>
</ol></li>
<li><p><strong>Insertion / Deletion metric logic.</strong> Fill <strong>“low"</strong> or <strong>“high"</strong> in the blanks:</p>
<ol>
<li><p>A good explanation produces a <u></u> AUC for deletion.</p></li>
<li><p>A good explanation produces a <u></u> AUC for insertion.</p></li>
</ol></li>
<li><p><strong>Reasoning (masking evaluation pitfall).</strong> A masking-based network evaluation keeps only the pixels inside an explanation heatmap and measures the model’s confidence for class <span class="math inline">\(P\)</span>. Why might a <strong>larger heatmap</strong> appear to perform better under this evaluation, even if it is not minimal?</p>
<ol>
<li><p>Because a larger heatmap preserves more evidence, so the model’s confidence stays higher even if the explanation is not specific.</p></li>
<li><p>Because larger heatmaps always identify the unique minimal set of truly important pixels.</p></li>
<li><p>Because larger heatmaps guarantee a lower deletion AUC, which always indicates better explanations.</p></li>
<li><p>Because larger heatmaps necessarily reduce insertion AUC by adding evidence too quickly.</p></li>
</ol>
<p><strong>Canvas note:</strong> Canvas contains only keywords; full questions are shown on the projector.</p></li>
</ol>

</main>
</body>
</html>
