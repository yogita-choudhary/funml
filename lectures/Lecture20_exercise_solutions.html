<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>Lecture20 In-class Exercise Solutions</title>
  <link rel="stylesheet" href="../assets/style.css"/>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav>
  <a href="../index.html">Home</a>
</nav>
<main>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>_Lecture20_exercise_solutions</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div class="center">
<p><span><strong>In-Class Exercise — Questions + Solutions</strong></span><br />
<span><strong>Lecture 20: Sequence Modeling (RNN + Causal Convolution)</strong></span></p>
</div>
<p>We use the RNN update equations: <span class="math display">\[\mathbf{a}^{(t)}=\mathbf{b}+\mathbf{W}\mathbf{h}^{(t-1)}+\mathbf{U}\mathbf{x}^{(t)},
\qquad
\mathbf{h}^{(t)}=\tanh\!\left(\mathbf{a}^{(t)}\right).\]</span></p>
<p><strong>Question 1 (RNN memory — concept).</strong><br />
Which statement best describes why an RNN can model temporal dependence?</p>
<ol>
<li><p>Each time step uses a different parameter set, so it learns a separate model per time.</p></li>
<li><p>The hidden state <span class="math inline">\(\mathbf{h}^{(t)}\)</span> summarizes past inputs and is reused at future time steps.</p></li>
<li><p>The model sees future inputs <span class="math inline">\(\mathbf{x}^{(t+1)},\mathbf{x}^{(t+2)}\)</span> when computing <span class="math inline">\(\mathbf{h}^{(t)}\)</span>.</p></li>
<li><p>The nonlinearity <span class="math inline">\(\tanh(\cdot)\)</span> alone creates temporal memory even without <span class="math inline">\(\mathbf{W}\mathbf{h}^{(t-1)}\)</span>.</p></li>
</ol>
<p><strong>Solution 1:</strong><br />
Temporal dependence comes from feeding the previous hidden state into the current update via <span class="math inline">\(\mathbf{W}\mathbf{h}^{(t-1)}\)</span>. This makes <span class="math inline">\(\mathbf{h}^{(t)}\)</span> a running summary of the past. <span class="math display">\[\boxed{\textbf{Answer: (B)}}\]</span></p>
<p><strong>Question 2 (Dependency tracing — concept).</strong><br />
Assume <span class="math inline">\(\mathbf{h}^{(0)}=\mathbf{0}\)</span>. For <span class="math inline">\(t=2\)</span>, which inputs can influence <span class="math inline">\(\mathbf{h}^{(2)}\)</span>?</p>
<ol>
<li><p>Only <span class="math inline">\(\mathbf{x}^{(2)}\)</span></p></li>
<li><p>Only <span class="math inline">\(\mathbf{x}^{(1)}\)</span></p></li>
<li><p>Both <span class="math inline">\(\mathbf{x}^{(1)}\)</span> and <span class="math inline">\(\mathbf{x}^{(2)}\)</span></p></li>
<li><p>Neither <span class="math inline">\(\mathbf{x}^{(1)}\)</span> nor <span class="math inline">\(\mathbf{x}^{(2)}\)</span></p></li>
</ol>
<p><strong>Solution 2:</strong><br />
<span class="math inline">\(\mathbf{h}^{(2)}\)</span> depends on <span class="math inline">\(\mathbf{h}^{(1)}\)</span> and <span class="math inline">\(\mathbf{x}^{(2)}\)</span>. But <span class="math inline">\(\mathbf{h}^{(1)}\)</span> depends on <span class="math inline">\(\mathbf{x}^{(1)}\)</span>. Therefore both inputs can influence <span class="math inline">\(\mathbf{h}^{(2)}\)</span>. <span class="math display">\[\boxed{\textbf{Answer: (C)}}\]</span></p>
<p><strong>Question 3 (When does the RNN forget? — concept).</strong><br />
Which condition most directly removes dependence on the past hidden state?</p>
<ol>
<li><p><span class="math inline">\(\mathbf{U}=\mathbf{0}\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{W}=\mathbf{0}\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{b}=\mathbf{0}\)</span></p></li>
<li><p>Replacing <span class="math inline">\(\tanh(\cdot)\)</span> with <span class="math inline">\(\mathrm{ReLU}(\cdot)\)</span></p></li>
</ol>
<p><strong>Solution 3:</strong><br />
The only term that carries information from the past is <span class="math inline">\(\mathbf{W}\mathbf{h}^{(t-1)}\)</span>. Setting <span class="math inline">\(\mathbf{W}=\mathbf{0}\)</span> removes that pathway, so the state no longer depends on earlier time steps. <span class="math display">\[\boxed{\textbf{Answer: (B)}}\]</span></p>
<p><strong>Question 4 (Causal convolution — core idea).</strong><br />
For kernel length <span class="math inline">\(k\)</span>, how many zeros must be padded on the <em>left</em> to enforce causality?</p>
<ol>
<li><p><span class="math inline">\(k\)</span></p></li>
<li><p><span class="math inline">\(k-1\)</span></p></li>
<li><p><span class="math inline">\(k+1\)</span></p></li>
<li><p><span class="math inline">\(2k-1\)</span></p></li>
</ol>
<p><strong>Solution 4:</strong><br />
A causal 1D convolution with kernel length <span class="math inline">\(k\)</span> must not access future inputs. Left-padding by <span class="math inline">\(k-1\)</span> ensures the output at time <span class="math inline">\(t\)</span> can depend on <span class="math inline">\(\mathbf{x}^{(t)},\mathbf{x}^{(t-1)},\ldots,\mathbf{x}^{(t-k+1)}\)</span> only. <span class="math display">\[\boxed{\textbf{Answer: (B)}}\]</span></p>
<p><strong>Question 5 (Causal vs non-causal — concept check).</strong><br />
Which statement is correct?</p>
<ol>
<li><p>A causal convolution can use future inputs, but only during training.</p></li>
<li><p>A non-causal convolution may use <span class="math inline">\(\mathbf{x}^{(t+1)}\)</span> to compute the output at time <span class="math inline">\(t\)</span>.</p></li>
<li><p>Causality depends only on the activation function (e.g., <span class="math inline">\(\tanh\)</span> vs ReLU), not padding.</p></li>
<li><p>Causal convolutions require right-padding only.</p></li>
</ol>
<p><strong>Solution 5:</strong><br />
Non-causal convolutions (e.g., centered kernels) can incorporate future context; causal convolutions explicitly forbid that by design (via padding/alignment). <span class="math display">\[\boxed{\textbf{Answer: (B)}}\]</span></p>
</body>
</html>

</main>
</body>
</html>
