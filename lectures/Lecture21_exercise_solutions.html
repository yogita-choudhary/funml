<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>Lecture21 In-class Exercise Solutions</title>
  <link rel="stylesheet" href="../assets/style.css"/>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav>
  <a href="../index.html">Home</a>
</nav>
<main>
<div class="center">
<p><span><strong>In-Class Exercise â€” Questions + Solutions</strong></span><br />
<span><strong>Lecture 21: Sequence Modeling II (TCN + Attention)</strong></span></p>
</div>
<p><strong>Question 1 (TCN: single-layer receptive field).</strong><br />
A <em>single</em> dilated causal convolution layer has kernel size <span class="math inline">\(k=3\)</span> and dilation <span class="math inline">\(d=2\)</span>. Which input indices can affect <span class="math inline">\(y[t]\)</span> for this single layer?</p>
<ol>
<li><p><span class="math inline">\(\{t,\;t-1,\;t-2\}\)</span></p></li>
<li><p><span class="math inline">\(\{t,\;t-2,\;t-4\}\)</span></p></li>
<li><p><span class="math inline">\(\{t,\;t-2,\;t-3\}\)</span></p></li>
<li><p><span class="math inline">\(\{t,\;t-4,\;t-8\}\)</span></p></li>
</ol>
<p><strong>Solution 1:</strong><br />
A dilated causal convolution with dilation <span class="math inline">\(d\)</span> and kernel size <span class="math inline">\(k\)</span> uses <span class="math display">\[x[t],\;x[t-d],\;x[t-2d],\;\dots,\;x[t-(k-1)d].\]</span> With <span class="math inline">\(k=3\)</span> and <span class="math inline">\(d=2\)</span>, the indices are <span class="math inline">\(\{t,\;t-2,\;t-4\}\)</span>. <span class="math display">\[\boxed{\textbf{Answer: (B)}}\]</span></p>
<p><strong>Question 2 (TCN: concrete time index).</strong><br />
Using the same layer (<span class="math inline">\(k=3,d=2\)</span>), for <span class="math inline">\(t=10\)</span> which exact input indices are used?</p>
<ol>
<li><p><span class="math inline">\(\{10,\;9,\;8\}\)</span></p></li>
<li><p><span class="math inline">\(\{10,\;8,\;6\}\)</span></p></li>
<li><p><span class="math inline">\(\{10,\;6,\;2\}\)</span></p></li>
<li><p><span class="math inline">\(\{10,\;12,\;14\}\)</span></p></li>
</ol>
<p><strong>Solution 2:</strong><br />
Plug <span class="math inline">\(t=10\)</span> into <span class="math inline">\(\{t,\;t-2,\;t-4\}\)</span>: <span class="math display">\[\{10,\;8,\;6\}.\]</span> <span class="math display">\[\boxed{\textbf{Answer: (B)}}\]</span></p>
<p><strong>Question 3 (Attention: which token gets most weight?).</strong><br />
Dot-product attention uses <span class="math display">\[e_i=\mathbf{s}^\top \mathbf{h}_i,\qquad
\alpha_i=\frac{\exp(e_i)}{\sum_{j=1}^3 \exp(e_j)}.\]</span> Given <span class="math display">\[\mathbf{s}=\begin{bmatrix}1\\0\end{bmatrix},\quad
\mathbf{h}_1=\begin{bmatrix}1\\0\end{bmatrix},\quad
\mathbf{h}_2=\begin{bmatrix}0\\1\end{bmatrix},\quad
\mathbf{h}_3=\begin{bmatrix}-1\\0\end{bmatrix},\]</span> which vector receives the <em>largest</em> attention weight?</p>
<ol>
<li><p><span class="math inline">\(\mathbf{h}_1\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{h}_2\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{h}_3\)</span></p></li>
<li><p>All equal</p></li>
</ol>
<p><strong>Solution 3:</strong><br />
The largest weight corresponds to the largest score <span class="math inline">\(e_i=\mathbf{s}^\top \mathbf{h}_i\)</span>. Compute scores conceptually from alignment with <span class="math inline">\(\mathbf{s}=[1,0]^\top\)</span>: <span class="math display">\[e_1=1,\qquad e_2=0,\qquad e_3=-1.\]</span> So <span class="math inline">\(\alpha_1\)</span> is the largest. <span class="math display">\[\boxed{\textbf{Answer: (A)}}\]</span></p>
<p><strong>Question 4 (Attention: score ordering).</strong><br />
With the same <span class="math inline">\(\mathbf{s},\mathbf{h}_1,\mathbf{h}_2,\mathbf{h}_3\)</span>, which ordering is correct?</p>
<ol>
<li><p><span class="math inline">\(e_1 &gt; e_2 &gt; e_3\)</span></p></li>
<li><p><span class="math inline">\(e_2 &gt; e_1 &gt; e_3\)</span></p></li>
<li><p><span class="math inline">\(e_3 &gt; e_2 &gt; e_1\)</span></p></li>
<li><p><span class="math inline">\(e_1 = e_2 = e_3\)</span></p></li>
</ol>
<p><strong>Solution 4:</strong><br />
From Solution 3, <span class="math inline">\((e_1,e_2,e_3)=(1,0,-1)\)</span>, so <span class="math display">\[e_1&gt;e_2&gt;e_3.\]</span> <span class="math display">\[\boxed{\textbf{Answer: (A)}}\]</span></p>
<p><strong>Question 5 (Attention: direction/sign of context vector).</strong><br />
The context vector is <span class="math display">\[\mathbf{c}=\sum_{i=1}^3 \alpha_i \mathbf{h}_i.\]</span> Without computing the full softmax numerically, which statement is correct about <span class="math inline">\(\mathbf{c}\)</span>?</p>
<ol>
<li><p>The first component of <span class="math inline">\(\mathbf{c}\)</span> is positive and the second component is positive.</p></li>
<li><p>The first component of <span class="math inline">\(\mathbf{c}\)</span> is negative and the second component is positive.</p></li>
<li><p>The first component of <span class="math inline">\(\mathbf{c}\)</span> is zero and the second component is positive.</p></li>
<li><p>Both components of <span class="math inline">\(\mathbf{c}\)</span> are zero.</p></li>
</ol>
<p><strong>Solution 5:</strong><br />
We have <span class="math inline">\(\alpha_1&gt;\alpha_2&gt;\alpha_3\)</span> and <span class="math display">\[\mathbf{h}_1=\begin{bmatrix}1\\0\end{bmatrix},\quad
\mathbf{h}_2=\begin{bmatrix}0\\1\end{bmatrix},\quad
\mathbf{h}_3=\begin{bmatrix}-1\\0\end{bmatrix}.\]</span> So <span class="math display">\[\mathbf{c}
=\alpha_1\begin{bmatrix}1\\0\end{bmatrix}
+\alpha_2\begin{bmatrix}0\\1\end{bmatrix}
+\alpha_3\begin{bmatrix}-1\\0\end{bmatrix}
=
\begin{bmatrix}\alpha_1-\alpha_3\\ \alpha_2\end{bmatrix}.\]</span> Since <span class="math inline">\(\alpha_1&gt;\alpha_3\)</span>, the first component is positive; since <span class="math inline">\(\alpha_2&gt;0\)</span>, the second component is positive. <span class="math display">\[\boxed{\textbf{Answer: (A)}}\]</span></p>

</main>
</body>
</html>
