<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>Lecture13 In-class Exercise</title>
  <link rel="stylesheet" href="../assets/style.css"/>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav>
  <a href="../index.html">Home</a>
</nav>
<main>
<div class="center">
<p><span><strong>In-Class Exercise (Canvas Quiz) â€” 10 minutes</strong></span><br />
<span><strong>Lecture 13: Neural Networks</strong></span></p>
</div>
<p>Answer all questions. Show work on scratch paper if helpful.</p>
<ol>
<li><p><strong>Forward pass with ReLU</strong><br />
A neuron computes <span class="math inline">\(h=\bm{w}^T\bm{x}+b\)</span> and outputs <span class="math inline">\(\mathrm{ReLU}(h)=\max(0,h)\)</span>.<br />
Given <span class="math display">\[\bm{x}=\begin{bmatrix}1\\-2\end{bmatrix},\quad
\bm{w}=\begin{bmatrix}2\\1\end{bmatrix},\quad
b=-1,\]</span> what is the output after ReLU?</p>
<ul>
<li><p><span class="math inline">\(0\)</span></p></li>
<li><p><span class="math inline">\(1\)</span></p></li>
<li><p><span class="math inline">\(3\)</span></p></li>
</ul></li>
<li><p><strong>Why add nonlinearities?</strong><br />
What is the main reason we include nonlinear activation functions (e.g., ReLU, sigmoid) in neural networks?</p>
<ul>
<li><p>They make the optimization problem convex.</p></li>
<li><p>They allow the network to represent non-linear functions/decision boundaries.</p></li>
<li><p>They guarantee perfect generalization.</p></li>
</ul></li>
<li><p><strong>Softmax sanity check</strong><br />
Softmax maps scores <span class="math inline">\(\bm{s}=[s_1,s_2,s_3]\)</span> to <span class="math display">\[p_k=\frac{e^{s_k}}{\sum_{j=1}^3 e^{s_j}}.\]</span> Which statement is always true?</p>
<ul>
<li><p>Some <span class="math inline">\(p_k\)</span> can be negative.</p></li>
<li><p><span class="math inline">\(\sum_{k=1}^3 p_k = 1\)</span>.</p></li>
<li><p>The largest score always maps to probability exactly <span class="math inline">\(1\)</span>.</p></li>
</ul></li>
<li><p><strong>Cross-entropy intuition</strong><br />
If the true class is class 2, the loss is <span class="math inline">\(L=-\log(p_2)\)</span>. Which prediction gives the smallest loss?</p>
<ul>
<li><p><span class="math inline">\(p_2 = 0.90\)</span></p></li>
<li><p><span class="math inline">\(p_2 = 0.40\)</span></p></li>
<li><p><span class="math inline">\(p_2 = 0.10\)</span></p></li>
</ul></li>
<li><p><strong>Counting parameters</strong><br />
Fully-connected network: <span class="math display">\[\text{Input }(4)\rightarrow \text{Hidden }(3)\rightarrow \text{Output }(2),\]</span> with biases in each layer. How many trainable parameters (weights + biases) are there?</p>
<ul>
<li><p><span class="math inline">\(4\cdot3 + 3\cdot2\)</span></p></li>
<li><p><span class="math inline">\(4\cdot3 + 3 + 3\cdot2 + 2\)</span></p></li>
<li><p><span class="math inline">\(4 + 3 + 2\)</span></p></li>
</ul></li>
</ol>

</main>
</body>
</html>
