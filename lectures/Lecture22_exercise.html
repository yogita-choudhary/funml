<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>Lecture22 In-class Exercise</title>
  <link rel="stylesheet" href="../assets/style.css"/>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav>
  <a href="../index.html">Home</a>
</nav>
<main>
<div class="center">
<p><span><strong>In-Class Exercise (Canvas Quiz) â€” 10 minutes</strong></span><br />
<span><strong>Lecture 22: Explainability in Neural Networks</strong></span></p>
</div>
<p>We study explanation methods for CNN classifiers:</p>
<ul>
<li><p><strong>Occlusion saliency:</strong> mask a region and observe the drop in class probability.</p></li>
<li><p><strong>Gradient saliency:</strong> use <span class="math inline">\(\frac{\partial y_c}{\partial \mathbf{x}}\)</span> as pixel importance.</p></li>
<li><p><strong>Grad-CAM:</strong> uses gradients w.r.t. the last convolutional feature maps <span class="math inline">\(\mathbf{A}^k\)</span>: <span class="math display">\[\alpha_k^c = \frac{1}{Z}\sum_i\sum_j \frac{\partial y_c}{\partial A_{ij}^k},
\qquad
L_{\text{Grad-CAM}}^{c} = \mathrm{ReLU}\!\left(\sum_k \alpha_k^c \mathbf{A}^k\right).\]</span></p></li>
</ul>
<p><strong>Given:</strong> A classifier predicts class <span class="math inline">\(c\)</span> with probability <span class="math inline">\(p=0.85\)</span>. Masking three patches gives probabilities: <span class="math display">\[p_1=0.60,\qquad p_2=0.82,\qquad p_3=0.30.\]</span></p>
<p>For Grad-CAM, suppose the last convolutional layer has two feature maps: <span class="math display">\[\mathbf{A}^1=
\begin{bmatrix}
1 &amp; 2\\
0 &amp; 1
\end{bmatrix},
\qquad
\mathbf{A}^2=
\begin{bmatrix}
2 &amp; 1\\
1 &amp; 0
\end{bmatrix},
\qquad
\alpha_1^c=0.50,\quad \alpha_2^c=-0.25.\]</span></p>
<ol>
<li><p><strong>Occlusion Saliency (Most Important Patch)</strong><br />
Which patch is <strong>most important</strong> for predicting class <span class="math inline">\(c\)</span> (largest probability drop)?</p>
<ol>
<li><p>Patch 1</p></li>
<li><p>Patch 2</p></li>
<li><p>Patch 3</p></li>
<li><p>All patches are equally important</p></li>
</ol></li>
<li><p><strong>Faithfulness vs Cost (Occlusion)</strong><br />
Why is occlusion saliency considered <strong>computationally expensive</strong>?</p>
<ol>
<li><p>It requires retraining the model.</p></li>
<li><p>It requires many forward passes with masked inputs.</p></li>
<li><p>It needs gradients of all layers.</p></li>
<li><p>It increases the number of model parameters.</p></li>
</ol></li>
<li><p><strong>Gradient Saliency (Concept)</strong><br />
Why do input gradients approximate pixel importance?</p>
<ol>
<li><p>They linearize the model locally around the input.</p></li>
<li><p>They guarantee causal explanations.</p></li>
<li><p>They remove nonlinearities from the network.</p></li>
<li><p>They compute feature correlations.</p></li>
</ol></li>
<li><p><strong>Why the Last Convolutional Layer?</strong><br />
Grad-CAM uses the <strong>last convolutional layer</strong> because it:</p>
<ol>
<li><p>Has the largest number of parameters.</p></li>
<li><p>Preserves spatial structure and captures high-level semantics.</p></li>
<li><p>Produces the final class probabilities.</p></li>
<li><p>Eliminates the need for backpropagation.</p></li>
</ol></li>
<li><p><strong>Grad-CAM (After ReLU)</strong><br />
Let <span class="math inline">\(S=\alpha_1^c\mathbf{A}^1+\alpha_2^c\mathbf{A}^2\)</span>. After applying <span class="math inline">\(L_{\text{Grad-CAM}}^{c}=\mathrm{ReLU}(S)\)</span>, what happens to <strong>negative</strong> entries of <span class="math inline">\(S\)</span>?</p>
<ol>
<li><p>They become positive.</p></li>
<li><p>They remain negative.</p></li>
<li><p>They become zero.</p></li>
<li><p>They are doubled in magnitude.</p></li>
</ol></li>
<li><p><strong>Concept Check (True/False)</strong><br />
Guided Backpropagation can produce sharp visual explanations but may fail to be <strong>class-discriminative</strong>.</p>
<ol>
<li><p>True</p></li>
<li><p>False</p></li>
</ol></li>
</ol>

</main>
</body>
</html>
