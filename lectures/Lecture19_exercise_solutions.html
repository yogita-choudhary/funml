<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>Lecture19 In-class Exercise Solutions</title>
  <link rel="stylesheet" href="../assets/style.css"/>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav>
  <a href="../index.html">Home</a>
</nav>
<main>
<div class="center">
<p><span><strong>In-Class Exercise (Canvas Quiz) — 10 minutes</strong></span><br />
<span><strong>Lecture 19: Autoencoder Extensions (Sparse, Denoising, VAE)</strong></span></p>
</div>
<p>This quiz focuses on three regularized autoencoders discussed in Lecture 19:</p>
<ul>
<li><p><strong>Sparse AE</strong>: encourages sparse latent activations (e.g., via an <span class="math inline">\(\ell_1\)</span> penalty)</p></li>
<li><p><strong>Denoising AE</strong>: reconstructs clean <span class="math inline">\(\mathbf{X}\)</span> from corrupted <span class="math inline">\(\tilde{\mathbf{X}}\)</span></p></li>
<li><p><strong>VAE</strong>: models latent variables probabilistically and uses the reparameterization trick</p></li>
</ul>
<p><strong>Question 1 (Sparse AE — what sparsity does):</strong></p>
<p>A sparse autoencoder trains with <span class="math display">\[\mathcal{L}(\mathbf{x},\hat{\mathbf{x}}) = \|\mathbf{x}-\hat{\mathbf{x}}\|_2^2 + \lambda \|\mathbf{z}\|_1.\]</span> Which statement best captures the effect of the <span class="math inline">\(\lambda\|\mathbf{z}\|_1\)</span> term?</p>
<ol>
<li><p>It forces every coordinate of <span class="math inline">\(\mathbf{z}\)</span> to be large in magnitude.</p></li>
<li><p>It encourages many coordinates of <span class="math inline">\(\mathbf{z}\)</span> to be exactly (or near) zero, so only a few latent units are active.</p></li>
<li><p>It makes the autoencoder supervised by introducing labels.</p></li>
<li><p>It guarantees perfect reconstruction on any dataset.</p></li>
</ol>
<p><strong>Solution 1:</strong><br />
The <span class="math inline">\(\ell_1\)</span> penalty encourages sparsity in the latent activations, meaning only a few components of <span class="math inline">\(\mathbf{z}\)</span> are active. <span class="math display">\[\boxed{\textbf{Answer: (B)}}\]</span></p>
<p><strong>Question 2 (Sparse AE — effect of increasing <span class="math inline">\(\lambda\)</span>):</strong></p>
<p>Suppose you increase <span class="math inline">\(\lambda\)</span> while keeping the model architecture fixed. Which outcome is <strong>most likely</strong> on the training set?</p>
<ol>
<li><p>Latent activations become less sparse; more units activate.</p></li>
<li><p>Latent activations become more sparse, but reconstruction quality may worsen.</p></li>
<li><p>Reconstruction must improve because regularization always helps.</p></li>
<li><p>The encoder becomes the exact inverse of the decoder.</p></li>
</ol>
<p><strong>Solution 2:</strong><br />
Higher <span class="math inline">\(\lambda\)</span> increases the sparsity pressure, often reducing active units but potentially harming reconstruction. <span class="math display">\[\boxed{\textbf{Answer: (B)}}\]</span></p>
<p><strong>Question 3 (Denoising AE — what loss compares):</strong></p>
<p>A denoising autoencoder uses <span class="math display">\[\hat{\mathbf{X}} = G_{\boldsymbol{\phi}}\!\left(E_{\boldsymbol{\theta}}(\tilde{\mathbf{X}})\right),
\qquad
\min \ \|\mathbf{X}-\hat{\mathbf{X}}\|_2^2.\]</span> Choose the correct pairing of <strong>input to encoder</strong> and <strong>target in the loss</strong>.</p>
<ol>
<li><p>Input: <span class="math inline">\(\mathbf{X}\)</span> Target: <span class="math inline">\(\tilde{\mathbf{X}}\)</span></p></li>
<li><p>Input: <span class="math inline">\(\tilde{\mathbf{X}}\)</span> Target: <span class="math inline">\(\mathbf{X}\)</span></p></li>
<li><p>Input: <span class="math inline">\(\tilde{\mathbf{X}}\)</span> Target: <span class="math inline">\(\tilde{\mathbf{X}}\)</span></p></li>
<li><p>Input: <span class="math inline">\(\mathbf{X}\)</span> Target: <span class="math inline">\(\mathbf{X}\)</span> (no corruption needed)</p></li>
</ol>
<p><strong>Solution 3:</strong><br />
The encoder consumes the corrupted input <span class="math inline">\(\tilde{\mathbf{X}}\)</span>, but the loss compares reconstruction to the clean data <span class="math inline">\(\mathbf{X}\)</span>. <span class="math display">\[\boxed{\textbf{Answer: (B)}}\]</span></p>
<p><strong>Question 4 (Denoising AE — what it learns conceptually):</strong></p>
<p>Why can denoising autoencoders learn <em>more robust</em> representations than plain autoencoders?</p>
<ol>
<li><p>They are trained to copy noise exactly, so they become noise-sensitive.</p></li>
<li><p>They must map corrupted inputs back toward clean structure, encouraging features stable under corruption.</p></li>
<li><p>They guarantee the latent space follows a Gaussian distribution.</p></li>
<li><p>They do not use any reconstruction loss.</p></li>
</ol>
<p><strong>Solution 4:</strong><br />
Training on corrupted inputs with a clean target encourages features that are stable under noise/corruption. <span class="math display">\[\boxed{\textbf{Answer: (B)}}\]</span></p>
<p><strong>Question 5 (VAE — what reparameterization enables):</strong></p>
<p>A VAE samples <span class="math display">\[\mathbf{z} = \boldsymbol{\mu} + \boldsymbol{\sigma}\odot \boldsymbol{\epsilon},
\qquad
\boldsymbol{\epsilon}\sim \mathcal{N}(\mathbf{0},\mathbf{I}),\]</span> where <span class="math inline">\(\odot\)</span> is elementwise multiplication. What is the main reason we write sampling in this form?</p>
<ol>
<li><p>It removes randomness completely so the model becomes deterministic.</p></li>
<li><p>It allows gradients to backpropagate through <span class="math inline">\(\boldsymbol{\mu}\)</span> and <span class="math inline">\(\boldsymbol{\sigma}\)</span> despite stochastic sampling.</p></li>
<li><p>It makes the decoder a strict inverse of the encoder.</p></li>
<li><p>It guarantees that <span class="math inline">\(\mathbf{z}\)</span> has no variance.</p></li>
</ol>
<p><strong>Solution 5:</strong><br />
The reparameterization trick isolates randomness in <span class="math inline">\(\boldsymbol{\epsilon}\)</span>, enabling gradient-based optimization of <span class="math inline">\(\boldsymbol{\mu}\)</span> and <span class="math inline">\(\boldsymbol{\sigma}\)</span>. <span class="math display">\[\boxed{\textbf{Answer: (B)}}\]</span></p>

</main>
</body>
</html>
