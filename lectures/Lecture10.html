<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>ECE4252-8803_Scribe_Notes_Lecture_10</title>
  <link rel="stylesheet" href="../assets/style.css"/>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav>
  <a href="../index.html">Home</a>
</nav>
<main>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>_Lecture10</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div class="center">

</div>
<p>Dr. Ahmad Mustafa, Dr. Motaz Alfarraj, Dr. Ashraf Alattar, Dr. Chen Zhou</p>
<p>with remarkable contributions include: Kuo-Wei Lai, Wuyang Du, Shiva Mahato, Michael Zhou, Ninghan Zhong</p>
<p>: <span>All content of these notes are part of this course at Georgia Tech. Any re-use or distribution is not permitted without pre-approved permission. All these notes belong to, created by, and copyrighted for Ghassan AlRegib and Mohit Prabhushankar, Georgia Tech, 2021–2028.</span></p>
<p>: <span>These lecture notes are licensed under the <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.</span></p>
<p>: <span><em>Please submit any errata you find using the following form: <a href="https://forms.office.com/r/fbg9dMWPgY">Errata Form for FunML Textbook</a> or visit: <a href="https://forms.office.com/r/fbg9dMWPgY">https://forms.office.com/r/fbg9dMWPgY</a></em></span></p>
<section id="introduction-to-clustering" data-number="0.1">
<h2 data-number="1.1"><span class="header-section-number">1.1</span> Introduction to Clustering</h2>
<section id="recap" data-number="0.1.1">
<h3 data-number="1.1.1"><span class="header-section-number">1.1.1</span> Recap</h3>
<p>In supervised learning, we train models using labeled data. Each training example comes as a sample–label pair, and the model learns a mapping from inputs to outputs. For instance, we might train an image classifier using images of cats labeled as 0 and dogs labeled as 1. Similarly, we could label text with the emotion it expresses and train a model to detect emotion from new text.</p>
<p>In contrast, unsupervised learning does not require labels. Instead of learning from sample–label pairs, the goal is to discover structure in the data by grouping together samples that share similar characteristics. For example, given a collection of audio clips, an unsupervised method might group them by speaker identity based on voice characteristics. Likewise, words can be grouped based on the contexts in which they commonly appear, forming clusters of semantically related terms.</p>
<p>Clustering is a core example of unsupervised learning. The objective is to group a set of samples so that items in the same group (called a <em>cluster</em>) are more similar to each other than they are to items in other clusters. To do this effectively, we need to make several key choices. First, we must decide which features of the samples will be used for comparison. Next, we need a proximity (or similarity/distance) measure that quantifies how close two samples are based on those features. Finally, we need a clustering algorithm that uses these proximity relationships to assign similar samples to the same cluster.</p>
</section>
<section id="common-uses" data-number="0.1.2">
<h3 data-number="1.1.2"><span class="header-section-number">1.1.2</span> Common Uses</h3>
<p>Clustering has many practical applications across science, engineering, and industry. One of the most well-known applications is <strong>image segmentation</strong>. In image segmentation, clustering is used to partition a digital image into multiple regions. Each region groups pixels that are similar in some way, such as color, brightness, or texture. The result is a simplified representation of the image that highlights important structures and boundaries, which is useful in medical imaging, autonomous driving, and computer vision.</p>
<p>Another common application is <strong>document and news article clustering</strong>. In this setting, each article is treated as a data point and represented by features derived from the words it contains (for example, word frequencies or embeddings). Clustering can then automatically group articles into topics such as politics, sports, or technology. This task is challenging because news topics evolve over time, making the data dynamic and constantly changing.</p>
<p>Clustering also plays an important role in scientific discovery. For example, <strong>language clustering</strong> is used in linguistics to group languages based on structural similarity, and <strong>species clustering</strong> is used in evolutionary biology to group organisms according to shared traits and behaviors.</p>
<p>A non-exhaustive list of additional applications includes:</p>
<div class="multicols">
<p><span>2</span></p>
<ul>
<li><p>Recommender systems</p></li>
<li><p>Genome sequence analysis</p></li>
<li><p>Analysis of antimicrobial activity</p></li>
<li><p>Grouping of shopping items</p></li>
<li><p>Search result grouping</p></li>
<li><p>Slippy map optimization</p></li>
<li><p>Crime analysis</p></li>
<li><p>Climatology</p></li>
</ul>
</div>
</section>
<section id="terminologies" data-number="0.1.3">
<h3 data-number="1.1.3"><span class="header-section-number">1.1.3</span> Terminologies</h3>
<p>Figure <a href="#clusteringdefs" data-reference-type="ref" data-reference="clusteringdefs">1</a> illustrates several core concepts that are used throughout clustering methods. Before studying specific algorithms, it is important to understand the terminology that describes how clustering works.</p>
<p><strong>Features.</strong> Each data sample is described by a set of measurable attributes called <em>features</em>. For a dataset with <span class="math inline">\(N\)</span> samples and <span class="math inline">\(P\)</span> features, the <span class="math inline">\(i\)</span>th sample is represented as <span class="math display">\[\mathbf{x}_i = [x_{i1}, x_{i2}, \ldots, x_{iP}]^T .\]</span> This means each sample corresponds to a point in a <span class="math inline">\(P\)</span>-dimensional feature space. For example, if we cluster news articles, features may represent word frequencies; if we cluster images, features may represent color or texture statistics.</p>
<p><strong>Proximity measures.</strong> Clustering relies on a notion of similarity (or dissimilarity) between samples. This is quantified using a <em>proximity measure</em>. In the illustrative figure, the proximity measure is the <span class="math inline">\(\ell_2\)</span> distance (Euclidean distance), which measures how far apart two samples are in the feature space. Samples that are closer together are considered more similar.</p>
<p><strong>Clusters.</strong> A <em>cluster</em> is a group of samples that are more similar to each other than to samples in other groups. In Figure <a href="#clusteringdefs" data-reference-type="ref" data-reference="clusteringdefs">1</a>, the clusters are visually represented using different colors.</p>
<p><strong>Hard vs. soft clustering.</strong> In <strong>hard clustering</strong> (also called <em>crisp clustering</em>), each sample belongs to exactly one cluster. This is the scenario shown in the figure. In contrast, <strong>soft (fuzzy) clustering</strong> allows each sample to belong to multiple clusters with different degrees of membership. For example, a sample might belong to one cluster with probability <span class="math inline">\(0.7\)</span> and another cluster with probability <span class="math inline">\(0.3\)</span>, reflecting uncertainty or overlap between groups.</p>
<figure>
<img src="img/lecture10/clusteringdefinitions.png" id="clusteringdefs" alt="Illustration of key clustering terminology." /><figcaption aria-hidden="true">Illustration of key clustering terminology.</figcaption>
</figure>
</section>
<section id="formalization" data-number="0.1.4">
<h3 data-number="1.1.4"><span class="header-section-number">1.1.4</span> Formalization</h3>
<p>We now describe clustering more formally. Suppose we are given a dataset <span class="math display">\[\mathbf{X} = \{\mathbf{x}_1,\ldots,\mathbf{x}_N\},\]</span> where each sample <span class="math inline">\(\mathbf{x}_i \in \mathbb{R}^P\)</span> is a point in a <span class="math inline">\(P\)</span>-dimensional feature space.</p>
<p>Clustering aims to partition the dataset into <span class="math inline">\(K\)</span> groups, <span class="math display">\[\{\mathcal{C}_1, \mathcal{C}_2, \ldots, \mathcal{C}_K\},\]</span> called clusters. A valid clustering must satisfy three properties:</p>
<ol>
<li><p><strong>All samples are assigned to a cluster</strong> <span class="math display">\[\bigcup_{k=1}^{K} \mathcal{C}_k = \mathbf{X}.\]</span> Every data point must belong to some cluster.</p></li>
<li><p><strong>Clusters are non-empty</strong> <span class="math display">\[\mathcal{C}_k \neq \varnothing, \quad k=1,\ldots,K.\]</span> Each cluster must contain at least one sample.</p></li>
<li><p><strong>Clusters do not overlap (hard clustering)</strong> <span class="math display">\[\mathcal{C}_k \cap \mathcal{C}_j = \varnothing, \quad k \neq j.\]</span> In hard clustering, each sample belongs to only one cluster.</p></li>
</ol>
<section id="example." data-number="0.1.4.0.1">
<h5 data-number="1.1.4.0.1"><span class="header-section-number">1.1.4.0.1</span> Example.</h5>
<p>In Figure <a href="#clusteringdefs" data-reference-type="ref" data-reference="clusteringdefs">1</a>, all samples are assigned a color, so the entire dataset is covered by the clusters. Each colored region contains at least one point, so no cluster is empty. Finally, each sample has exactly one color and clusters do not overlap, making this an example of <strong>hard clustering</strong>.</p>
</section>
</section>
</section>
<section id="proximity-measures" data-number="0.2">
<h2 data-number="1.2"><span class="header-section-number">1.2</span> Proximity Measures</h2>
<p>Clustering relies on a notion of similarity between data points. To group samples into clusters, we need a way to quantify how similar or dissimilar two samples <span class="math inline">\(\mathbf{x}_j\)</span> and <span class="math inline">\(\mathbf{x}_k\)</span> are.</p>
<p>A <strong>similarity function</strong> <span class="math inline">\(s(\mathbf{x}_j,\mathbf{x}_k)\)</span> produces large values when samples are similar (for example between 0 and 1), while a <strong>distance (dissimilarity) function</strong> <span class="math inline">\(d(\mathbf{x}_j,\mathbf{x}_k)\)</span> produces large values when samples are far apart (ranging from 0 to <span class="math inline">\(+\infty\)</span>). In practice, clustering algorithms may use either similarity or distance measures depending on the application.</p>
<p>In the following subsections, we introduce several commonly used proximity measures.</p>
<section id="running-example-used-throughout-this-section" data-number="0.2.0.0.1">
<h5 data-number="1.2.0.0.1"><span class="header-section-number">1.2.0.0.1</span> Running Example (used throughout this section)</h5>
<p>To compare proximity measures, we use the same four 2-D samples from Lecture 10 (slide 27):</p>
<p><span class="math display">\[\mathbf{x}_1=(0,2),\quad
\mathbf{x}_2=(2,0),\quad
\mathbf{x}_3=(3,1),\quad
\mathbf{x}_4=(5,1).\]</span></p>
<table>
<caption>Running example used to compare proximity measures.</caption>
<thead>
<tr class="header">
<th style="text-align: center;">sample</th>
<th style="text-align: center;"><span class="math inline">\(x_{i,1}\)</span></th>
<th style="text-align: center;"><span class="math inline">\(x_{i,2}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(\mathbf{x}_1\)</span></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(\mathbf{x}_2\)</span></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(\mathbf{x}_3\)</span></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(\mathbf{x}_4\)</span></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<figure>
<img src="img/lecture10/running_example.png" id="fig:running-example" alt="Visualization of the running example points used throughout Section 10.2." /><figcaption aria-hidden="true">Visualization of the running example points used throughout Section 10.2.</figcaption>
</figure>
<p>We will compute pairwise distances using different metrics and observe how the notion of “closeness’’ changes depending on the chosen measure.</p>
</section>
<section id="euclidean-distance" data-number="0.2.1">
<h3 data-number="1.2.1"><span class="header-section-number">1.2.1</span> Euclidean Distance</h3>
<p>The most familiar distance is the <strong>Euclidean distance</strong>: <span class="math display">\[d(\mathbf{x}_j,\mathbf{x}_k)
=\sqrt{\sum_{i=1}^{P}(x_{ji}-x_{ki})^2}.\]</span></p>
<p>This is the straight-line distance between two points in feature space and is therefore often called the <strong><span class="math inline">\(L_2\)</span> distance</strong>.</p>
<section id="geometric-intuition." data-number="0.2.1.0.1">
<h5 data-number="1.2.1.0.1"><span class="header-section-number">1.2.1.0.1</span> Geometric intuition.</h5>
<p>Euclidean distance measures the length of the shortest path between two points. Because squared differences are used, larger coordinate differences have a stronger influence on the distance. This metric works best when features are dense and measured on comparable scales.</p>
</section>
<section id="euclidean-distances-on-the-running-example" data-number="0.2.1.0.2">
<h5 data-number="1.2.1.0.2"><span class="header-section-number">1.2.1.0.2</span> Euclidean distances on the running example</h5>
<p>Example calculation: <span class="math display">\[d_2(\mathbf{x}_1,\mathbf{x}_2)
=\sqrt{(0-2)^2+(2-0)^2}
=\sqrt{8}\approx 2.828\]</span></p>
<table>
<caption>Pairwise Euclidean distances for the running example.</caption>
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(r=2\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\mathbf{x}_1\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\mathbf{x}_2\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\mathbf{x}_3\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\mathbf{x}_4\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(\mathbf{x}_1\)</span></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2.828</td>
<td style="text-align: center;">3.162</td>
<td style="text-align: center;">5.099</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(\mathbf{x}_2\)</span></td>
<td style="text-align: center;">2.828</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1.414</td>
<td style="text-align: center;">3.162</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(\mathbf{x}_3\)</span></td>
<td style="text-align: center;">3.162</td>
<td style="text-align: center;">1.414</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(\mathbf{x}_4\)</span></td>
<td style="text-align: center;">5.099</td>
<td style="text-align: center;">3.162</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
</section>
<section id="interpretation." data-number="0.2.1.0.3">
<h5 data-number="1.2.1.0.3"><span class="header-section-number">1.2.1.0.3</span> Interpretation.</h5>
<p>Under Euclidean distance, the closest pair is <span class="math inline">\(\mathbf{x}_2\)</span> and <span class="math inline">\(\mathbf{x}_3\)</span> (<span class="math inline">\(d\approx1.414\)</span>). Comparing this table with the Manhattan and <span class="math inline">\(L_\infty\)</span> results shows that different metrics can change which points are considered nearest neighbors.</p>
</section>
</section>
<section id="manhattan-distance" data-number="0.2.2">
<h3 data-number="1.2.2"><span class="header-section-number">1.2.2</span> Manhattan Distance</h3>
<p>The <strong>Manhattan distance</strong> measures the sum of absolute differences: <span class="math display">\[d(\mathbf{x}_j,\mathbf{x}_k)=\sum_{i=1}^{P}|x_{ji}-x_{ki}|.\]</span></p>
<p>Also known as the <strong><span class="math inline">\(L_1\)</span> distance</strong> or <strong>city-block distance</strong>, this metric measures distance as if we were moving along a grid of horizontal and vertical streets.</p>
<section id="geometric-intuition.-1" data-number="0.2.2.0.1">
<h5 data-number="1.2.2.0.1"><span class="header-section-number">1.2.2.0.1</span> Geometric intuition.</h5>
<p>Unlike Euclidean distance (straight-line distance), Manhattan distance restricts movement to axis-aligned directions. This makes it less sensitive to a single large coordinate difference and often more robust in high-dimensional or sparse settings.</p>
</section>
<section id="manhattan-distances-on-the-running-example" data-number="0.2.2.0.2">
<h5 data-number="1.2.2.0.2"><span class="header-section-number">1.2.2.0.2</span> Manhattan distances on the running example</h5>
<p>Example calculation: <span class="math display">\[d_1(\mathbf{x}_1,\mathbf{x}_2)=|0-2|+|2-0|=4\]</span></p>
<table>
<caption>Pairwise Manhattan distances for the running example.</caption>
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(r=1\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\mathbf{x}_1\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\mathbf{x}_2\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\mathbf{x}_3\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\mathbf{x}_4\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(\mathbf{x}_1\)</span></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">6</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(\mathbf{x}_2\)</span></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(\mathbf{x}_3\)</span></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(\mathbf{x}_4\)</span></td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
</section>
<section id="interpretation.-1" data-number="0.2.2.0.3">
<h5 data-number="1.2.2.0.3"><span class="header-section-number">1.2.2.0.3</span> Interpretation.</h5>
<p>Notice that <span class="math inline">\(\mathbf{x}_2\)</span> and <span class="math inline">\(\mathbf{x}_3\)</span> are the closest pair under Manhattan distance (<span class="math inline">\(d=2\)</span>). The ordering of nearest neighbors differs from Euclidean distance, highlighting how the choice of metric changes our notion of “closeness.’’</p>
</section>
</section>
<section id="minkowski-distance" data-number="0.2.3">
<h3 data-number="1.2.3"><span class="header-section-number">1.2.3</span> Minkowski Distance</h3>
<p>The <strong>Minkowski distance</strong> provides a single family of distances that includes many commonly used metrics as special cases: <span class="math display">\[d_r(\mathbf{x}_j,\mathbf{x}_k)
=\left(\sum_{i=1}^{P}|x_{ji}-x_{ki}|^r\right)^{1/r}.\]</span></p>
<p>Rather than defining separate distance formulas, Minkowski distance introduces a parameter <span class="math inline">\(r\)</span> that <em>controls the geometry</em> of distance. By changing <span class="math inline">\(r\)</span>, we smoothly move between different notions of closeness.</p>
<section id="special-cases." data-number="0.2.3.0.1">
<h5 data-number="1.2.3.0.1"><span class="header-section-number">1.2.3.0.1</span> Special cases.</h5>
<ul>
<li><p><span class="math inline">\(r=1\)</span> → Manhattan distance (<span class="math inline">\(L_1\)</span>)</p></li>
<li><p><span class="math inline">\(r=2\)</span> → Euclidean distance (<span class="math inline">\(L_2\)</span>)</p></li>
<li><p><span class="math inline">\(r\to\infty\)</span> → Maximum coordinate difference (<span class="math inline">\(L_\infty\)</span>)</p></li>
<li><p><span class="math inline">\(r=0\)</span> → Number of differing coordinates (<span class="math inline">\(L_0\)</span>, not a true norm)</p></li>
</ul>
</section>
<section id="geometric-intuition.-2" data-number="0.2.3.0.2">
<h5 data-number="1.2.3.0.2"><span class="header-section-number">1.2.3.0.2</span> Geometric intuition.</h5>
<p>The parameter <span class="math inline">\(r\)</span> controls how strongly large coordinate differences affect the distance:</p>
<ul>
<li><p>Small <span class="math inline">\(r\)</span> (near <span class="math inline">\(1\)</span>): distances depend on the <em>sum</em> of differences.</p></li>
<li><p>Large <span class="math inline">\(r\)</span>: distances are dominated by the <em>largest</em> difference.</p></li>
</ul>
<p>Thus, Minkowski distance acts as a “tuning knob’’ that determines how we measure closeness in feature space.</p>
</section>
<section id="limit-case-l_infty-distance" data-number="0.2.3.1">
<h4 data-number="1.2.3.1"><span class="header-section-number">1.2.3.1</span> Limit Case: <span class="math inline">\(L_\infty\)</span> Distance</h4>
<p>As <span class="math inline">\(r\rightarrow\infty\)</span>, the largest coordinate difference dominates and the distance becomes</p>
<p><span class="math display">\[d_\infty(\mathbf{x}_j,\mathbf{x}_k)=
\max_i |x_{ji}-x_{ki}|.\]</span></p>
<p>This distance asks:</p>
<div class="center">
<p><em>“What is the worst coordinate difference?”</em></p>
</div>
<section id="running-example." data-number="0.2.3.1.1">
<h5 data-number="1.2.3.1.1"><span class="header-section-number">1.2.3.1.1</span> Running example.</h5>
<p>Pairwise <span class="math inline">\(L_\infty\)</span> distances for the four points:</p>
<table>
<caption><span class="math inline">\(L_\infty\)</span> distances on the running example.</caption>
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(r=\infty\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\mathbf{x}_1\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\mathbf{x}_2\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\mathbf{x}_3\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\mathbf{x}_4\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(\mathbf{x}_1\)</span></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">5</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(\mathbf{x}_2\)</span></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(\mathbf{x}_3\)</span></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(\mathbf{x}_4\)</span></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>Notice how the distance is determined by the largest coordinate difference. For example, <span class="math display">\[d_\infty(\mathbf{x}_1,\mathbf{x}_3)=\max(|0-3|,|2-1|)=3.\]</span></p>
</section>
</section>
<section id="limit-case-l_0-coordinate-difference-count" data-number="0.2.3.2">
<h4 data-number="1.2.3.2"><span class="header-section-number">1.2.3.2</span> Limit Case: <span class="math inline">\(L_0\)</span> Coordinate Difference Count</h4>
<p>When <span class="math inline">\(r=0\)</span>, the Minkowski expression becomes the number of coordinates that differ: <span class="math display">\[d_0(\mathbf{x}_j,\mathbf{x}_k)=
\#\{i : x_{ji}\neq x_{ki}\}.\]</span></p>
<p>This quantity is <em>not a true norm</em>, but it is useful in practice for measuring how many features change.</p>
<section id="running-example.-1" data-number="0.2.3.2.1">
<h5 data-number="1.2.3.2.1"><span class="header-section-number">1.2.3.2.1</span> Running example.</h5>
<p>Going back to this running example, we get:</p>
<table>
<caption><span class="math inline">\(L_0\)</span> coordinate difference counts on the running example.</caption>
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(r=0\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\mathbf{x}_1\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\mathbf{x}_2\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\mathbf{x}_3\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\mathbf{x}_4\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(\mathbf{x}_1\)</span></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(\mathbf{x}_2\)</span></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(\mathbf{x}_3\)</span></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(\mathbf{x}_4\)</span></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>For instance, <span class="math display">\[d_0(\mathbf{x}_3,\mathbf{x}_4)=1\]</span> because only the first coordinate differs.</p>
</section>
<section id="key-takeaway." data-number="0.2.3.2.2">
<h5 data-number="1.2.3.2.2"><span class="header-section-number">1.2.3.2.2</span> Key takeaway.</h5>
<p>Minkowski distance unifies multiple distance measures and shows that different metrics correspond to different ways of defining “closeness’’. Choosing a distance metric therefore directly affects clustering results.</p>
</section>
</section>
</section>
<section id="cosine-similarity" data-number="0.2.4">
<h3 data-number="1.2.4"><span class="header-section-number">1.2.4</span> Cosine Similarity</h3>
<p><strong>Cosine similarity</strong> measures the angle between two vectors in feature space: <span class="math display">\[s(\mathbf{x}_j,\mathbf{x}_k)=\cos\theta
=\frac{\mathbf{x}_j^T\mathbf{x}_k}{\|\mathbf{x}_j\|\|\mathbf{x}_k\|}
=\frac{\sum_{i=1}^{P}x_{ji}x_{ki}}
{\sqrt{\sum_{i=1}^{P}x_{ji}^2}\sqrt{\sum_{i=1}^{P}x_{ki}^2}}.\]</span></p>
<p>Rather than measuring distance in space, cosine similarity measures <em>directional similarity</em>. This makes it especially useful for high-dimensional sparse data such as text documents.</p>
<p>The value of <span class="math inline">\(\cos\theta\)</span> lies in <span class="math inline">\([-1,1]\)</span>:</p>
<ul>
<li><p><span class="math inline">\(1\)</span> → vectors point in the same direction (maximum similarity)</p></li>
<li><p><span class="math inline">\(0\)</span> → vectors are orthogonal (uncorrelated)</p></li>
<li><p><span class="math inline">\(-1\)</span> → vectors point in opposite directions</p></li>
</ul>
<p>A corresponding distance is often defined as <span class="math display">\[d = 1 - \cos\theta,\]</span> which lies in <span class="math inline">\([0,2]\)</span>.</p>
<figure>
<img src="img/lecture10/cosinesimilarity.png" alt="Special cases of cosine similarity." /><figcaption aria-hidden="true">Special cases of cosine similarity.</figcaption>
</figure>
<section id="example.-1" data-number="0.2.4.0.1">
<h5 data-number="1.2.4.0.1"><span class="header-section-number">1.2.4.0.1</span> Example.</h5>
<p>Cosine similarity is computationally efficient for sparse vectors because only non-zero entries contribute to the dot product.</p>
<p><span class="math display">\[\mathbf{x}_1=[3\ 2\ 0\ 5\ 0\ 0\ 0\ 2\ 0\ 0]^T,\quad
\mathbf{x}_2=[1\ 0\ 0\ 0\ 0\ 0\ 0\ 1\ 0\ 2]^T\]</span></p>
<p><span class="math display">\[s(\mathbf{x}_1,\mathbf{x}_2)=
\frac{3\cdot1+2\cdot1}{\sqrt{3^2+2^2+5^2+2^2}\sqrt{1^2+1^2+2^2}}
\approx 0.315\]</span></p>
<p><span class="math display">\[d=1-s\approx 0.685\]</span></p>
</section>
</section>
</section>
<section id="covariance-matrix-and-uncertainty" data-number="0.3">
<h2 data-number="1.3"><span class="header-section-number">1.3</span> Covariance Matrix and Uncertainty</h2>
<p>The covariance matrix is a key tool for understanding the relationships between multiple variables in a dataset. In clustering and distance-based learning, it provides the mathematical foundation for understanding the <em>shape</em> and <em>orientation</em> of data in feature space.</p>
<p>Suppose we are given a dataset <span class="math inline">\(\mathbf{X}\)</span> with <span class="math inline">\(N\)</span> samples and <span class="math inline">\(P\)</span> features. The covariance matrix <span class="math inline">\(\Sigma\)</span> captures how pairs of features vary together. The <span class="math inline">\((i,j)\)</span> entry of the covariance matrix is the covariance between the <span class="math inline">\(i^{th}\)</span> and <span class="math inline">\(j^{th}\)</span> features: <span class="math display">\[\sigma(x_i, x_j) = \frac{1}{N-1} \sum_{k=1}^{N} (x_{ki} - \bar{x}_i)(x_{kj} - \bar{x}_j).\]</span> Here, <span class="math inline">\(\bar{x}_i\)</span> and <span class="math inline">\(\bar{x}_j\)</span> are the means of features <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>. The diagonal entries <span class="math inline">\(\sigma(x_i,x_i)\)</span> represent the <strong>variance</strong> of individual features, while the off-diagonal entries represent how features vary together. A positive covariance indicates that two features tend to increase together, a negative covariance indicates that one feature tends to increase as the other decreases, and a covariance of zero indicates that the features are uncorrelated.</p>
<section id="correlation-normalized-covariance." data-number="0.3.0.0.1">
<h5 data-number="1.3.0.0.1"><span class="header-section-number">1.3.0.0.1</span> Correlation (Normalized Covariance).</h5>
<p>Because covariance depends on the scale of the features, it is often useful to normalize it to obtain the <strong>correlation coefficient</strong>: <span class="math display">\[\rho(x_i,x_j)=\frac{\sigma(x_i,x_j)}
{\sqrt{\sigma(x_i,x_i)\sigma(x_j,x_j)}}.\]</span> This quantity lies in the interval <span class="math inline">\([-1,1]\)</span> and provides a scale-independent and easily interpretable measure of the strength and direction of the relationship between variables.</p>
</section>
<h3 class="unnumbered" id="covariance-and-the-geometry-of-data">Covariance and the Geometry of Data</h3>
<p>One of the most important roles of the covariance matrix is that it describes the <strong>geometry of the data distribution</strong>. When the covariance matrix equals the identity matrix (<span class="math inline">\(\Sigma = I\)</span>), the data forms a spherical distribution, meaning all features have equal variance and are uncorrelated. When the covariance matrix contains non-zero off-diagonal entries, the distribution becomes elliptical (or ellipsoidal in higher dimensions), indicating that the features are correlated and that the data is stretched along certain directions.</p>
<p>This geometric interpretation will directly explain why Mahalanobis distance works, which we introduce in the next section. The orientation of the resulting ellipse is determined by the <strong>eigenvectors</strong> of <span class="math inline">\(\Sigma\)</span>, while the spread along each direction is determined by the corresponding <strong>eigenvalues</strong>. Eigenvectors define the principal axes of the data distribution, while eigenvalues quantify the variance along those axes, and eigenvalues quantify how much variation occurs along those directions. Large eigenvalues indicate directions of high variability, while small eigenvalues correspond to directions where the data is tightly concentrated.</p>
<p>These ideas form the foundation of several important machine learning methods, including Principal Component Analysis (PCA), Gaussian Mixture Models, and anomaly detection methods.</p>
<h3 class="unnumbered" id="covariance-as-a-measure-of-uncertainty">Covariance as a Measure of Uncertainty</h3>
<p>Covariance matrices also provide a natural way to quantify <strong>uncertainty</strong> in multivariate settings. For a single variable, uncertainty is typically visualized using a confidence interval. For multiple variables, this concept extends to <strong>confidence ellipses</strong> (or ellipsoids in higher dimensions). These ellipses are centered at the mean of the data, oriented according to the eigenvectors of the covariance matrix, and scaled according to the square roots of the eigenvalues.</p>
<p>For example, a 95% confidence ellipse represents the region in which we expect approximately 95% of the data points to lie. Large eigenvalues correspond to greater uncertainty and therefore produce longer ellipse axes, while smaller eigenvalues indicate greater certainty and produce shorter axes.</p>
<p>This geometric view of uncertainty is fundamental in probabilistic machine learning. Many models assume that data follows a multivariate Gaussian distribution, where the covariance matrix directly determines the model’s confidence in its predictions. Understanding covariance therefore explains why Mahalanobis distance works, how Gaussian clustering produces elliptical clusters, and how uncertainty can be represented in high-dimensional data.</p>
<p>These geometric and probabilistic interpretations of covariance lead naturally to a distance measure that respects the shape of the data distribution. This idea motivates the Mahalanobis distance introduced next.</p>
</section>
<section id="mahalanobis-distance" data-number="0.4">
<h2 data-number="1.4"><span class="header-section-number">1.4</span> Mahalanobis Distance</h2>
<p>Building on the covariance matrix introduced in the previous section, the <strong>Mahalanobis distance</strong> incorporates feature correlations and data scale: <span class="math display">\[d_M(\mathbf{x}_j,\mathbf{x}_k)
= \sqrt{(\mathbf{x}_j-\mathbf{x}_k)^T\Sigma^{-1}(\mathbf{x}_j-\mathbf{x}_k)},\]</span> where <span class="math inline">\(\Sigma\)</span> is the covariance matrix of the dataset.</p>
<p>Multiplication by <span class="math inline">\(\Sigma^{-1/2}\)</span> whitens the data, transforming the distribution into an isotropic Gaussian where Euclidean distance applies.</p>
<p>Unlike Euclidean distance, this metric accounts for the <em>shape of the data distribution</em>. It rescales directions with high variance and compresses directions with low variance.</p>
<p>If <span class="math inline">\(\Sigma = I\)</span>, Mahalanobis distance reduces to Euclidean distance.</p>
<p>This distance is especially useful for:</p>
<ul>
<li><p>Detecting outliers</p></li>
<li><p>Modeling elliptical or correlated clusters</p></li>
</ul>
<figure>
<img src="img/lecture10/mahalanobisdistance.png" alt="Mahalanobis distance adapts to the covariance structure of the data." /><figcaption aria-hidden="true">Mahalanobis distance adapts to the covariance structure of the data.</figcaption>
</figure>
<p>Intuitively, Mahalanobis distance measures how many <em>multivariate standard deviations</em> apart two samples are.</p>
<h3 class="unnumbered" id="running-demonstration-same-euclidean-distance-different-mahalanobis-distance">Running Demonstration: Same Euclidean Distance, Different Mahalanobis Distance</h3>
<p>To understand why Mahalanobis distance matters, we fix two points: <span class="math display">\[\mathbf{x}_a=(0,0), \qquad \mathbf{x}_b=(2,0)\]</span></p>
<p>Their Euclidean distance is constant: <span class="math display">\[d_E(\mathbf{x}_a,\mathbf{x}_b)=2.\]</span></p>
<p>To visualize each covariance structure, we generate synthetic samples from <span class="math inline">\(\mathcal{N}(\mathbf{0},\Sigma_i)\)</span> and highlight the two fixed points <span class="math inline">\(\mathbf{x}_a=(0,0)\)</span> and <span class="math inline">\(\mathbf{x}_b=(2,0)\)</span>.</p>
<p>We now construct five Gaussian distributions with different covariance matrices and compute the Mahalanobis distance between the <em>same</em> two points.</p>
<p><strong>Case 1 — Isotropic Gaussian (Baseline)</strong></p>
<p><span class="math display">\[\Sigma_1=
\begin{bmatrix}
1 &amp; 0\\
0 &amp; 1
\end{bmatrix},
\qquad
\Sigma_1^{-1}=I\]</span></p>
<p><span class="math display">\[d_M^2=(2,0)I\begin{bmatrix}2\\0\end{bmatrix}=4
\quad\Rightarrow\quad
d_M=2.\]</span></p>
<p><em>Mahalanobis equals Euclidean when the data is spherical.</em></p>
<figure>
<img src="img/lecture10/case1_isotropic.png" style="width:48.0%" alt="Samples from \mathcal{N}(\mathbf{0},\Sigma_1). The highlighted markers denote the fixed points \mathbf{x}_a=(0,0) and \mathbf{x}_b=(2,0). " /><figcaption aria-hidden="true">Samples from <span class="math inline">\(\mathcal{N}(\mathbf{0},\Sigma_1)\)</span>. The highlighted markers denote the fixed points <span class="math inline">\(\mathbf{x}_a=(0,0)\)</span> and <span class="math inline">\(\mathbf{x}_b=(2,0)\)</span>. </figcaption>
</figure>
<p><strong>Case 2 — Large Variance in <span class="math inline">\(x\)</span>-direction</strong></p>
<p><span class="math display">\[\Sigma_2=
\begin{bmatrix}
4 &amp; 0\\
0 &amp; 1
\end{bmatrix},
\qquad
\Sigma_2^{-1}=
\begin{bmatrix}
1/4 &amp; 0\\
0 &amp; 1
\end{bmatrix}\]</span></p>
<p><span class="math display">\[d_M^2=1
\quad\Rightarrow\quad
d_M=1.\]</span></p>
<p><em>Movement along high-variance directions is less surprising, so distance shrinks.</em></p>
<p>Geometrically, the data ellipse is stretched along the <span class="math inline">\(x\)</span>-axis. Displacement along this direction lies inside a wider ellipse, so it corresponds to fewer standard deviations.</p>
<figure>
<img src="img/lecture10/case2_large_var_x.png" style="width:48.0%" alt="Samples from \mathcal{N}(\mathbf{0},\Sigma_2). High variance along the x-direction stretches the data horizontally." /><figcaption aria-hidden="true">Samples from <span class="math inline">\(\mathcal{N}(\mathbf{0},\Sigma_2)\)</span>. High variance along the <span class="math inline">\(x\)</span>-direction stretches the data horizontally.</figcaption>
</figure>
<p><strong>Case 3 — Small Variance in <span class="math inline">\(x\)</span>-direction</strong></p>
<p><span class="math display">\[\Sigma_3=
\begin{bmatrix}
0.25 &amp; 0\\
0 &amp; 1
\end{bmatrix},
\qquad
\Sigma_3^{-1}=
\begin{bmatrix}
4 &amp; 0\\
0 &amp; 1
\end{bmatrix}\]</span></p>
<p><span class="math display">\[d_M^2=16
\quad\Rightarrow\quad
d_M=4.\]</span></p>
<p><em>Movement along low-variance directions is very surprising, so distance grows.</em></p>
<figure>
<img src="img/lecture10/case3_small_var_x.png" style="width:48.0%" alt="Samples from \mathcal{N}(\mathbf{0},\Sigma_3). Low variance along the x-direction compresses the data horizontally." /><figcaption aria-hidden="true">Samples from <span class="math inline">\(\mathcal{N}(\mathbf{0},\Sigma_3)\)</span>. Low variance along the <span class="math inline">\(x\)</span>-direction compresses the data horizontally.</figcaption>
</figure>
<p>Here the ellipse is compressed along the <span class="math inline">\(x\)</span>-axis. A horizontal displacement quickly exits the high-density region, so it corresponds to many standard deviations.</p>
<p><strong>Case 4 — Correlated Features</strong></p>
<p><span class="math display">\[\Sigma_4=
\begin{bmatrix}
1 &amp; 0.8\\
0.8 &amp; 1
\end{bmatrix},
\qquad
\Sigma_4^{-1}=
\begin{bmatrix}
2.78 &amp; -2.22\\
-2.22 &amp; 2.78
\end{bmatrix}\]</span></p>
<p><span class="math display">\[d_M \approx 3.33.\]</span></p>
<p><em>Correlation rotates the principal axes of the data ellipse.</em></p>
<figure>
<img src="img/lecture10/case4_correlated.png" style="width:48.0%" alt="Samples from \mathcal{N}(\mathbf{0},\Sigma_4). Correlation rotates the principal axes of the distribution." /><figcaption aria-hidden="true">Samples from <span class="math inline">\(\mathcal{N}(\mathbf{0},\Sigma_4)\)</span>. Correlation rotates the principal axes of the distribution.</figcaption>
</figure>
<p>The displacement vector <span class="math inline">\((2,0)\)</span> is no longer aligned with a principal direction of variance, so the effective distance increases.</p>
<p><strong>Case 5 — Rotated Elliptical Distribution</strong></p>
<p><span class="math display">\[\Sigma_5=
\begin{bmatrix}
2.125 &amp; 1.875\\
1.875 &amp; 2.125
\end{bmatrix},
\qquad
\Sigma_5^{-1}=
\begin{bmatrix}
1.062 &amp; -0.937\\
-0.937 &amp; 1.062
\end{bmatrix}\]</span></p>
<p><span class="math display">\[d_M \approx 2.06.\]</span></p>
<p><em>Distance depends on alignment with principal axes rather than the coordinate axes.</em></p>
<figure>
<img src="img/lecture10/case5_rotated.png" style="width:48.0%" alt="Samples from \mathcal{N}(\mathbf{0},\Sigma_5). Distance depends on alignment with the principal axes." /><figcaption aria-hidden="true">Samples from <span class="math inline">\(\mathcal{N}(\mathbf{0},\Sigma_5)\)</span>. Distance depends on alignment with the principal axes.</figcaption>
</figure>
<p>Mahalanobis distance depends on alignment with eigenvectors of <span class="math inline">\(\Sigma\)</span>, not with the coordinate axes.</p>
<p>These figures visually illustrate how the same Euclidean displacement can correspond to very different standardized distances under different covariance structures.</p>
<section id="important-interpretation." data-number="0.4.0.0.1">
<h5 data-number="1.4.0.0.1"><span class="header-section-number">1.4.0.0.1</span> Important Interpretation.</h5>
<p>Level sets of Mahalanobis distance satisfy <span class="math display">\[(\mathbf{x}-\mu)^T\Sigma^{-1}(\mathbf{x}-\mu)=c,\]</span> which defines ellipses aligned with the eigenvectors of <span class="math inline">\(\Sigma\)</span>. Euclidean distance instead produces circular level sets.</p>
<p>Mahalanobis distance is therefore the natural distance measure for Gaussian-distributed data.</p>
</section>
<h3 class="unnumbered" id="key-takeaway">Key Takeaway</h3>
<div class="center">
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Distribution</th>
<th style="text-align: center;">Euclidean Distance</th>
<th style="text-align: center;">Mahalanobis Distance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Isotropic</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="even">
<td style="text-align: center;">Large variance in <span class="math inline">\(x\)</span></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Small variance in <span class="math inline">\(x\)</span></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4</td>
</tr>
<tr class="even">
<td style="text-align: center;">Correlated features</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3.33</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Rotated ellipse</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2.06</td>
</tr>
</tbody>
</table>
</div>
<p>Even when Euclidean distance is identical, Mahalanobis distance changes because it measures distance <em>relative to the data distribution</em>. It answers the question:</p>
<p><span class="math display">\[\text{``How unusual is this displacement for this dataset?&#39;&#39;}\]</span></p>
</section>
<section id="qa-section" data-number="0.5">
<h2 data-number="1.5"><span class="header-section-number">1.5</span> Q&amp;A Section</h2>
<p>The following table shows the regression outputs from three different models on five samples, along with the ground-truth (GT) values. We will use this table to build intuition about bias, variance, and underfitting.</p>
<table>
<caption>Regression outputs of three models along with ground truth values.</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Sample</th>
<th style="text-align: center;">GT</th>
<th style="text-align: center;">Model 1</th>
<th style="text-align: center;">Model 2</th>
<th style="text-align: center;">Model 3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">5.2</td>
<td style="text-align: center;">5.0</td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">6.0</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">7.2</td>
</tr>
<tr class="even">
<td style="text-align: center;">4</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">5.7</td>
<td style="text-align: center;">8.0</td>
</tr>
<tr class="odd">
<td style="text-align: center;">5</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">5.2</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">9.3</td>
</tr>
</tbody>
</table>
<p><strong>Important note.</strong> In the formal bias–variance decomposition, bias and variance are defined with respect to many possible training datasets. Since we only have one dataset here, we will use a <strong>simplified empirical proxy</strong>: we treat the model predictions across samples as if they were repeated predictions. This lets us build intuition about bias and variance without requiring multiple datasets.</p>
<ol>
<li><p><strong>Question:</strong> What is the average <strong>empirical bias proxy</strong> for each model across the five samples? <strong>Options:</strong></p>
<ol>
<li><p>Model 1: 2.92, Model 2: 0.44, Model 3: 0.08</p></li>
<li><p>Model 1: 6.33, Model 2: 4.19, Model 3: 2.01</p></li>
<li><p>Model 1: 2.92, Model 2: 1.44, Model 3: 0.08</p></li>
<li><p>Model 1: 1.92, Model 2: 4.19, Model 3: 2.01</p></li>
</ol>
<p><strong>Solution:</strong> We approximate bias using the squared difference between the model’s mean prediction and the ground truth values for each sample.</p>
<p><strong>Model 1</strong>: <span class="math display">\[\text{Mean of Model 1&#39;s predictions} = \frac{4.8 + 4.7 + 4.9 + 5.0 + 5.2}{5} = 4.92\]</span> <span class="math display">\[\text{Avg Bias for Model 1} = \frac{(4.92 - 5)^2 + (4.92 - 6)^2 + (4.92 - 7)^2 + (4.92 - 8)^2 + (4.92 - 9)^2}{5} = 6.33\]</span></p>
<p><strong>Model 2</strong>: <span class="math display">\[\text{Mean of Model 2&#39;s predictions} = \frac{5.2 + 5.3 + 5.5 + 5.7 + 5.9}{5} = 5.52\]</span> <span class="math display">\[\text{Avg Bias for Model 2} = \frac{(5.52 - 5)^2 + (5.52 - 6)^2 + (5.52 - 7)^2 + (5.52 - 8)^2 + (5.52 - 9)^2}{5} = 4.19\]</span></p>
<p><strong>Model 3</strong>: <span class="math display">\[\text{Mean of Model 3&#39;s predictions} = \frac{5.0 + 6.0 + 7.2 + 8.0 + 9.3}{5} = 7.1\]</span> <span class="math display">\[\text{Avg Bias for Model 3} = \frac{(7.1 - 5)^2 + (7.1 - 6)^2 + (7.1 - 7)^2 + (7.1 - 8)^2 + (7.1 - 9)^2}{5} = 2.01\]</span> The correct answer is <strong>(b) Model 1: 6.33, Model 2: 4.19, Model 3: 2.01</strong>.</p></li>
<li><p><strong>Question:</strong> What is the average <strong>empirical variance proxy</strong> for each model? <strong>Options:</strong></p>
<ol>
<li><p>Model 1: 0.03, Model 2: 0.08, Model 3: 0.13</p></li>
<li><p>Model 1: 0.05, Model 2: 0.12, Model 3: 0.15</p></li>
<li><p>Model 1: 0.04, Model 2: 0.08, Model 3: 0.12</p></li>
<li><p>Model 1: 0.03, Model 2: 0.07, Model 3: 2.26</p></li>
</ol>
<p><strong>Solution:</strong> We approximate variance as the variance of the model’s predictions.</p>
<p>Variance is calculated as the average of the squared differences between each model’s predictions and its mean prediction.</p>
<p><strong>Model 1</strong>: <span class="math display">\[\text{Variance for Model 1} = \frac{(4.8 - 4.92)^2 + (4.7 - 4.92)^2 + (4.9 - 4.92)^2 + (5.0 - 4.92)^2 + (5.2 - 4.92)^2}{5} = 0.03\]</span></p>
<p><strong>Model 2</strong>: <span class="math display">\[\text{Variance for Model 2} = \frac{(5.2 - 5.52)^2 + (5.3 - 5.52)^2 + (5.5 - 5.52)^2 + (5.7 - 5.52)^2 + (5.9 - 5.52)^2}{5} = 0.07\]</span></p>
<p><strong>Model 3</strong>: <span class="math display">\[\text{Variance for Model 3} = \frac{(5.0 - 7.1)^2 + (6.0 - 7.1)^2 + (7.2 - 7.1)^2 + (8.0 - 7.1)^2 + (9.3 - 7.1)^2}{5} = 2.26\]</span></p>
<p>The correct answer is <strong>(d) Model 1: 0.03, Model 2: 0.07, Model 3: 2.26</strong>.</p></li>
<li><p><strong>Question:</strong> If the differences in bias/variance values are significant enough, would we expect model 1 is more likely to have overfit or underfit? <strong>Options:</strong></p>
<ol>
<li><p>Underfit</p></li>
<li><p>Overfit</p></li>
</ol>
<p><strong>Solution:</strong> Model 1 exhibits <strong>underfitting</strong> because it has high bias and low variance, meaning it is too simple and fails to capture the complexity of the data. Hence, the correct answer is <strong>(a) Underfit</strong>.</p></li>
</ol>
</section>
</body>
</html>

</main>
</body>
</html>
