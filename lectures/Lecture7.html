<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>add-on</title>
  <link rel="stylesheet" href="../assets/style.css"/>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav>
  <a href="../index.html">Home</a>
</nav>
<main>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>_Lecture7</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div class="center">

</div>
<p><span><strong>Disclaimer</strong></span>: <span><em>These notes have not been subjected to the usual scrutiny reserved for formal publications.</em></span></p>
<section id="lecture-objectives" data-number="0.1">
<h2 data-number="1.1"><span class="header-section-number">1.1</span> Lecture Objectives</h2>
<p>This lecture covers the fundamentals of regression, specifically modeling the relationship between the input variable <span class="math inline">\(x\)</span> and the continuous output variable <span class="math inline">\(y\)</span> by fitting a function <span class="math inline">\(f(x)\)</span> to the observed data. We focus primarily on linear regression, exploring performance evaluation and model fitting techniques.</p>
</section>
<section id="performance-evaluation" data-number="0.2">
<h2 data-number="1.2"><span class="header-section-number">1.2</span> Performance Evaluation</h2>
<p>Key performance metrics discussed in the previous lecture include:</p>
<ul>
<li><p><strong>Precision</strong>: <span class="math display">\[P = \frac{TP}{TP + FP}\]</span> Precision indicates the portion of items labeled as positive that are actually positive. It is crucial when the cost of false positives is high.</p></li>
<li><p><strong>Recall</strong>: <span class="math display">\[R = \frac{TP}{TP + FN}\]</span> Recall measures the portion of actual positives that have been labeled correctly. It is preferred when the cost of false negatives is high (e.g., in medical diagnoses).</p></li>
<li><p><strong>F1 Score</strong>: <span class="math display">\[F_1 = 2 \times \frac{P \times R}{P + R}\]</span> The F1 score combines precision and recall into a single metric, suitable when the class distribution is imbalanced.</p></li>
</ul>
</section>
<section id="performance-evaluation-in-action" data-number="0.3">
<h2 data-number="1.3"><span class="header-section-number">1.3</span> Performance Evaluation in Action</h2>
<section id="precision-recall-trade-off" data-number="0.3.1">
<h3 data-number="1.3.1"><span class="header-section-number">1.3.1</span> Precision-Recall Trade-off</h3>
<p>For any given data set, an inverse relationship exists between precision and recall at varying thresholds. A stricter threshold results in higher precision and lower recall (minimizing false positives while increasing false negatives), whereas a softer threshold leads to higher recall but lower precision.</p>
</section>
<section id="roc-curves" data-number="0.3.2">
<h3 data-number="1.3.2"><span class="header-section-number">1.3.2</span> ROC Curves</h3>
<p>ROC (Receiver Operating Characteristic) curves plot the true positive rate (TPR) against the false positive rate (FPR) for varying classifier thresholds. Important metrics: <span class="math display">\[\text{TPR (Recall)} = \frac{TP}{TP + FN}\]</span> <span class="math display">\[\text{FPR} = \frac{FP}{FP + TN} = 1 - \text{Specificity}\]</span> The area under the curve (AUC) provides a single value to evaluate the classifierâ€™s performance, with values closer to 1 indicating a better classifier.</p>
</section>
<section id="evaluation-under-imbalanced-data" data-number="0.3.3">
<h3 data-number="1.3.3"><span class="header-section-number">1.3.3</span> Evaluation under Imbalanced Data</h3>
<p>ROC curves may not be effective for imbalanced data. In such cases, the precision-recall curve offers a better evaluation tool.</p>
</section>
</section>
<section id="linear-regression" data-number="0.4">
<h2 data-number="1.4"><span class="header-section-number">1.4</span> Linear Regression</h2>
<section id="introduction" data-number="0.4.1">
<h3 data-number="1.4.1"><span class="header-section-number">1.4.1</span> Introduction</h3>
<p>Regression models the relationship between an input variable <span class="math inline">\(x\)</span> and a continuous output variable <span class="math inline">\(y\)</span> by fitting a function to observed data. Unlike classification, the output is continuous. This lecture focuses on linear regression models, though polynomial regressions are also used for more complex relationships.</p>
</section>
<section id="example-better-life-index-oecd" data-number="0.4.2">
<h3 data-number="1.4.2"><span class="header-section-number">1.4.2</span> Example: Better Life Index (OECD)</h3>
<p>A simple linear model can describe the relationship between GDP per capita and life satisfaction: <span class="math display">\[\text{Life Satisfaction} = \theta_0 + \theta_1 \times \text{GDP per Capita}\]</span> Where <span class="math inline">\(\theta_0\)</span> is the intercept and <span class="math inline">\(\theta_1\)</span> the slope.</p>
</section>
<section id="general-linear-regression-model" data-number="0.4.3">
<h3 data-number="1.4.3"><span class="header-section-number">1.4.3</span> General Linear Regression Model</h3>
<p>Given a dataset <span class="math inline">\((\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \ldots, (\mathbf{x}_N, y_N)\)</span>, where <span class="math inline">\(\mathbf{x}_i\)</span> is a feature vector, the linear relationship is modeled as: <span class="math display">\[y_i = \theta_0 + \mathbf{\theta}^T \mathbf{x}_i\]</span> In matrix form: <span class="math display">\[\mathbf{y} = \mathbf{X} \boldsymbol{\theta}\]</span> where <span class="math inline">\(\mathbf{X}\)</span> is the matrix of input samples, and <span class="math inline">\(\boldsymbol{\theta}\)</span> is the vector of weights.</p>
</section>
</section>
<section id="least-squares-loss-function" data-number="0.5">
<h2 data-number="1.5"><span class="header-section-number">1.5</span> Least Squares Loss Function</h2>
<p>The least squares loss function minimizes the residuals (errors) between the predicted and observed values: <span class="math display">\[L(\boldsymbol{\theta}) = \frac{1}{N} \sum_{i=1}^{N} (\mathbf{x}_i^T \boldsymbol{\theta} - y_i)^2\]</span> This function is minimized to find the optimal parameters <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span>.</p>
</section>
<section id="common-notations" data-number="0.6">
<h2 data-number="1.6"><span class="header-section-number">1.6</span> Common Notations</h2>
<div class="multicols">
<p><span>2</span></p>
<ul>
<li><p><span class="math inline">\(\mathbf{X}\)</span>: Matrix of input features</p></li>
<li><p><span class="math inline">\(\mathbf{y}\)</span>: Vector of outputs</p></li>
<li><p><span class="math inline">\(\boldsymbol{\theta}\)</span>: Vector of coefficients</p></li>
<li><p><span class="math inline">\(\mathbf{x}_i\)</span>: Feature vector of the <span class="math inline">\(i\)</span>-th sample</p></li>
<li><p><span class="math inline">\(\epsilon\)</span>: Residual error</p></li>
</ul>
</div>
</section>
<section id="optimal-parameters-via-normal-equation" data-number="0.7">
<h2 data-number="1.7"><span class="header-section-number">1.7</span> Optimal Parameters via Normal Equation</h2>
<p>The optimal parameters for linear regression can be directly computed using the normal equation. Given the least squares loss function, we want to minimize it with respect to the coefficients <span class="math inline">\(\boldsymbol{\theta}\)</span>. This results in the following expression:</p>
<p><span class="math display">\[\frac{\partial L(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} = 2\mathbf{X}^T (\mathbf{X} \boldsymbol{\theta} - \mathbf{y}) = 0\]</span> Let <span class="math inline">\(L(\hat{\boldsymbol{\theta}})\)</span> be the loss function that measures the difference between the predicted values <span class="math inline">\(\hat{\mathbf{y}}\)</span> and the actual target values <span class="math inline">\(\mathbf{y}\)</span>.</p>
<p>We want to find the optimal <span class="math inline">\(\hat{\boldsymbol{\theta}}^*\)</span> such that:</p>
<p><span class="math display">\[\hat{\boldsymbol{\theta}}^* = \arg \min_{\hat{\boldsymbol{\theta}}} L(\hat{\boldsymbol{\theta}})\]</span></p>
<p>To find this optimal <span class="math inline">\(\hat{\boldsymbol{\theta}}^*\)</span>, we take the derivative of the loss function with respect to <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> and set it equal to zero. That is, the loss function reaches its minimum when:</p>
<p><span class="math display">\[\frac{dL(\hat{\boldsymbol{\theta}})}{d\hat{\boldsymbol{\theta}}} = 0\]</span></p>
<p>Since the Least Squares loss function for linear regression is convex and quadratic, the solution for the optimal <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> can be found by solving the **Normal Equation**:</p>
<p>Solving for <span class="math inline">\(\boldsymbol{\theta}\)</span>, we get the normal equation:</p>
<p><span class="math display">\[\boldsymbol{\hat{\theta}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}\]</span></p>
<p>The matrix <span class="math inline">\((\mathbf{X}^T \mathbf{X})\)</span> must be invertible for the normal equation to be valid. This method gives us the closed-form solution for the optimal weights in linear regression.</p>
</section>
<section id="geometric-interpretation-of-linear-regression" data-number="0.8">
<h2 data-number="1.8"><span class="header-section-number">1.8</span> Geometric Interpretation of Linear Regression</h2>
<p>Linear regression can also be understood from a geometric perspective. Consider the data matrix <span class="math inline">\(\mathbf{X}\)</span> with <span class="math inline">\(N\)</span> samples and <span class="math inline">\(P\)</span> features. The columns of <span class="math inline">\(\mathbf{X}\)</span> define a <span class="math inline">\((P+1)\)</span>-dimensional linear subspace (including the bias term). The goal is to project the target vector <span class="math inline">\(\mathbf{y}\)</span> onto this subspace.</p>
<p>The estimated target vector <span class="math inline">\(\mathbf{\hat{y}}\)</span> lies in this subspace and is as close as possible to <span class="math inline">\(\mathbf{y}\)</span> in terms of the squared distance:</p>
<p><span class="math display">\[\hat{\mathbf{y}} = \mathbf{X} \hat{\boldsymbol{\theta}} = \mathbf{X} (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}\]</span></p>
<p>The estimated target <span class="math inline">\(\hat{\mathbf{y}}\)</span> is the orthogonal projection of <span class="math inline">\(\mathbf{y}\)</span> onto the subspace spanned by the columns of <span class="math inline">\(\mathbf{X}\)</span>.</p>
</section>
<section id="singular-value-decomposition-svd-for-linear-regression" data-number="0.9">
<h2 data-number="1.9"><span class="header-section-number">1.9</span> Singular Value Decomposition (SVD) for Linear Regression</h2>
<p>When the matrix <span class="math inline">\(\mathbf{X}^T \mathbf{X}\)</span> is not invertible (i.e., the columns of <span class="math inline">\(\mathbf{X}\)</span> are linearly dependent), the normal equation cannot be applied directly. In such cases, we can use Singular Value Decomposition (SVD) to address this issue.</p>
<p>The SVD of the matrix <span class="math inline">\(\mathbf{X}\)</span> is given by:</p>
<p><span class="math display">\[\mathbf{X} = \mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^T\]</span></p>
<p>where <span class="math inline">\(\mathbf{U}\)</span> and <span class="math inline">\(\mathbf{V}\)</span> are orthogonal matrices, and <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is a diagonal matrix of singular values. If any singular values in <span class="math inline">\(\boldsymbol{\Sigma}\)</span> are close to zero, it indicates redundancy in the features. By removing such singular values, we can compute the optimal <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> even when <span class="math inline">\(\mathbf{X}^T \mathbf{X}\)</span> is not invertible.</p>
</section>
<section id="multi-output-regression" data-number="0.10">
<h2 data-number="1.10"><span class="header-section-number">1.10</span> Multi-output Regression</h2>
<p>In some cases, we need to predict multiple outputs simultaneously. This is known as multi-output regression. If we assume a linear relationship between the input vector <span class="math inline">\(\mathbf{x}_i\)</span> and all dimensions of the output <span class="math inline">\(\mathbf{y}_i\)</span>, the model can be written as:</p>
<p><span class="math display">\[\mathbf{Y} = \mathbf{X} \boldsymbol{\Theta}\]</span></p>
<p>where <span class="math inline">\(\mathbf{Y}\)</span> is the matrix of outputs and <span class="math inline">\(\boldsymbol{\Theta}\)</span> is the matrix of coefficients. Each column of <span class="math inline">\(\mathbf{Y}\)</span> represents a different output, and the corresponding column of <span class="math inline">\(\boldsymbol{\Theta}\)</span> contains the coefficients for predicting that output.</p>
</section>
<section id="regularization-techniques" data-number="0.11">
<h2 data-number="1.11"><span class="header-section-number">1.11</span> Regularization Techniques</h2>
<p>To prevent overfitting and address issues with collinearity in the data, regularization techniques are often used in linear regression. The two most common regularization methods are:</p>
<section id="ridge-regression-l2-regularization" data-number="0.11.1">
<h3 data-number="1.11.1"><span class="header-section-number">1.11.1</span> Ridge Regression (L2 Regularization)</h3>
<p>Ridge regression adds a penalty term to the least squares loss function based on the L2 norm of the coefficients:</p>
<p><span class="math display">\[L(\boldsymbol{\theta}) = \frac{1}{N} \sum_{i=1}^{N} (\mathbf{x}_i^T \boldsymbol{\theta} - y_i)^2 + \lambda \|\boldsymbol{\theta}\|_2^2\]</span></p>
<p>The parameter <span class="math inline">\(\lambda\)</span> controls the amount of regularization. As <span class="math inline">\(\lambda\)</span> increases, the model becomes less sensitive to the specific features, preventing overfitting.</p>
</section>
<section id="lasso-regression-l1-regularization" data-number="0.11.2">
<h3 data-number="1.11.2"><span class="header-section-number">1.11.2</span> Lasso Regression (L1 Regularization)</h3>
<p>Lasso regression adds a penalty term based on the L1 norm of the coefficients:</p>
<p><span class="math display">\[L(\boldsymbol{\theta}) = \frac{1}{N} \sum_{i=1}^{N} (\mathbf{x}_i^T \boldsymbol{\theta} - y_i)^2 + \lambda \|\boldsymbol{\theta}\|_1\]</span></p>
<p>Lasso regression encourages sparsity in the coefficients, meaning it will set some of the coefficients to zero, effectively selecting a simpler model with fewer features.</p>
</section>
<section id="elastic-net" data-number="0.11.3">
<h3 data-number="1.11.3"><span class="header-section-number">1.11.3</span> Elastic Net</h3>
<p>Elastic Net combines both L1 and L2 regularization:</p>
<p><span class="math display">\[L(\boldsymbol{\theta}) = \frac{1}{N} \sum_{i=1}^{N} (\mathbf{x}_i^T \boldsymbol{\theta} - y_i)^2 + \lambda_1 \|\boldsymbol{\theta}\|_1 + \lambda_2 \|\boldsymbol{\theta}\|_2^2\]</span></p>
<p>This method leverages the benefits of both Ridge and Lasso regression.</p>
</section>
</section>
<section id="applications-of-multi-output-regression" data-number="0.12">
<h2 data-number="1.12"><span class="header-section-number">1.12</span> Applications of Multi-output Regression</h2>
<p>Multi-output regression is widely used in various applications, such as:</p>
<ul>
<li><p><strong>Forest properties prediction</strong>: Estimating forest vegetation height and canopy cover from satellite imagery.</p></li>
<li><p><strong>Soil quality prediction</strong>: Predicting soil quality measures like Acari abundance and biodiversity.</p></li>
<li><p><strong>Monthly product sales prediction</strong>: Estimating online sales based on product and advertising data.</p></li>
<li><p><strong>Energy consumption estimation</strong>: Predicting heating and cooling loads of residential buildings.</p></li>
</ul>
</section>
<section id="conclusion" data-number="0.13">
<h2 data-number="1.13"><span class="header-section-number">1.13</span> Conclusion</h2>
<p>In this lecture, we covered the fundamentals of linear regression, focusing on the performance evaluation, the geometric interpretation, and regularization techniques. We also discussed the applications of regression in real-world scenarios, as well as strategies for handling multi-output regression.</p>
</section>
<section id="common-notations-1" data-number="0.14">
<h2 data-number="1.14"><span class="header-section-number">1.14</span> Common Notations</h2>
<div class="multicols">
<p><span>2</span></p>
<ul>
<li><p><span class="math inline">\(\mathbf{X}\)</span>: Matrix of input features</p></li>
<li><p><span class="math inline">\(\mathbf{y}\)</span>: Vector of outputs</p></li>
<li><p><span class="math inline">\(\mathbf{x}_i\)</span>: Feature vector of the <span class="math inline">\(i\)</span>-th sample</p></li>
<li><p><span class="math inline">\(\boldsymbol{\theta}\)</span>: Coefficient vector</p></li>
<li><p><span class="math inline">\(\epsilon\)</span>: Residual error</p></li>
<li><p><span class="math inline">\(P\)</span>: Number of features</p></li>
<li><p><span class="math inline">\(N\)</span>: Number of samples</p></li>
<li><p><span class="math inline">\(\lambda\)</span>: Regularization parameter</p></li>
<li><p><span class="math inline">\(\hat{\mathbf{y}}\)</span>: Predicted output</p></li>
</ul>
</div>
</section>
</body>
</html>

</main>
</body>
</html>
