<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>Lecture13 In-class Exercise Solutions</title>
  <link rel="stylesheet" href="../assets/style.css"/>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav>
  <a href="../index.html">Home</a>
</nav>
<main>
<div class="center">
<p><span><strong>In-Class Exercise â€” Questions &amp; Solutions</strong></span><br />
<span><strong>Lecture 13: Neural Networks</strong></span></p>
</div>
<ol>
<li><p><strong>Forward pass with ReLU</strong></p>
<p>A neuron computes <span class="math inline">\(h=\bm{w}^T\bm{x}+b\)</span> and outputs <span class="math inline">\(\mathrm{ReLU}(h)=\max(0,h)\)</span>.</p>
<p><span class="math display">\[\bm{x}=\begin{bmatrix}1\\-2\end{bmatrix},\quad
\bm{w}=\begin{bmatrix}2\\1\end{bmatrix},\quad
b=-1\]</span></p>
<p>What is the output after ReLU?</p>
<ul>
<li><p><span class="math inline">\(0\)</span></p></li>
<li><p><span class="math inline">\(1\)</span></p></li>
<li><p><span class="math inline">\(3\)</span></p></li>
</ul>
<p><strong>Solution</strong></p>
<p><span class="math display">\[h=\bm{w}^T\bm{x}+b=(2)(1)+(1)(-2)-1=2-2-1=-1\]</span> <span class="math display">\[\mathrm{ReLU}(h)=\max(0,-1)=0\]</span></p>
<p><span class="math display">\[\boxed{(A)}\]</span></p></li>
<li><p><strong>Why add nonlinearities?</strong></p>
<p>What is the main reason we include nonlinear activation functions (e.g., ReLU, sigmoid) in neural networks?</p>
<ul>
<li><p>They make the optimization problem convex.</p></li>
<li><p>They allow the network to represent non-linear functions/decision boundaries.</p></li>
<li><p>They guarantee perfect generalization.</p></li>
</ul>
<p><strong>Solution</strong></p>
<p>Stacking only linear layers still produces a linear function. Nonlinear activations give neural networks the ability to model nonlinear relationships.</p>
<p><span class="math display">\[\boxed{(B)}\]</span></p></li>
<li><p><strong>Softmax sanity check</strong></p>
<p>Softmax maps scores <span class="math inline">\(\bm{s}=[s_1,s_2,s_3]\)</span> to probabilities <span class="math display">\[p_k=\frac{e^{s_k}}{\sum_{j=1}^3 e^{s_j}}.\]</span></p>
<p>Which statement is always true?</p>
<ul>
<li><p>Some <span class="math inline">\(p_k\)</span> can be negative.</p></li>
<li><p><span class="math inline">\(\sum_{k=1}^3 p_k = 1\)</span>.</p></li>
<li><p>The largest score always maps to probability exactly <span class="math inline">\(1\)</span>.</p></li>
</ul>
<p><strong>Solution</strong></p>
<p>Softmax outputs form a probability distribution: each <span class="math inline">\(p_k&gt;0\)</span> and they sum to 1.</p>
<p><span class="math display">\[\boxed{(B)}\]</span></p></li>
<li><p><strong>Cross-entropy intuition</strong></p>
<p>If the true class is class 2, the loss is <span class="math display">\[L=-\log(p_2).\]</span></p>
<p>Which prediction gives the smallest loss?</p>
<ul>
<li><p><span class="math inline">\(p_2 = 0.90\)</span></p></li>
<li><p><span class="math inline">\(p_2 = 0.40\)</span></p></li>
<li><p><span class="math inline">\(p_2 = 0.10\)</span></p></li>
</ul>
<p><strong>Solution</strong></p>
<p>The loss decreases as <span class="math inline">\(p_2\)</span> increases. Largest probability <span class="math inline">\(\Rightarrow\)</span> smallest loss.</p>
<p><span class="math display">\[\boxed{(A)}\]</span></p></li>
<li><p><strong>Counting parameters</strong></p>
<p>Fully-connected network: <span class="math display">\[\text{Input }(4)\rightarrow \text{Hidden }(3)\rightarrow \text{Output }(2)\]</span> with biases in each layer.</p>
<p>How many trainable parameters are there?</p>
<ul>
<li><p><span class="math inline">\(4\cdot3 + 3\cdot2\)</span></p></li>
<li><p><span class="math inline">\(4\cdot3 + 3 + 3\cdot2 + 2\)</span></p></li>
<li><p><span class="math inline">\(4 + 3 + 2\)</span></p></li>
</ul>
<p><strong>Solution</strong></p>
<p>Layer 1 (input <span class="math inline">\(\rightarrow\)</span> hidden): <span class="math display">\[\text{weights}=4\cdot3,\quad \text{biases}=3\]</span></p>
<p>Layer 2 (hidden <span class="math inline">\(\rightarrow\)</span> output): <span class="math display">\[\text{weights}=3\cdot2,\quad \text{biases}=2\]</span></p>
<p>Total parameters: <span class="math display">\[4\cdot3 + 3 + 3\cdot2 + 2\]</span></p>
<p><span class="math display">\[\boxed{(B)}\]</span></p></li>
</ol>

</main>
</body>
</html>
