<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>main</title>
  <link rel="stylesheet" href="../assets/style.css"/>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav>
  <a href="../index.html">Home</a>
</nav>
<main>
<div class="center">
</div>





<section data-number="0.1" id="lecture-objectives">
<h2 data-number="1.1"><span class="header-section-number">1.1</span> Lecture Objectives</h2>
<p>The objectives of this lecture are to provide a comprehensive understanding of anomaly detection as a critical machine learning techniques for identifying rare or unusual events within data patterns. This includes defining anomalies in the context of statistical frameworks, introducing the core components of anomaly detection algorithms-statistic and decision rule-and exploring various performance metrics such as True Positive Rate (TPR), False Positive Rate (FPR), and Area Under the Curve (AUC). Additionally, the lecture examines different anomaly detection settings, including semi-supervised, unsupervised, and supervised approaches, emphasizing their unique assumptions, methodologies, and applications. Advanced topics such as density-based methods, reconstruction-based techniques, and hybrid approaches integrating statistical modeling and deep learning are also discussed, equipping learners with the knowledge to address challenges in high-dimensional data and complex anomaly detection scenarios effectively.</p>
</section>
<section data-number="0.2" id="anomaly-detection">
<h2 data-number="1.2"><span class="header-section-number">1.2</span> Anomaly Detection</h2>
<p>The anomaly detection scheme was first introduced in this lecture. The following things were discussed to demonstrate and elaborate the concept.</p>
<ul>
<li><p>Anomaly Definition</p></li>
<li><p>Problem Setup</p></li>
<li><p>Performance Metrics</p></li>
<li><p>Anomaly Detection Settings</p></li>
<li><p>Statistical Methods</p></li>
<li><p>Reconstruction Methods</p></li>
<li><p>GradCON</p></li>
</ul>
<section data-number="0.2.1" id="definition">
<h3 data-number="1.2.1"><span class="header-section-number">1.2.1</span> Definition</h3>
<p>Anomaly detection is a machine learning technique aimed at identifying rare or unusual events within normal patterns of data. While anomaly detection is not necessarily an application in itself, it plays a critical role in various domains by flagging significant deviations. Specifically, anomalies are defined as patterns that deviate significantly from a well-established notion of normal behavior. The key challenge lies in accurately defining what determines “normal”. Historically, the approach to anomaly detection has evolved significantly, but for simplicity, we will adhere to basic assumptions in this class.</p>
<figure>
<img alt="Example of anomalous structures: Anomalies within a structured material" id="fig:enter-label" src="img/lecture24/image1.png"/><figcaption aria-hidden="true">Example of anomalous structures: Anomalies within a structured material</figcaption>
</figure>
<figure>
<img alt="Another Example of anomalous structures: Anomaly detection scenario in a public setting" id="fig:enter-label" src="img/lecture24/Picture2.png"/><figcaption aria-hidden="true">Another Example of anomalous structures: Anomaly detection scenario in a public setting</figcaption>
</figure>
<p>In data modeling, normal data are typically generated from a stationary process <span class="math inline">\(P_N\)</span>, where the statistical properties (e.g., mean, variance) remain constant over time. Anomalies, on the other hand, arise from distinct, non-stationary processes <span class="math inline">\(P_A\)</span>, where <span class="math inline">\(P_A\neq P_N\)</span>. This divergence from normality allows anomalies to stand out in datasets.</p>
</section>
<section data-number="0.2.2" id="example">
<h3 data-number="1.2.2"><span class="header-section-number">1.2.2</span> Example</h3>
<p>Examples of anomaly detection include:</p>
<ul>
<li><p>Identifying fraudulent transactions within streams of credit card activities</p></li>
<li><p>Detecting arrhythmias in ECG tracings that deviate from normal heart activity</p></li>
<li><p>Locating defective regions in images that do not conform to a reference pattern</p></li>
</ul>
<p>It is important to note that anomalies often manifest as spurious, seemingly irregular elements. Despite their rarity, these anomalous data points are typically the most informative and significant samples within a dataset, as they often highlight critical insights or events of interest.</p>
</section>
<section data-number="0.2.3" id="anomaly-detection-in-images">
<h3 data-number="1.2.3"><span class="header-section-number">1.2.3</span> Anomaly Detection in Images</h3>
<p>To address anomaly detection in images, the problem can be formulated mathematically as follows:</p>
<ul>
<li><p>Let <span class="math inline">\(s\)</span> represent an image defined over the pixel domain <span class="math inline">\(x\in Z^2\)</span></p></li>
<li><p>Let <span class="math inline">\(c \in x\)</span> denote a specific pixel, and <span class="math inline">\(s(c)\)</span> he corresponding intensity at pixel <span class="math inline">\(c\)</span></p></li>
</ul>
<p>The objective is to identify anomalous region in the image <span class="math inline">\(s\)</span> for each pixel <span class="math inline">\(c\)</span>, which can be expressed through the estimation of an anomaly mask <span class="math inline">\(\Omega\)</span> defined as:</p>
<p><span class="math display">\[\Omega(c)=\begin{cases}
    0, &amp; \text{if $c$ belongs to a normal region}.\\
    1, &amp; \text{if $c$ belongs to a normal region}.
  \end{cases}\]</span></p>
<p>If we observe a set of data over time, not necessarily in a stream, represented as:</p>
<div class="center">
<p><span class="math inline">\({x(t), t=t_0, ...}, x(t) \in R^d\)</span></p>
</div>
<p>where <span class="math inline">\(x(t)\)</span> are realizations of a random variable with a probability density function <span class="math inline">\(\phi_0\)</span>, anomaly detection involves identifying outliers by analyzing deviations from the normal data distribution <span class="math inline">\(\phi_0\)</span>. In all cases, we’re assuming that there’s a process generating data. Specifically, the process can be modeled as:</p>
<p><span class="math display">\[x(t)=\begin{cases}
    \phi_0, &amp; \text{if $x(t)$ belongs to normal data}.\\
    \phi_1, &amp; \text{if $x(t)$ belongs to anomalous data}.
  \end{cases}\]</span></p>
<figure>
<img alt="Process of detecting anomalies in a time-series signal x(t)" id="fig:enter-label" src="img/lecture24/Picture3.png"/><figcaption aria-hidden="true">Process of detecting anomalies in a time-series signal <span class="math inline">\(x(t)\)</span></figcaption>
</figure>
</section>
<section data-number="0.2.4" id="anomaly-detection-in-a-statistical-framework">
<h3 data-number="1.2.4"><span class="header-section-number">1.2.4</span> Anomaly Detection in a statistical framework</h3>
<p>Anomaly detection algorithms in a statistical context involves two fundamental components: a statistic and a decision rule. Since data integration process is something explicitly known, it is important to construct measurement.</p>
<section data-number="0.2.4.1" id="statistics">
<h4 data-number="1.2.4.1"><span class="header-section-number">1.2.4.1</span> Statistics</h4>
<p>A statistic quantifies the data’s behavior and has a predictable response under normal conditions, which means it’s always constant. Examples include:</p>
<ul>
<li><p>The average or mean</p></li>
<li><p>Sample variance</p></li>
<li><p>Log-likelihood values</p></li>
<li><p>Classifier confidence scores</p></li>
<li><p>An “anomaly score” specifically designed to highlight deviations</p></li>
</ul>
</section>
<section data-number="0.2.4.2" id="decision-rule">
<h4 data-number="1.2.4.2"><span class="header-section-number">1.2.4.2</span> Decision Rule</h4>
<p>The decision rule interprets the statistic to classify data as normal or anomalous. It can be changed or personalized depending on different tasks. Examples include:</p>
<ul>
<li><p>Adaptive thresholds: A dynamic boundary is set based on the observed statistic.</p></li>
<li><p>Confidence region: Zones within which data is considered normal</p></li>
</ul>
<figure>
<img alt="Figure showing how anomalies are identified based on statistical deviations" id="fig:enter-label" src="img/lecture24/Picture4.png"/><figcaption aria-hidden="true">Figure showing how anomalies are identified based on statistical deviations</figcaption>
</figure>
</section>
</section>
<section data-number="0.2.5" id="performance-metrics">
<h3 data-number="1.2.5"><span class="header-section-number">1.2.5</span> Performance Metrics</h3>
<p>When applying anomaly detection, it is crucial to define the specific goals for each scenario. For example, if a dataset is initially designed for multi-class classification, it may need to be adapted for anomaly detection. This involves transforming the dataset from NNN-class classification into a binary classification problem (i.e., normal vs. anomalous). In this new context, the focus shifts to addressing the imbalance between the two classes, as anomalies are typically much rarer than normal data. Effectively handling this class imbalance is essential for achieving accurate and meaningful results in anomaly detection.</p>
<section data-number="0.2.5.1" id="tpr-fpr">
<h4 data-number="1.2.5.1"><span class="header-section-number">1.2.5.1</span> TPR, FPR</h4>
<p>Evaluating the effectiveness of an anomaly detection algorithm requires the use of well-defined performance metrics. Two primary indicators are the True Positive Rate (TPR) and the False Positive Rate (FPR):</p>
<p><span class="math display">\[TPR=\frac{\text{number of anomalies detected}}{\text{number of anomalies}}\]</span></p>
<ul>
<li><p>Also referred to as recall, sensitivity, or hit rate.</p></li>
<li><p>Measures the proportion of actual anomalies correctly identified by the algorithm.</p></li>
</ul>
<p><span class="math display">\[FPR=\frac{\text{number of normal samples detected}}{\text{number of normal samples}}\]</span></p>
<ul>
<li><p>Represents the fraction of normal samples that were misclassified as anomalies.</p></li>
</ul>
<p>Note that we’ve discussed in the previous lectures:</p>
<div class="center">
<p>False Negative Rate (FNR) = 1 - TPR</p>
</div>
<div class="center">
<p>True Negative Rate (TNR) = 1 - FPR</p>
</div>
<p><span class="math display">\[\text{(Precision on anomalies)}=\frac{\text{anomalies detected}}{\text{detections}}\]</span></p>
<p>There is an inherent trade-off between TPR and FPR, which is governed by the choice of the threshold parameter <span class="math inline">\(\gamma\)</span>.</p>
<p>Adjusting <span class="math inline">\(\gamma\)</span> impacts the detection performance:</p>
<ul>
<li><p>Lower <span class="math inline">\(\gamma\)</span>: Increases TPR but may raise FPR, leading to more false positives.</p></li>
<li><p>Higher <span class="math inline">\(\gamma\)</span>: Reduces FPR but may lower TPR, resulting in missed anomalies.</p></li>
</ul>
<figure>
<img alt="Statistical Thresholding in Anomaly Detection" id="fig:enter-label" src="img/lecture24/Picture24.5.png"/><figcaption aria-hidden="true">Statistical Thresholding in Anomaly Detection</figcaption>
</figure>
<p>Therefore, to mitigate the trade-off between TPR and FPR, we need to consider at least two indicators (e.g. TPR, FPR) when assessing performance. To name a few example indicators combining both TPR and FPR, <span class="math display">\[\text{(Accuracy)}=\frac{\text{(anomalies detected) + (normal samples not detected)}}{\text{(samples)}}\]</span> which indicates the overall correctness of the model. <span class="math display">\[\text{(F1 score)}=\frac{2*\text{(anomalies detected)}}{\text{(detections + anomalies)}}\]</span> which balances precision and recall, providing a harmonic mean. In an ideal detector, the model detects all anomalies without false positives, and thus both accuracy and F1 score reach their maximum value of 1.</p>
</section>
<section data-number="0.2.5.2" id="area-under-the-curve-auc">
<h4 data-number="1.2.5.2"><span class="header-section-number">1.2.5.2</span> Area Under the Curve (AUC)</h4>
<p>Comparing different anomaly detection methods can be challenging, as it requires ensuring that all methods are configured optimally for fair evaluation. To achieve this, performance is typically visualized using the Receiver Operating Characteristic (ROC) curve. The ROC curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR) for various threshold values. The ideal detector is represented by a point at (0,1), where FPR = 0% and TPR = 100%. This corresponds to perfect performance with no false positives and all anomalies correctly identified. We can observe that</p>
<ul>
<li><p>A curve closer to the top-left corner (0,1) indicates better performance.</p></li>
<li><p>The Area Under the Curve (AUC) serves as a summary statistic to compare models: A higher AUC value indicates superior overall performance. The optimal parameters yield a curve and AUC value as close as possible to the ideal point.</p></li>
</ul>
<p>Thus, the ROC curve and AUC provide a robust framework for assessing and comparing the effectiveness of different models or algorithms in anomaly detection.</p>
<figure>
<img alt="ROC Curve Comparison Across Methods" id="fig:enter-label" src="img/lecture24/Picture6.png"/><figcaption aria-hidden="true">ROC Curve Comparison Across Methods</figcaption>
</figure>
</section>
</section>
</section>
<section data-number="0.3" id="anomaly-detection-settings">
<h2 data-number="1.3"><span class="header-section-number">1.3</span> Anomaly Detection Settings</h2>
<p>Anomaly detection can be approached in three primary scenarios:</p>
<ul>
<li><p><strong>Semi-supervised:</strong> Assumes access to mostly normal data with limited or no labeled anomalies. In other words, it is told that all the training data is normal.</p></li>
<li><p><strong>Unsupervised:</strong> Detects anomalies without any labeled data, relying entirely on inherent data patterns. In other words, training data can be either normal or anomaly, and it is not told.</p></li>
<li><p><strong>Supervised:</strong> Requires a labeled dataset with examples of both normal and anomalous instances. In other words, it is explicitly told that which data is normal and which data is anomaly.</p></li>
</ul>
</section>
<section data-number="0.4" id="semi-supervised-anomaly-detection">
<h2 data-number="1.4"><span class="header-section-number">1.4</span> Semi-supervised Anomaly Detection</h2>
<section data-number="0.4.1" id="context-and-assumptions">
<h3 data-number="1.4.1"><span class="header-section-number">1.4.1</span> Context and Assumptions</h3>
<p>In semi-supervised settings, the training data TR consists primarily of normal samples:</p>
<div class="center">
<p>TR = <span class="math inline">\(\{x(t), x \sim \phi_0\)</span> and <span class="math inline">\(t &lt; t_0\}\)</span></p>
</div>
<p>This approach is based on the following assumptions:</p>
<ol>
<li><p>Normal data are easy to gather and the vast majority.</p></li>
<li><p>Anomalous data are difficult and costly to collect and also select, so it would be difficult to gather a representative training set.</p></li>
<li><p>Training examples in TR might not be representative of all the possible anomalies that can occur.</p></li>
</ol>
<p>For these reasons, semi-supervised anomaly detection is often referred to as novelty detection.</p>
</section>
<section data-number="0.4.2" id="density-based-methods">
<h3 data-number="1.4.2"><span class="header-section-number">1.4.2</span> Density-based methods</h3>
<p>One common and popular approach for semi-supervised anomaly detection involves monitoring the log-likelihood of data with respect to the normal data distribution <span class="math inline">\(\phi_0\)</span>. The method can be outlined as follows:</p>
<ol>
<li><p><strong>Training Phase:</strong> Estimate the probability density function (PDF) <span class="math inline">\(\phi_0\)</span> using the training data TR. <span class="math inline">\(\phi_0\)</span> can be estimated from the training set using GMMs, which assumes data has normal generation process and takes advantage of mean, standard deviation, and multiple Gaussian distributions.</p></li>
<li><p><strong>Testing Phase:</strong> Compute the log-likelihood for each data point during testing</p>
<div class="center">
<p><span class="math inline">\(L(x(t)) = log(\phi_0(x(t)))\)</span></p>
</div>
<p>Monitor the log-likelihood values over time:</p>
<div class="center">
<p><span class="math inline">\({L(x(t)), t = 1, ...}\)</span></p>
</div></li>
</ol>
<p>If anomalies stay near the Gaussian mixture model, it means that the statistics used for the process are not correct.</p>
</section>
<section data-number="0.4.3" id="advantages-and-disadvantages">
<h3 data-number="1.4.3"><span class="header-section-number">1.4.3</span> Advantages and Disadvantages</h3>
<ul>
<li><p><strong>Advantages:</strong> The PDF <span class="math inline">\(\phi_0\)</span> provides a measure of confidence in the detection, analogous to a p-value. Robust density estimation methods can tolerate a small number of anomalous samples in the training data (TR).</p></li>
<li><p><strong>Disadvantages:</strong> High-dimensional data poses significant challenges for density estimation, as it becomes computationally expensive and prone to overfitting.</p></li>
</ul>
</section>
</section>
<section data-number="0.5" id="unsupervised-anomaly-detection">
<h2 data-number="1.5"><span class="header-section-number">1.5</span> Unsupervised Anomaly Detection</h2>
<p>Unsupervised, data-driven, detection of anomalies is a standard technique in machine learning. Throughout the years, many methods, or algorithms, have been developed to detect anomalies. Most anomaly detection tasks are conducted unsupervised, which means that no labels are available to the user. Consequently, this means that regular optimization, like grid searches for optimal hyperparameters used in supervised learning, are not used within unsupervised anomaly detection. Most unsupervised anomaly detection algorithms produce scores, rather than labels, to samples. The most common convention is that a higher score indicates a higher likelihood that a sample is an anomaly, making unsupervised anomaly detection a ranking problem.</p>
<section data-number="0.5.1" id="definition-1">
<h3 data-number="1.5.1"><span class="header-section-number">1.5.1</span> Definition</h3>
<p>Unsupervised anomaly detection occurs when there are no labeled anomalies in the training data, and the model needs to identify anomalies without prior knowledge of what constitutes an anomaly. The model’s task is to find data points that deviate significantly from most of the data, making it suitable for cases where anomalies are rare or poorly understood.</p>
</section>
<section data-number="0.5.2" id="problem-setting">
<h3 data-number="1.5.2"><span class="header-section-number">1.5.2</span> Problem Setting</h3>
<p>In the context of unsupervised anomaly detection, we assume the training set TR contains both normal and anomalous data but lacks explicit labels. The training set is denoted as:</p>
<div class="center">
<p><span class="math inline">\(TR = \{x(t), t &lt; t_0\}\)</span>, where x(t) represents a data point observed at time t</p>
</div>
<p>The underlying assumption is that anomalies are rare relative to normal data. This scarcity forms the basis for differentiating anomalies from most data points. Without prior knowledge of class labels, the problem becomes one of identifying data points that exhibit statistical or structural deviations from the majority.</p>
</section>
<section data-number="0.5.3" id="methodologies">
<h3 data-number="1.5.3"><span class="header-section-number">1.5.3</span> Methodologies</h3>
<p>Several methodologies have been developed for unsupervised anomaly detection, leveraging diverse principles such as proximity, density, and isolation. The most common techniques are discussed below.</p>
<section data-number="0.5.3.1" id="distance-based-methods">
<h4 data-number="1.5.3.1"><span class="header-section-number">1.5.3.1</span> Distance-Based Methods</h4>
<p>Distance-based methods rely on the hypothesis that normal data resides in dense neighborhoods, whereas anomalies are distant from their nearest neighbors.</p>
<figure>
<img alt="Distance-based Methods" id="fig:enter-label" src="img/lecture24/Picture1.png"/><figcaption aria-hidden="true">Distance-based Methods</figcaption>
</figure>
<ul>
<li><p>These methods monitor in the following steps:</p>
<ol>
<li><p>Measure the distance between each data point and its <span class="math inline">\(k\)</span>-nearest neighbors (<span class="math inline">\(k\)</span>-NN).</p></li>
<li><p>Determining if a data point belongs to sparse clusters, exists at the periphery of dense clusters, or is entirely isolated.</p></li>
<li><p>The effectiveness of distance-based methods hinges on selecting an appropriate similarity metric (e.g., Euclidean, Manhattan, distance).</p></li>
</ol></li>
<li><p>Key steps:</p>
<ol>
<li><p>For each data point, calculate the distance to <span class="math inline">\(k\)</span>-NN.</p></li>
<li><p>Normalize distances relative to neighbors to identify sparsity.</p></li>
<li><p>Points with high normalized distances are flagged as anomalies.</p></li>
</ol></li>
<li><p>Challenges:</p>
<ol>
<li><p>High computational costs for large datasets.</p></li>
<li><p>Sensitivity to parameter choices such as k in k-nearest neighbors.</p></li>
</ol></li>
</ul>
</section>
<section data-number="0.5.3.2" id="isolation-forest">
<h4 data-number="1.5.3.2"><span class="header-section-number">1.5.3.2</span> Isolation Forest</h4>
<p>The term Isolation means ‘separating an instance from the rest of the instances.’ Since anomalies are ‘few and different’ and therefore they are more susceptible to isolation. The Isolation Forest algorithm is grounded in the notion that anomalies are easier to isolate from most normal data points. It employs a forest of binary trees constructed iteratively by:</p>
<ol>
<li><p>Selecting a feature <span class="math inline">\(x_i\)</span>and a random split value within its range.</p></li>
<li><p>Splitting data recursively, isolating anomalies in fewer steps due to their sparsity.</p></li>
<li><p>The path length to isolate a data point is inversely proportional to its normalcy.</p></li>
</ol>
<figure>
<img alt="Isolation Forest: Step-by-step" id="fig:enter-label" src="img/lecture24/Picture8.png"/><figcaption aria-hidden="true">Isolation Forest: Step-by-step</figcaption>
</figure>
<p>Isolation Forest is computationally efficient and scalable, making it suitable for high-dimensional datasets.</p>
</section>
</section>
<section data-number="0.5.4" id="enhancements-and-integration">
<h3 data-number="1.5.4"><span class="header-section-number">1.5.4</span> Enhancements and Integration</h3>
<p>While unsupervised methods excel in label-free scenarios, their efficacy can be improved through semi-supervised learning when partial labels are available. For instance:</p>
<ul>
<li><p>Integration with DBSCAN to identify clusters and outliers.</p></li>
<li><p>Combining Isolation Forest with neural network-based autoencoders for feature extraction.</p></li>
<li><p>Dynamically adjusting thresholds for anomaly scores based on data characteristics.</p></li>
</ul>
</section>
<section data-number="0.5.5" id="applications">
<h3 data-number="1.5.5"><span class="header-section-number">1.5.5</span> Applications</h3>
<p>Unsupervised anomaly detection methods have broad applicability across domains:</p>
<ul>
<li><p>Cybersecurity: Detecting unusual access patterns or malware activity.</p></li>
<li><p>Industrial Monitoring: Identifying equipment faults or inefficiencies.</p></li>
<li><p>- Finance: Uncovering fraudulent transactions in banking and e-commerce.</p></li>
</ul>
</section>
<section data-number="0.5.6" id="challenges">
<h3 data-number="1.5.6"><span class="header-section-number">1.5.6</span> Challenges</h3>
<p>Unsupervised anomaly detection faces significant challenges, including:</p>
<ul>
<li><p>Scalability to datasets with numerous features</p></li>
<li><p>Adapting to evolving data distributions in dynamic environments</p></li>
<li><p>Explaining why certain data points are deemed anomalous</p></li>
</ul>
</section>
</section>
<section data-number="0.6" id="statistical-methods">
<h2 data-number="1.6"><span class="header-section-number">1.6</span> Statistical Methods</h2>
<p>Image-based anomaly detection identifies regions or patches in an image that deviate from normal patterns. Unlike global anomaly detection, which considers an entire image as a single entity, patch-based techniques and pixel-level analysis offer higher granularity and better sensitivity for anomalies localized to specific regions.</p>
<section data-number="0.6.1" id="patch-based-image-analysis">
<h3 data-number="1.6.1"><span class="header-section-number">1.6.1</span> Patch-based Image Analysis</h3>
<p>Patch-based image analysis divides an image into smaller, non-overlapping or overlapping regions (patches), enabling localized feature extraction. The methodology relies on isolating normal patterns during training and comparing these to unseen patches during inference.</p>
<section data-number="0.6.1.1" id="training-process">
<h4 data-number="1.6.1.1"><span class="header-section-number">1.6.1.1</span> Training Process</h4>
<ol>
<li><p>Normal images divided into patches s, typically of fixed dimensions (e.g., 4x4 or 8x8 pixels).</p></li>
<li><p>Each patch is modeled as a multivariate Gaussian distribution: <span class="math inline">\(\phi_0 = N(\mu, \sum)\)</span>, where <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sum\)</span> represents the mean vector and covariance matrix of normal patches, respectively.</p></li>
<li><p>The statistical model captures the likelihood of a patch belonging to the normal distribution.</p></li>
</ol>
</section>
<section data-number="0.6.1.2" id="testing-process">
<h4 data-number="1.6.1.2"><span class="header-section-number">1.6.1.2</span> Testing Process</h4>
<ol>
<li><p>Test images are similarly divided into patches.</p></li>
<li><p>The likelihood <span class="math inline">\(\phi_0(s)\)</span> of each patch is evaluated based on the trained model.</p></li>
<li><p>Patches with likelihood values below a predefined threshold are flagged as anomalous.</p></li>
</ol>
</section>
<section data-number="0.6.1.3" id="challenges-1">
<h4 data-number="1.6.1.3"><span class="header-section-number">1.6.1.3</span> Challenges</h4>
<p>Larger patch size increases the dimensionality of the feature space, making Gaussian modeling less effective. Adjacent patches often exhibit dependencies that Gaussian models fail to capture. This method assumes independence between pixel intensities in a patch, which may not always hold in real-world scenarios. As patch size increases, modeling becomes increasingly challenging due to high-dimensional data correlations.</p>
</section>
</section>
<section data-number="0.6.2" id="adjacent-pixel-value-distribution">
<h3 data-number="1.6.2"><span class="header-section-number">1.6.2</span> Adjacent Pixel-value Distribution</h3>
<p>Adjacent pixel-value distribution focuses on the spatial correlations between neighboring pixels, providing insights into texture and structural patterns within an image.</p>
<section data-number="0.6.2.1" id="correlation-in-spatial-data">
<h4 data-number="1.6.2.1"><span class="header-section-number">1.6.2.1</span> Correlation in Spatial Data</h4>
<ul>
<li><p>- Image pixels are inherently correlated due to continuous and smooth transitions in natural images.</p></li>
<li><p>- Modeling such dependencies using simple probabilistic functions (e.g., Gaussians) is challenging.</p></li>
</ul>
</section>
<section data-number="0.6.2.2" id="key-insights">
<h4 data-number="1.6.2.2"><span class="header-section-number">1.6.2.2</span> Key Insights</h4>
<ul>
<li><p>Histograms and Scatter Plots: Visual analysis of pixel intensities reveals spatial dependencies and correlations.</p></li>
<li><p>Limitations of Gaussian Models: Gaussian models fail to capture the complexity of high-dimensional and highly correlated data, especially in larger patches.</p></li>
</ul>
</section>
<section data-number="0.6.2.3" id="implications-for-anomaly-detection">
<h4 data-number="1.6.2.3"><span class="header-section-number">1.6.2.3</span> Implications for Anomaly Detection</h4>
<ul>
<li><p>Correlations among pixel values can obscure anomalies, necessitating advanced techniques that account for spatial structure (e.g., convolutional filters in neural networks).</p></li>
</ul>
<p>Scatter plots of adjacent pixel values reveal intrinsic patterns of normal images, allowing anomalies to be identified as deviations from these distributions. Incorporating local pixel interactions enhances the robustness of anomaly detection.</p>
</section>
</section>
<section data-number="0.6.3" id="maximum-softmax-probability-msp">
<h3 data-number="1.6.3"><span class="header-section-number">1.6.3</span> Maximum Softmax Probability (MSP)</h3>
<p>Maximum Softmax Probability is a neural network-based approach to detect anomalies by analyzing the output probability distribution of a classification model.</p>
<figure>
<img alt="Convolution Neural Network" id="fig:enter-label" src="img/lecture24/Picture10.png"/><figcaption aria-hidden="true">Convolution Neural Network</figcaption>
</figure>
<section data-number="0.6.3.1" id="concept">
<h4 data-number="1.6.3.1"><span class="header-section-number">1.6.3.1</span> Concept</h4>
<ul>
<li><p>Neural networks produce softmax scores for classification tasks, where each score represents the likelihood of an input belonging to a specific class.</p></li>
<li><p>MSP leverages these scores to identify out-of-distribution (OOD) samples</p></li>
</ul>
</section>
<section data-number="0.6.3.2" id="methodology">
<h4 data-number="1.6.3.2"><span class="header-section-number">1.6.3.2</span> Methodology</h4>
<ul>
<li><p>Softmax Thresholding: A threshold is defined for softmax scores. If the maximum softmax probability for an input falls below this threshold, the input is classified as anomalous.</p></li>
<li><p>In-distribution vs. Out-of-distribution: In-distribution samples yield high softmax probabilities for one class, whereas OOD samples produce lower scores.</p></li>
</ul>
</section>
<section data-number="0.6.3.3" id="advantage">
<h4 data-number="1.6.3.3"><span class="header-section-number">1.6.3.3</span> Advantage</h4>
<ul>
<li><p>MSP provides a simple yet effective method for anomaly detection in neural networks.</p></li>
<li><p>It allows for anomaly detection without modifying the training process, making it computationally efficient.</p></li>
</ul>
</section>
<section data-number="0.6.3.4" id="challenges-2">
<h4 data-number="1.6.3.4"><span class="header-section-number">1.6.3.4</span> Challenges</h4>
<ul>
<li><p>High dependence on the softmax calibration of the model.</p></li>
<li><p>Sensitivity to the choice of threshold, which may vary across datasets.</p></li>
</ul>
</section>
</section>
<section data-number="0.6.4" id="manifolds-in-natural-images">
<h3 data-number="1.6.4"><span class="header-section-number">1.6.4</span> Manifolds in Natural Images</h3>
<p>Natural images exhibit low-dimensional manifold structures embedded in high-dimensional spaces. This insight is pivotal in understanding how anomalies deviate from normal data.</p>
<ol>
<li><p>Theoretical Basis Image patches lie close to a low-dimensional manifold. Anomalies are outliers that reside far from this manifold.</p></li>
<li><p>Implementation vis Neural Networks Convolutional Neural Networks (CNNs) extract feature representations that are clustered closer on the manifold for normal patches. Latent representations in the manifold reveal the underlying structure of normal data, facilitating anomaly detection.</p></li>
</ol>
<p>This approach significantly improves the interpretability and precision of anomaly detection in complex datasets.</p>
</section>
</section>
<section data-number="0.7" id="extended-mythologies">
<h2 data-number="1.7"><span class="header-section-number">1.7</span> Extended Mythologies</h2>
<p>To address the limitation of each method, hybrid approaches can be developed by integrating statistical modeling, manifold learning, and deep learning:</p>
<section data-number="0.7.1" id="reconstruction-based-methods">
<h3 data-number="1.7.1"><span class="header-section-number">1.7.1</span> Reconstruction-Based Methods</h3>
<ol>
<li><p>Definition: Reconstruction-based methods fit a statistical model to the observation to describe dependence and apply anomaly detection on the independent residuals. The rationale is that <span class="math inline">\(\mu\)</span> can reconstruct only normal data, and thus anomalies are expected to yield large reconstruction errors.</p></li>
<li><p>Process: Detection is performed by using a model <span class="math inline">\(\mu\)</span>, which can encode and reconstruct normal data as follows:</p>
<ul>
<li><p>During training: Learn the model <span class="math inline">\(\mu\)</span> from training set TR.</p></li>
<li><p>During testing:</p>
<ul>
<li><p>Encode and reconstruct each test signal s through <span class="math inline">\(\mu\)</span></p></li>
<li><p>Assess err(s), namely the residual between s and its reconstruction through <span class="math inline">\(\mu\)</span></p></li>
</ul></li>
</ul></li>
<li><p>Example: Autoencoders</p>
<figure>
<img alt="Structure of Autocoder" id="fig:enter-label" src="img/lecture24/Picture11.png"/><figcaption aria-hidden="true">Structure of Autocoder</figcaption>
</figure>
<ol>
<li><p>Neural networks used for data reconstruction since they learn the identity function.</p></li>
<li><p>Autoencoders are trained to reconstruct all the samples in the training set. The reconstruction loss over the training set TR is <span class="math display">\[L(TR)=\Sigma_{s\in{TR}}\Big|\big|s-D(E(S))\big|\Big|_2\]</span></p></li>
<li><p>Training of D(E(•)) is performed through standard backpropagation methods, e.g. SGD.</p></li>
</ol></li>
</ol>
</section>
<section data-number="0.7.2" id="gradient-constraints">
<h3 data-number="1.7.2"><span class="header-section-number">1.7.2</span> Gradient Constraints</h3>
<ul>
<li><p>Introduce gradient-based regularizations during training to enhance anomaly separation</p></li>
<li><p>Use GradCON (Gradient Constrained Optimization) to penalize anomalies during model updates</p></li>
</ul>
</section>
<section data-number="0.7.3" id="ensemble-techniques">
<h3 data-number="1.7.3"><span class="header-section-number">1.7.3</span> Ensemble Techniques</h3>
<ul>
<li><p>Combine MSP with Patch-based and manifold approaches for robust multi-scale anomaly detection</p></li>
</ul>
</section>
</section>


</main>
</body>
</html>
