<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>L8_ECE4252-8803_PolynomialRegression</title>
  <link rel="stylesheet" href="../assets/style.css"/>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav>
  <a href="../index.html">Home</a>
</nav>
<main>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>_Lecture8</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div class="center">

</div>
<p><strong>Contributors:</strong> Dr. Ahmad Mustafa, Dr. Motaz Alfarraj, Dr. Ashraf Alattar, Dr. Chen Zhou</p>
<p><strong>Teaching Assistants</strong> with remarkable contributions include: Kuo-Wei Lai, Wuyang Du, Shiva Mahato, Michael Zhou, Ninghan Zhong</p>
<p><strong>Disclaimer</strong>: <span>All content of these notes are part of this course at Georgia Tech. Any re-use or distribution is not permitted without pre-approved permission. All these notes belong to, created by, and copyrighted for Ghassan AlRegib and Mohit Prabhushankar, Georgia Tech, 2021–2028.</span></p>
<p><strong>License</strong>: <span>These lecture notes are licensed under the <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.</span></p>
<p><strong>Errata</strong>: <span><em>Please submit any errata you find using the following form: <a href="https://forms.office.com/r/fbg9dMWPgY">Errata Form for FunML Textbook</a> or visit: <a href="https://forms.office.com/r/fbg9dMWPgY">https://forms.office.com/r/fbg9dMWPgY</a></em></span></p>
<section id="overview" data-number="0.1">
<h2 data-number="1.1"><span class="header-section-number">1.1</span> Overview</h2>
<p>This lecture extends <strong>linear regression</strong> to settings where the relationship between inputs and outputs is <strong>nonlinear</strong>. We will see that many nonlinear models can still be written in a form that is <strong>linear in the parameters</strong> by introducing a feature mapping <span class="math inline">\(\phi(\cdot)\)</span>. In particular, <strong>polynomial regression</strong> creates nonlinear behavior by expanding the original inputs into polynomial features, while keeping the prediction rule in the simple form <span class="math inline">\(\hat{y}=\boldsymbol{\theta^T}\phi(\boldsymbol{x})\)</span>.</p>
<p>The first theme of the lecture is <strong>high-degree polynomial regression</strong>. We motivate why polynomial features are useful, how they capture curvature and interactions (cross terms) in multi-feature datasets, and why increasing the polynomial degree increases model capacity. We emphasize the core trade-off: higher degree can improve expressiveness, but it also increases the number of features rapidly and can lead to <strong>overfitting</strong>, where the model fits noise rather than the underlying signal.</p>
<p>The second theme is <strong>training by gradient descent</strong>. While the Normal Equation provides a closed-form solution for least squares regression, it becomes computationally expensive and numerically fragile as the feature dimension grows (especially after polynomial expansion). This motivates iterative optimization. We review the gradient descent update rule <span class="math inline">\(\boldsymbol{\theta}^{(t+1)} = \boldsymbol{\theta}^{(t)} - \alpha \nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta}^{(t)})\)</span>, interpret the learning rate <span class="math inline">\(\alpha\)</span> as a step size, and compare Batch, Stochastic, and Mini-batch gradient descent as practical alternatives with different stability and computational trade-offs. We also preview common optimization considerations, including convergence behavior, learning rate selection, and the role of learning-rate schedules.</p>
<p>The third theme is <strong>regularization</strong>. Polynomial regression can easily become a high-capacity model, so controlling complexity is essential for good generalization. We introduce regularization as adding a penalty term to the least squares objective, typically written as <span class="math display">\[\min_{\boldsymbol{\theta}}\ 
\frac{1}{N}\sum_{i=1}^{N}
\left(\boldsymbol{\theta}^T \boldsymbol{\phi}(\mathbf{x}_i) - y_i\right)^2
+ \lambda\,\Omega(\boldsymbol{\theta}).\]</span> We discuss how <span class="math inline">\(L_2\)</span> regularization (Ridge) tends to shrink weights smoothly and improve numerical stability, while <span class="math inline">\(L_1\)</span> regularization (Lasso) can encourage sparsity and act as a form of feature selection. Conceptually, regularization shifts the solution toward simpler models, reducing variance and improving test-time performance.</p>
<p>Overall, the lecture proceeds from constructing nonlinear models via polynomial feature maps, to optimizing their parameters efficiently with gradient-based methods, and finally to preventing overfitting through regularization. By the end, you should be able to explain how polynomial regression is built, how it is trained in practice, and why regularization is often necessary when model capacity grows.</p>
</section>
<section id="nonlinear-regression" data-number="0.2">
<h2 data-number="1.2"><span class="header-section-number">1.2</span> Nonlinear Regression</h2>
<p>When the relationship between the input and output variables is not linear, a standard linear regression model is insufficient. Instead, we introduce a nonlinear regression framework through a feature transformation.</p>
<p>Rather than directly using the original feature matrix <span class="math inline">\(\mathbf{X}\)</span>, we map each input vector <span class="math inline">\(\mathbf{x}_i\)</span> into a higher-dimensional feature space using a nonlinear transformation <span class="math inline">\(\boldsymbol{\phi}(\mathbf{x}_i)\)</span>. The regression model then becomes</p>
<p><span class="math display">\[y_i = \boldsymbol{\theta}^T \boldsymbol{\phi}(\mathbf{x}_i)\]</span></p>
<p>Although this model can represent nonlinear relationships in the original input space, it remains <em>linear in the parameters</em> <span class="math inline">\(\theta\)</span>. This is an extension of the linear model in a transformed feature space, as illustrated in Figure 8.1.</p>
<figure>
<img src="img/lecture8/non-linear regression.png" alt="Linear model in transformed feature space capturing nonlinear relationships" /><figcaption aria-hidden="true">Linear model in transformed feature space capturing nonlinear relationships</figcaption>
</figure>
<p>More expressive nonlinear regression models use multiple basis functions:</p>
<p><span class="math display">\[y_i = \theta_0 + \theta_1\phi_1(\mathbf{x}_i) + \theta_2\phi_2(\mathbf{x}_i) + \dots + \theta_m\phi_m(\mathbf{x}_i), \quad i = 1,\dots,N\]</span></p>
<p>In matrix form, if we define the transformed design matrix</p>
<p><span class="math display">\[\boldsymbol{\Phi} =
\begin{bmatrix}
\boldsymbol{\phi}(\mathbf{x}_1)^T \\
\boldsymbol{\phi}(\mathbf{x}_2)^T \\
\vdots \\
\boldsymbol{\phi}(\mathbf{x}_N)^T
\end{bmatrix}\]</span></p>
<p>then the model becomes</p>
<p><span class="math display">\[\boldsymbol{\hat{y}} = \boldsymbol{\Phi} \boldsymbol{\theta}\]</span></p>
<p>This formulation allows us to reuse all tools from linear regression (least squares, gradient descent, regularization) while modeling nonlinear relationships through feature engineering.</p>
</section>
<section id="polynomial-regression" data-number="0.3">
<h2 data-number="1.3"><span class="header-section-number">1.3</span> Polynomial Regression</h2>
<p>A common and important special case of nonlinear regression is <strong>polynomial regression</strong>. This method is used when the relationship between the input and output exhibits curvature that cannot be captured by a linear function. Instead of assuming linear dependence on the original feature, we expand the input using polynomial basis functions.</p>
<p>For a single input feature <span class="math inline">\(x\)</span>, the feature mapping is defined as</p>
<p><span class="math display">\[\boldsymbol{\phi}(\boldsymbol{x_i}) = [1, x_i, x_i^2, x_i^3, \dots, x_i^m]^T\]</span></p>
<p>where <span class="math inline">\(m\)</span> is the degree of the polynomial. The regression model becomes</p>
<p><span class="math display">\[\hat{y}_i = \theta_0 + \theta_1 x_i + \theta_2 x_i^2 + \dots + \theta_m x_i^m\]</span></p>
<p>Although the model is nonlinear in the input variable <span class="math inline">\(x\)</span>, it remains <strong>linear in the parameters</strong> <span class="math inline">\(\boldsymbol \theta\)</span>, which allows us to use all linear regression optimization techniques.</p>
<figure>
<img src="img/lecture8/Nonlinear data generated via quadratic equation and Polynomial regression model prediction fitted to data.png" alt="Nonlinear data generated from a quadratic function with noise and the fitted polynomial regression curve" /><figcaption aria-hidden="true">Nonlinear data generated from a quadratic function with noise and the fitted polynomial regression curve</figcaption>
</figure>
<p>In matrix form, the transformed design matrix becomes</p>
<p><span class="math display">\[\mathbf{\Phi} =
\begin{bmatrix}
1 &amp; x_1 &amp; x_1^2 &amp; \dots &amp; x_1^m \\
1 &amp; x_2 &amp; x_2^2 &amp; \dots &amp; x_2^m \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x_N &amp; x_N^2 &amp; \dots &amp; x_N^m
\end{bmatrix}\]</span></p>
<p>so that</p>
<p><span class="math display">\[\hat{\mathbf{y}} = \mathbf{\Phi}\boldsymbol{\theta}\]</span></p>
<p>This framework naturally extends to multiple input features, where polynomial terms also include cross-products such as <span class="math inline">\(x_1 x_2\)</span>, <span class="math inline">\(x_1^2 x_2\)</span>, etc., leading to higher expressive power but also rapid growth in feature dimensionality.</p>
<section id="high-degree-polynomial-regression" data-number="0.3.1">
<h3 data-number="1.3.1"><span class="header-section-number">1.3.1</span> High-Degree Polynomial Regression</h3>
<p>When the input vector has multiple features, polynomial regression extends the feature mapping to include powers and interactions between features. Suppose each data point has <span class="math inline">\(P\)</span> input features,</p>
<p><span class="math display">\[\mathbf{x}_i = [x_{i,1}, x_{i,2}, \dots, x_{i,P}]^T.\]</span></p>
<p>A polynomial feature mapping of degree <span class="math inline">\(m\)</span> includes all monomials whose total degree is less than or equal to <span class="math inline">\(m\)</span>. For example, with <span class="math inline">\(P = 2\)</span> features and polynomial degree <span class="math inline">\(m = 2\)</span>, the model becomes</p>
<p><span class="math display">\[y_i = \theta_0 
+ \theta_1 x_{i,1} 
+ \theta_2 x_{i,2} 
+ \theta_3 x_{i,1}^2 
+ \theta_4 x_{i,2}^2 
+ \theta_5 x_{i,1} x_{i,2} 
+ \epsilon_i.\]</span></p>
<p>Here, the term <span class="math inline">\(x_{i,1}x_{i,2}\)</span> represents an interaction between features, allowing the model to capture more complex relationships.</p>
<p>More generally, polynomial regression with multiple features constructs a feature vector</p>
<p><span class="math display">\[\phi(\mathbf{x}_i) = 
[1, x_{i,1}, \dots, x_{i,P}, x_{i,1}^2, x_{i,1}x_{i,2}, \dots, x_{i,P}^m]^T,\]</span></p>
<p>leading again to a linear model in the parameters,</p>
<p><span class="math display">\[\hat{y}_i = \boldsymbol{\theta}^T \phi(\mathbf{x}_i).\]</span></p>
<p>Stacking all transformed feature vectors produces the polynomial design matrix</p>
<p><span class="math display">\[\boldsymbol{\Phi} =
\begin{bmatrix}
\phi(\mathbf{x}_1)^T \\
\phi(\mathbf{x}_2)^T \\
\vdots \\
\phi(\mathbf{x}_N)^T
\end{bmatrix},
\qquad
\hat{\mathbf{y}} = \boldsymbol{\Phi}\boldsymbol{\theta}.\]</span></p>
<p>However, the number of features grows rapidly with both <span class="math inline">\(P\)</span> and <span class="math inline">\(m\)</span>. The total number of polynomial terms (including interactions) is</p>
<p><span class="math display">\[\binom{P + m}{m},\]</span></p>
<p>which can become very large even for moderate <span class="math inline">\(P\)</span> and <span class="math inline">\(m\)</span>. For example, a dataset like IRIS with <span class="math inline">\(P=4\)</span> features and degree <span class="math inline">\(m=4\)</span> already produces <span class="math inline">\(70\)</span> polynomial features. This rapid growth increases computational cost and the risk of overfitting, motivating the need for regularization and more efficient optimization methods.</p>
</section>
<section id="least-squares-cost-function" data-number="0.3.2">
<h3 data-number="1.3.2"><span class="header-section-number">1.3.2</span> Least Squares Cost Function</h3>
<p>The least squares loss for polynomial regression is</p>
<p><span class="math display">\[L(\boldsymbol{\theta}) 
= \frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i)^2 
= \frac{1}{N} \sum_{i=1}^{N} (\boldsymbol{\theta}^T \phi(\mathbf{x}_i) - y_i)^2.\]</span></p>
<p>Here, <span class="math inline">\(L(\boldsymbol{\theta})\)</span> is the loss function dependent on the model parameters <span class="math inline">\(\boldsymbol{\theta}\)</span>. The term <span class="math inline">\(\frac{1}{N}\sum_{i=1}^N\)</span> computes the mean squared error across all <span class="math inline">\(N\)</span> data points. The model prediction is</p>
<p><span class="math display">\[\hat{y}_i = \boldsymbol{\theta}^T \phi(\mathbf{x}_i),\]</span></p>
<p>where <span class="math inline">\(\phi(\mathbf{x}_i)\)</span> denotes the nonlinear feature mapping applied to the input. Squaring the residual <span class="math inline">\((\hat{y}_i - y_i)\)</span> penalizes larger prediction errors more strongly.</p>
<p>In matrix form, the loss can be written compactly as</p>
<p><span class="math display">\[L(\boldsymbol{\theta})
= \frac{1}{N} \|\boldsymbol{\Phi}\boldsymbol{\theta} - \mathbf{y}\|_2^2.\]</span></p>
</section>
<section id="finding-optimal-parameters" data-number="0.3.3">
<h3 data-number="1.3.3"><span class="header-section-number">1.3.3</span> Finding Optimal Parameters</h3>
<p>We seek model parameters <span class="math inline">\(\boldsymbol{\theta}\)</span> that minimize the least squares loss</p>
<p><span class="math display">\[L(\boldsymbol{\theta}) 
= \frac{1}{N} \sum_{i=1}^{N} (\boldsymbol{\theta}^T \phi(\mathbf{x}_i) - y_i)^2
= \frac{1}{N}\|\boldsymbol{\Phi}\boldsymbol{\theta} - \mathbf{y}\|_2^2.\]</span></p>
<p>This is an optimization problem over the parameter vector <span class="math inline">\(\boldsymbol{\theta}\)</span>. The optimal parameters <span class="math inline">\(\boldsymbol{\theta}^\ast\)</span> satisfy the first-order optimality condition</p>
<p><span class="math display">\[\nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta}) = 0.\]</span></p>
<section id="gradient-of-the-loss." data-number="0.3.3.0.1">
<h5 data-number="1.3.3.0.1"><span class="header-section-number">1.3.3.0.1</span> Gradient of the loss.</h5>
<p>Taking the gradient of the matrix form gives</p>
<p><span class="math display">\[\nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta})
= \frac{2}{N}\boldsymbol{\Phi}^T(\boldsymbol{\Phi}\boldsymbol{\theta} - \mathbf{y}).\]</span></p>
<p>Setting the gradient equal to zero yields the <strong>normal equations</strong></p>
<p><span class="math display">\[\boldsymbol{\Phi}^T \boldsymbol{\Phi}\boldsymbol{\theta} 
= \boldsymbol{\Phi}^T \mathbf{y}.\]</span></p>
</section>
<section id="closed-form-solution." data-number="0.3.3.0.2">
<h5 data-number="1.3.3.0.2"><span class="header-section-number">1.3.3.0.2</span> Closed-form solution.</h5>
<p>If <span class="math inline">\(\boldsymbol{\Phi}^T\boldsymbol{\Phi}\)</span> is invertible, the optimal parameters are</p>
<p><span class="math display">\[\boxed{
\boldsymbol{\theta}^\ast
= (\boldsymbol{\Phi}^T\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}^T\mathbf{y}
}\]</span></p>
<p>This provides a closed-form solution for polynomial regression.</p>
</section>
</section>
<section id="the-normal-equation" data-number="0.3.4">
<h3 data-number="1.3.4"><span class="header-section-number">1.3.4</span> The Normal Equation</h3>
<p>For models that are linear in the parameters (including polynomial regression and other feature-mapped models), the least squares loss admits a closed-form solution. Let <span class="math inline">\(\boldsymbol{\Phi}\)</span> be the design matrix, where each row is <span class="math inline">\(\phi(\mathbf{x}_i)^T\)</span>, and let <span class="math inline">\(\mathbf{y}\)</span> be the vector of targets.</p>
<p>The loss function is</p>
<p><span class="math display">\[L(\boldsymbol{\theta}) = \frac{1}{N} \|\boldsymbol{\Phi}\boldsymbol{\theta} - \mathbf{y}\|_2^2.\]</span></p>
<p>Taking the gradient with respect to <span class="math inline">\(\boldsymbol{\theta}\)</span> and setting it to zero:</p>
<p><span class="math display">\[\nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta})
= \frac{2}{N}\boldsymbol{\Phi}^T(\boldsymbol{\Phi}\boldsymbol{\theta} - \mathbf{y}) = 0.\]</span></p>
<p>Rearranging gives</p>
<p><span class="math display">\[\boldsymbol{\Phi}^T \boldsymbol{\Phi}\boldsymbol{\theta}
= \boldsymbol{\Phi}^T \mathbf{y},\]</span></p>
<p>which leads to the <strong>Normal Equation</strong>:</p>
<p><span class="math display">\[\boldsymbol{\theta}
= (\boldsymbol{\Phi}^T \boldsymbol{\Phi})^{-1} \boldsymbol{\Phi}^T \mathbf{y}.\]</span></p>
<p>This provides a direct solution without iterative optimization methods such as gradient descent, provided that <span class="math inline">\(\boldsymbol{\Phi}^T \boldsymbol{\Phi}\)</span> is invertible.</p>
</section>
<h3 class="unnumbered" id="hypothesis-function">Hypothesis Function</h3>
<p>For models that are linear in the parameters, the hypothesis function can be written in matrix form as</p>
<p><span class="math display">\[\hat{\mathbf{y}} = \boldsymbol{\Phi}\boldsymbol{\theta}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\Phi}\)</span> is the design matrix whose <span class="math inline">\(i\)</span>-th row is <span class="math inline">\(\phi(\mathbf{x}_i)^T\)</span>, and <span class="math inline">\(\boldsymbol{\theta}\)</span> is the parameter vector. Each prediction is therefore</p>
<p><span class="math display">\[\hat{y}_i = \boldsymbol{\theta}^T \phi(\mathbf{x}_i).\]</span></p>
<p>This formulation includes linear regression, polynomial regression, and other feature-mapped models.</p>
<p>Linear regression is recovered as a special case when <span class="math inline">\(\phi(\mathbf{x}) = \mathbf{x}\)</span>.</p>
<h3 class="unnumbered" id="cost-function">Cost Function</h3>
<p>The mean squared error loss is defined as</p>
<p><span class="math display">\[L(\boldsymbol{\theta}) 
= \frac{1}{N} \|\boldsymbol{\Phi}\boldsymbol{\theta} - \mathbf{y}\|_2^2
= \frac{1}{N} \sum_{i=1}^{N} (\boldsymbol{\theta}^T \phi(\mathbf{x}_i) - y_i)^2\]</span></p>
<p>where <span class="math inline">\(N\)</span> is the number of training examples.</p>
<section id="computational-complexity-of-the-normal-equation" data-number="0.3.5">
<h3 data-number="1.3.5"><span class="header-section-number">1.3.5</span> Computational Complexity of the Normal Equation</h3>
<p>The computational cost of solving the Normal Equation is dominated by the linear algebra operations required to form and invert the matrix <span class="math inline">\(\boldsymbol{\Phi}^T \boldsymbol{\Phi}\)</span>, where <span class="math inline">\(\boldsymbol{\Phi}\)</span> is the design matrix with <span class="math inline">\(N\)</span> training examples and <span class="math inline">\(P\)</span> features (including any feature mappings such as polynomial terms).</p>
<p>Forming the matrix product <span class="math inline">\(\boldsymbol{\Phi}^T \boldsymbol{\Phi}\)</span> requires <span class="math inline">\(\mathcal{O}(N P^2)\)</span> operations, since each of the <span class="math inline">\(P^2\)</span> entries involves a sum over <span class="math inline">\(N\)</span> training examples. The subsequent matrix inversion of the <span class="math inline">\(P \times P\)</span> matrix <span class="math inline">\(\boldsymbol{\Phi}^T \boldsymbol{\Phi}\)</span> has a computational complexity of <span class="math inline">\(\mathcal{O}(P^3)\)</span>. Therefore, the overall computational complexity of computing the Normal Equation solution is</p>
<p><span class="math display">\[\mathcal{O}(N P^2 + P^3).\]</span></p>
<p>When the number of features <span class="math inline">\(P\)</span> is small, this cost is manageable, and the Normal Equation provides an efficient direct solution. However, as <span class="math inline">\(P\)</span> increases, the cubic term <span class="math inline">\(\mathcal{O}(P^3)\)</span> quickly becomes the dominant factor. This makes the method computationally expensive for high-dimensional feature spaces, particularly when polynomial feature expansions are used.</p>
<p>The Normal Equation is therefore most appropriate in settings where the number of training examples is much larger than the number of features (<span class="math inline">\(N \gg P\)</span>) and the feature dimension remains moderate. In high-dimensional problems, especially those arising from nonlinear feature mappings or large-scale datasets, iterative optimization methods such as gradient descent are preferred, since they avoid explicit matrix inversion and scale more favorably with dimensionality.</p>
</section>
</section>
<section id="gradient-descent" data-number="0.4">
<h2 data-number="1.4"><span class="header-section-number">1.4</span> Gradient Descent</h2>
<section id="overview-1" data-number="0.4.1">
<h3 data-number="1.4.1"><span class="header-section-number">1.4.1</span> Overview</h3>
<p>Gradient descent (GD) methods, including Batch GD, Stochastic GD, and Mini-batch GD, are commonly used for optimization in machine learning tasks. Each method shows different characteristics in terms of convergence behavior.</p>
</section>
<section id="finding-optimal-parameters-gradient-descent" data-number="0.4.2">
<h3 data-number="1.4.2"><span class="header-section-number">1.4.2</span> Finding Optimal Parameters (Gradient Descent)</h3>
<p>The Gradient Descent algorithm is used to find model parameters that minimize the loss function. We start from an initial parameter vector <span class="math inline">\(\boldsymbol{\theta}\)</span>, which may produce a high loss. By iteratively moving in the direction of the <strong>negative gradient</strong> of the loss, the parameters are updated to reduce the error.</p>
<p>There exists an optimal set of parameters <span class="math inline">\(\boldsymbol{\theta}^\ast\)</span> that minimizes the loss. Moving the parameters away from this point increases the loss.</p>
<p>In practice, it is almost impossible to achieve zero loss on real data. While prediction accuracy can sometimes reach <span class="math inline">\(100\%\)</span>, we do not directly optimize accuracy. Instead, we minimize a continuous loss function measuring the difference between the true targets <span class="math inline">\(\mathbf{y}\)</span> and the predictions <span class="math inline">\(\hat{\mathbf{y}}\)</span>. Using a continuous loss produces a smooth, typically convex, optimization landscape that is easier to optimize.</p>
<p>Understanding the idea of a <em>gradient</em> is therefore essential. In Figure 8.3, the horizontal axis represents <span class="math inline">\(\theta_1\)</span> in the single-feature case. With multiple features, additional axes correspond to parameters <span class="math inline">\(\theta_i\)</span>, producing a bowl-shaped loss surface in higher dimensions.</p>
<figure>
<img src="img/lecture8/The negative gradient (derivative) .png" id="fig:enter-label" alt="The negative gradient (derivative) points to the direction of the greatest rate of decrease of the cost function. Its magnitude is the slope of the function in that direction" /><figcaption aria-hidden="true">The negative gradient (derivative) points to the direction of the greatest rate of decrease of the cost function. Its magnitude is the slope of the function in that direction</figcaption>
</figure>
<p>The gradient is a tangent. We assume the shape and choose to travel along this gradient to pick our next point. Every path to the next point is a straight line. When we see a graph we visualize a continuous distribution, however, in a more practical setting, we move a certain distance in a straight line and the directionality of this point is given by the gradient. The learning rate is what gives you the distance you travel. Think of this as a vector with a scalar given by the learning rate and the direction given by the gradient.</p>
<p>The algorithm is called a Gradient Descent since we are moving in the direction of descending toward the minimum of the loss function. By definition, a gradient is in the direction of steepest <em>ascent</em>. Hence, we use the negative of the gradient to indicate that we’d like to <em>descend</em> the loss surface.</p>
</section>
<section id="gradients-of-the-least-squares-loss-function" data-number="0.4.3">
<h3 data-number="1.4.3"><span class="header-section-number">1.4.3</span> Gradients of the Least Squares Loss Function</h3>
<p>For models that are linear in the parameters with feature mapping <span class="math inline">\(\phi(\mathbf{x})\)</span>, the mean squared error loss is</p>
<p><span class="math display">\[L(\boldsymbol{\theta})
= \frac{1}{N} \sum_{i=1}^{N} \big(\boldsymbol{\theta}^T \phi(\mathbf{x}_i) - y_i\big)^2
= \frac{1}{N} \|\boldsymbol{\Phi}\boldsymbol{\theta} - \mathbf{y}\|_2^2\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\Phi}\)</span> is the design matrix whose <span class="math inline">\(i\)</span>-th row is <span class="math inline">\(\phi(\mathbf{x}_i)^T\)</span>.</p>
<h4 class="unnumbered" id="gradient-component-form">Gradient (Component Form)</h4>
<p>The derivative with respect to parameter <span class="math inline">\(\theta_p\)</span> is</p>
<p><span class="math display">\[\frac{\partial L(\boldsymbol{\theta})}{\partial \theta_p}
=
\frac{2}{N} \sum_{i=1}^{N}
\big(\boldsymbol{\theta}^T \phi(\mathbf{x}_i) - y_i\big)\,\phi_p(\mathbf{x}_i)\]</span></p>
<p>where <span class="math inline">\(\phi_p(\mathbf{x}_i)\)</span> is the <span class="math inline">\(p\)</span>-th feature in the transformed vector <span class="math inline">\(\phi(\mathbf{x}_i)\)</span>.</p>
<h4 class="unnumbered" id="gradient-matrix-form">Gradient (Matrix Form)</h4>
<p><span class="math display">\[\nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta})
=
\frac{2}{N}\,\boldsymbol{\Phi}^T (\boldsymbol{\Phi}\boldsymbol{\theta} - \mathbf{y})\]</span></p>
<p>This is the form typically used in implementations.</p>
<h4 class="unnumbered" id="gradient-descent-update-rule">Gradient Descent Update Rule</h4>
<p><span class="math display">\[\boldsymbol{\theta}^{(t+1)} 
= \boldsymbol{\theta}^{(t)} 
- \alpha \nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta}^{(t)})\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is the learning rate and <span class="math inline">\(t\)</span> is the iteration index.</p>
<h4 class="unnumbered" id="large-scale-training-considerations">Large-Scale Training Considerations</h4>
<p>When the number of training examples <span class="math inline">\(N\)</span> is large, computing the full gradient <span class="math inline">\(\boldsymbol{\Phi}^T(\boldsymbol{\Phi}\boldsymbol{\theta} - \mathbf{y})\)</span> at every step becomes computationally expensive. Instead, approximate gradients are used.</p>
<p>Batch Gradient Descent computes the gradient using the entire dataset at each iteration. This approach is accurate but slow when <span class="math inline">\(N\)</span> is large.</p>
<p>Stochastic Gradient Descent (SGD) updates parameters using a single randomly chosen training example at each step. This method is computationally cheap but introduces noise into the updates.</p>
<p>Mini-batch Gradient Descent uses small subsets of the training data at each step. It balances stability and efficiency and is the most common approach in practice.</p>
<p>Training proceeds iteratively until parameter updates become small or the loss stops decreasing.</p>
</section>
<section id="aside-contour-plots" data-number="0.4.4">
<h3 data-number="1.4.4"><span class="header-section-number">1.4.4</span> Aside: Contour Plots</h3>
<p>Before exploring the behavior of different Gradient Descent (GD) methods, we introduce the notion of <em>contour plots</em>. We consider an optimization setting where we aim to minimize a cost function <span class="math inline">\(L(\boldsymbol{\theta})\)</span> over two parameters, <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\theta_1\)</span>. This produces a <em>loss landscape</em> over the parameter space, which GD navigates in order to find a minimum.</p>
<p>Although we could visualize this optimization process using a 3D surface plot, it is often difficult to extract key insights from a static 3D view. Important features may be hidden depending on the viewing angle, making interpretation challenging.</p>
<p>As an example, consider the following loss landscape:</p>
<div class="center">
<p><img src="img/lecture8/loss-landscape.png" alt="image" /></p>
</div>
<p>Instead, we use a contour plot to represent the same loss landscape in 2D. In a contour plot, each <em>contour line</em> represents points where the cost function <span class="math inline">\(L(\boldsymbol{\theta})\)</span> has the same value. Moving along a contour line does not change the value of the cost function.</p>
<p>The goal of optimization is to find the minimum of the cost function, which corresponds to the lowest point on the surface. On a contour plot, this minimum is typically located at the center of the innermost contour. The trajectory traced by Gradient Descent on a contour plot shows how the parameters are updated step by step as we move toward this minimum.</p>
<section id="steps-to-create-a-contour-plot" data-number="0.4.4.0.1">
<h5 data-number="1.4.4.0.1"><span class="header-section-number">1.4.4.0.1</span> Steps to Create a Contour Plot</h5>
<p>To construct a contour plot for a given cost function:</p>
<ol>
<li><p>Define a grid of values for <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\theta_1\)</span>.</p></li>
<li><p>Compute the value of the cost function at each grid point.</p></li>
<li><p>Plot contour lines corresponding to different <em>levels</em> (constant values) of the cost function.</p></li>
<li><p>Overlay the path taken by a GD algorithm to visualize how it traverses the contours toward the minimum.</p></li>
</ol>
<p>For our example:</p>
<div class="center">
<p><img src="img/lecture8/contour-plot.png" alt="image" /></p>
</div>
</section>
<section id="what-contour-plots-reveal" data-number="0.4.4.0.2">
<h5 data-number="1.4.4.0.2"><span class="header-section-number">1.4.4.0.2</span> What Contour Plots Reveal</h5>
<p>From a contour plot, we can infer several important properties of the loss landscape:</p>
<ul>
<li><p><strong>Curvature and gradient magnitude:</strong> Closely spaced contours indicate a steep region (large gradient magnitude), while widely spaced contours indicate a flatter region (small gradient magnitude).</p></li>
<li><p><strong>Optimization efficiency:</strong> The paths taken by different GD variants show how efficiently each method reduces the cost.</p></li>
<li><p><strong>Learning rate effects:</strong> A learning rate that is too large can cause overshooting across contours, while one that is too small leads to slow progress. Contour plots help visualize these different behaviors.</p></li>
</ul>
</section>
<section id="higher-dimensions" data-number="0.4.4.0.3">
<h5 data-number="1.4.4.0.3"><span class="header-section-number">1.4.4.0.3</span> Higher Dimensions</h5>
<p>In higher-dimensional settings (more than two parameters), direct visualization becomes difficult. Common strategies include visualizing 2D slices of the parameter space, or using color/time to encode additional dimensions. The choice of visualization depends on the problem.</p>
</section>
</section>
<section id="batch-gradient-descent" data-number="0.4.5">
<h3 data-number="1.4.5"><span class="header-section-number">1.4.5</span> Batch Gradient Descent</h3>
<p>In <strong>Batch Gradient Descent</strong>, the gradient of the loss function is computed using the <em>entire training dataset</em> at every iteration. For the Mean Squared Error (MSE) loss, this means we evaluate</p>
<p><span class="math display">\[\nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta}) 
= \frac{2}{N} \mathbf{X}^T (\mathbf{X}\boldsymbol{\theta} - \mathbf{y})\]</span></p>
<p>using all <span class="math inline">\(N\)</span> training examples before updating the parameters.</p>
<p>The update rule is</p>
<p><span class="math display">\[\boldsymbol{\theta}^{(t+1)} 
= \boldsymbol{\theta}^{(t)} 
- \alpha \nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta})\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is the learning rate. Because each update uses the full dataset, the direction of descent is accurate, leading to <strong>smooth and stable convergence</strong>. The loss surface often appears bowl-shaped for convex problems, and the algorithm steadily moves toward the minimum.</p>
<p>However, the drawback is computational cost. Each step requires processing all <span class="math inline">\(N\)</span> samples, which becomes very expensive when <span class="math inline">\(N\)</span> is large. Therefore, Batch Gradient Descent is best suited for <strong>small to medium-sized datasets</strong> where stability is more important than speed.</p>
<div class="algorithm">
<div class="algorithmic">
<p>Training data <span class="math inline">\(\{(x_i,y_i)\}_{i=1}^N\)</span>, feature map <span class="math inline">\(\phi(\cdot)\)</span>, learning rate <span class="math inline">\(\alpha\)</span>, epochs <span class="math inline">\(E\)</span> Initialize <span class="math inline">\(\boldsymbol{\theta} \leftarrow 0\)</span> (or small random values) <span class="math inline">\(\mathbf{g} \leftarrow \frac{2}{N}\sum_{i=1}^{N}\big(\boldsymbol{\theta}^T\phi(\mathbf{x}_i)-y_i\big)\phi(\mathbf{x}_i)\)</span> <span class="math inline">\(\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \alpha \mathbf{g}\)</span> <span class="math inline">\(\boldsymbol{\theta}\)</span></p>
</div>
</div>
<figure>
<img src="img/lecture8/Convergence path of Batch GD.png" alt="Convergence path of Batch Gradient Descent" /><figcaption aria-hidden="true">Convergence path of Batch Gradient Descent</figcaption>
</figure>
</section>
<section id="stochastic-gradient-descent-sgd" data-number="0.4.6">
<h3 data-number="1.4.6"><span class="header-section-number">1.4.6</span> Stochastic Gradient Descent (SGD)</h3>
<p>In <strong>Stochastic Gradient Descent (SGD)</strong>, the model parameters are updated using <em>only one training example at a time</em>. Instead of computing the gradient over the entire dataset, we approximate it using a single randomly selected sample <span class="math inline">\((x_i, y_i)\)</span>:</p>
<p><span class="math display">\[\nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta})
\approx
2\big(\boldsymbol{\theta}^T \phi(\mathbf{x}_i) - y_i\big)\phi(\mathbf{x}_i)\]</span></p>
<p>The update becomes</p>
<p><span class="math display">\[\boldsymbol{\theta}^{(t+1)} =
\boldsymbol{\theta}^{(t)} - \alpha \nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta}^{(t)})\]</span></p>
<p>Since each update is extremely fast, SGD can process very large datasets efficiently. However, because the gradient is based on only one sample, the updates are noisy. As a result, the convergence path is more erratic and may oscillate around the minimum rather than settling smoothly.</p>
<p>Despite the noise, this randomness can help the algorithm escape shallow local minima and can act as a form of <strong>implicit regularization</strong>, often improving generalization performance. SGD is especially useful when:</p>
<ul>
<li><p>The dataset is very large,</p></li>
<li><p>The loss function is non-convex,</p></li>
<li><p>Fast, online updates are required.</p></li>
</ul>
<div class="algorithm">
<div class="algorithmic">
<p>Training data <span class="math inline">\(\{(x_i,y_i)\}_{i=1}^N\)</span>, feature map <span class="math inline">\(\phi(\cdot)\)</span>, learning rate <span class="math inline">\(\alpha\)</span>, epochs <span class="math inline">\(E\)</span> Initialize <span class="math inline">\(\boldsymbol{\theta} \leftarrow 0\)</span> (or small random values) <span class="math inline">\(\mathbf{g} \leftarrow 2\big(\boldsymbol{\theta}^T\phi(\mathbf{x}_i)-y_i\big)\phi(\mathbf{x}_i)\)</span> <span class="math inline">\(\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \alpha \mathbf{g}\)</span> <span class="math inline">\(\boldsymbol{\theta}\)</span></p>
</div>
</div>
<figure>
<img src="img/lecture8/Convergence path of Stochastic GD.png" alt="Convergence path of Stochastic Gradient Descent" /><figcaption aria-hidden="true">Convergence path of Stochastic Gradient Descent</figcaption>
</figure>
</section>
<section id="mini-batch-gradient-descent" data-number="0.4.7">
<h3 data-number="1.4.7"><span class="header-section-number">1.4.7</span> Mini-batch Gradient Descent</h3>
<p><strong>Mini-batch Gradient Descent</strong> combines ideas from Batch and Stochastic Gradient Descent. Instead of using all <span class="math inline">\(N\)</span> samples or just one sample, we use a small subset (mini-batch) of size <span class="math inline">\(b\)</span>.</p>
<p>Let <span class="math inline">\(\Phi_b\)</span> be the design matrix formed from the mini-batch and <span class="math inline">\(\mathbf{y}_b\)</span> the corresponding targets. The gradient approximation becomes</p>
<p><span class="math display">\[\nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta})
=
\frac{2}{b}\,\Phi_b^T(\Phi_b\boldsymbol{\theta}-\mathbf{y}_b)\]</span></p>
<p>The update rule remains</p>
<p><span class="math display">\[\boldsymbol{\theta}^{(t+1)}
=
\boldsymbol{\theta}^{(t)}-\alpha\nabla_{\boldsymbol{\theta}}L(\boldsymbol{\theta})\]</span></p>
<p>This approach provides a balance:</p>
<ul>
<li><p>More stable convergence than SGD,</p></li>
<li><p>Faster updates than Batch GD,</p></li>
<li><p>Efficient use of vectorized operations and parallel hardware (GPUs).</p></li>
</ul>
<p>Each epoch processes <span class="math inline">\(N/b\)</span> mini-batches, allowing the model to explore the loss surface while maintaining computational efficiency. Mini-batch Gradient Descent is the <strong>standard method used in modern machine learning and deep learning systems</strong>.</p>
<div class="algorithm">
<div class="algorithmic">
<p>Training data <span class="math inline">\(\{(x_i,y_i)\}_{i=1}^N\)</span>, feature map <span class="math inline">\(\phi(\cdot)\)</span>, learning rate <span class="math inline">\(\alpha\)</span>, epochs <span class="math inline">\(E\)</span>, batch size <span class="math inline">\(b\)</span> Initialize <span class="math inline">\(\boldsymbol{\theta} \leftarrow 0\)</span> (or small random values) Shuffle training data Construct <span class="math inline">\(\Phi_b\)</span> and <span class="math inline">\(\mathbf{y}_b\)</span> from <span class="math inline">\(\mathcal{B}\)</span> <span class="math inline">\(\mathbf{g} \leftarrow \frac{2}{b}\,\Phi_b^T(\Phi_b\boldsymbol{\theta}-\mathbf{y}_b)\)</span> <span class="math inline">\(\boldsymbol{\theta} \leftarrow \boldsymbol{\theta}-\alpha\mathbf{g}\)</span> <span class="math inline">\(\boldsymbol{\theta}\)</span></p>
</div>
</div>
<figure>
<img src="img/lecture8/Convergence path of mini-batch GD.png" alt="Convergence path of Mini-batch Gradient Descent" /><figcaption aria-hidden="true">Convergence path of Mini-batch Gradient Descent</figcaption>
</figure>
</section>
<section id="performance-comparisons" data-number="0.4.8">
<h3 data-number="1.4.8"><span class="header-section-number">1.4.8</span> Performance Comparisons</h3>
<p>Let <span class="math inline">\(N\)</span> be the number of training instances and <span class="math inline">\(P\)</span> be the number of features. The performance, memory requirements, and whether normalization is required are compared among methods mentioned previously.</p>
<table>
<caption>Comparison of Gradient Descent Methods</caption>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Algorithm</strong></th>
<th style="text-align: left;"><strong>Performance with Large <span class="math inline">\(N\)</span></strong></th>
<th style="text-align: left;"><strong>Memory Space Requirements</strong></th>
<th style="text-align: left;"><strong>Performance with Large <span class="math inline">\(P\)</span></strong></th>
<th style="text-align: left;"><strong>Normalization Required?</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Normal Equation</td>
<td style="text-align: left;">Fast</td>
<td style="text-align: left;">High</td>
<td style="text-align: left;">Slow</td>
<td style="text-align: left;">No</td>
</tr>
<tr class="even">
<td style="text-align: left;">Batch GD</td>
<td style="text-align: left;">Slow</td>
<td style="text-align: left;">High</td>
<td style="text-align: left;">Fast</td>
<td style="text-align: left;">Yes</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Stochastic GD</td>
<td style="text-align: left;">Fast</td>
<td style="text-align: left;">Minimum</td>
<td style="text-align: left;">Fast</td>
<td style="text-align: left;">Yes</td>
</tr>
<tr class="even">
<td style="text-align: left;">Mini-batch GD</td>
<td style="text-align: left;">Fast</td>
<td style="text-align: left;">Relative to batch size</td>
<td style="text-align: left;">Fast</td>
<td style="text-align: left;">Yes</td>
</tr>
</tbody>
</table>
<p>Stochastic and Mini-batch GD perform fastest with large datasets due to incremental updates, while Normal Equation and Batch GD use more memory. The Normal Equation is slower when <span class="math inline">\(P\)</span> is large because of matrix inversion. Gradient Descent methods require normalization for faster convergence, but the Normal Equation does not.</p>
</section>
<section id="epoch-vs-iteration" data-number="0.4.9">
<h3 data-number="1.4.9"><span class="header-section-number">1.4.9</span> Epoch vs Iteration</h3>
<p>In training algorithms such as Gradient Descent, two commonly used terms are <strong>epoch</strong> and <strong>iteration</strong>. Although they are related, they describe different aspects of the learning process.</p>
<p><strong>Epoch:</strong> An epoch refers to one complete pass through the <em>entire training dataset</em>. During one epoch, every training example has been used once for updating the model (either individually or as part of a batch).</p>
<p><strong>Iteration:</strong> An iteration refers to a <em>single parameter update step</em>. The number of samples used in an iteration depends on the type of Gradient Descent being used.</p>
<p>The relationship between epochs and iterations depends on whether Batch, Stochastic, or Mini-batch Gradient Descent is used.</p>
<table>
<caption>Relationship between Epochs and Iterations for different Gradient Descent methods</caption>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;"><strong>Batch GD</strong></th>
<th style="text-align: left;"><strong>Stochastic GD</strong></th>
<th style="text-align: left;"><strong>Mini-batch GD</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>In one iteration:</strong></td>
<td style="text-align: left;">The gradient is computed using the entire dataset and one update step is performed.</td>
<td style="text-align: left;">One randomly selected training sample is used to compute the gradient and update the parameters.</td>
<td style="text-align: left;">A small subset (mini-batch) of size <span class="math inline">\(b\)</span> is used to compute the gradient and update the parameters.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>An epoch completes:</strong></td>
<td style="text-align: left;">After 1 iteration (since the full dataset is used each time).</td>
<td style="text-align: left;">After <span class="math inline">\(N\)</span> iterations (when all <span class="math inline">\(N\)</span> samples have been used once on average).</td>
<td style="text-align: left;">After <span class="math inline">\(\frac{N}{b}\)</span> iterations (when all mini-batches have been processed).</td>
</tr>
</tbody>
</table>
</section>
<section id="convergence" data-number="0.4.10">
<h3 data-number="1.4.10"><span class="header-section-number">1.4.10</span> Convergence</h3>
<p>Although Batch Gradient Descent (Batch GD), Stochastic Gradient Descent (SGD), and Mini-batch Gradient Descent differ in how they compute updates, they all aim to minimize the same loss function and typically move toward the same region of the parameter space. In practice, all three methods tend to end up <em>near</em> the minimum of the loss surface, meaning they reach a neighborhood where the loss is low and further improvements become small.</p>
<p>However, their trajectories near the minimum look quite different. Batch GD computes the gradient using the <em>entire</em> training set at every step, so each update direction closely matches the true steepest descent direction. As a result, Batch GD usually follows a smooth, stable path and can settle at the minimum (or extremely close to it), with little to no oscillation once it arrives. The trade-off is that each step is computationally expensive: evaluating the full gradient requires processing all <span class="math inline">\(N\)</span> training examples, so Batch GD can be slow when the dataset is large.</p>
<p>In contrast, SGD and Mini-batch GD use <em>approximate</em> gradients computed from one example (SGD) or a small batch (Mini-batch). This makes each update much cheaper and often allows faster progress early in training, especially for large datasets. The downside is that these gradient estimates contain noise, so the updates do not perfectly align with the true steepest descent direction. Consequently, instead of stopping cleanly at the minimum, SGD and Mini-batch GD typically continue to fluctuate or “jitter” around it. Importantly, this does not mean they fail to converge; with an appropriate learning rate schedule (for example, gradually decreasing the learning rate over time), the step sizes shrink, the oscillations become smaller, and both SGD and Mini-batch GD can converge to the minimum or very close to it.</p>
<figure>
<img src="img/lecture9/P1.png" id="fig:P1.png" style="width:40.0%" alt="Visualization of Batch, Stochastic, and Mini-batch Gradient Descent convergence." /><figcaption aria-hidden="true">Visualization of Batch, Stochastic, and Mini-batch Gradient Descent convergence.</figcaption>
</figure>
<p>Convergence in Gradient Descent refers to the point at which further parameter updates produce negligible improvement in the objective function. In theory, one might say training stops when the loss becomes zero. In practice, this almost never occurs, especially in noisy or high-dimensional datasets.</p>
<p>Instead, convergence is defined using a stopping criterion. A common approach is to monitor the magnitude of the gradient. When the gradient becomes sufficiently small,</p>
<p><span class="math display">\[\|\nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta})\| &lt; \epsilon,\]</span></p>
<p>we stop the optimization. The threshold <span class="math inline">\(\epsilon\)</span> is called the <strong>tolerance</strong>. It determines how close we require the solution to be to a stationary point.</p>
<p>In machine learning, finding the exact minimizer of the training loss is not always desirable. Over-optimizing on the training data can reduce generalization performance. Therefore, stopping once the improvement becomes small often leads to better performance on unseen data (early stopping acts as a form of regularization).</p>
<p>The behavior of convergence differs across Gradient Descent variants:</p>
<p>Batch Gradient Descent follows the true gradient of the loss surface and, for convex problems, converges smoothly toward the minimum.</p>
<p>Stochastic Gradient Descent (SGD) and Mini-batch Gradient Descent introduce noise into the gradient estimate. As a result, instead of settling exactly at the minimum, they typically oscillate around it. Learning rate schedules (decaying the learning rate over time) help reduce this oscillation and improve convergence.</p>
<p>The learning rate <span class="math inline">\(\alpha\)</span> plays a critical role in convergence:</p>
<ul>
<li><p>If <span class="math inline">\(\alpha\)</span> is too small, updates are tiny and convergence is very slow.</p></li>
<li><p>If <span class="math inline">\(\alpha\)</span> is too large, the algorithm may overshoot the minimum, causing divergence or oscillation.</p></li>
</ul>
<p>A properly chosen learning rate allows the algorithm to reach a neighborhood of the minimum efficiently.</p>
<figure>
<img src="img/lecture8/Convergence (All).png" alt="Different Gradient Descent methods converge toward the minimum but exhibit different trajectories" /><figcaption aria-hidden="true">Different Gradient Descent methods converge toward the minimum but exhibit different trajectories</figcaption>
</figure>
</section>
<section id="pitfalls" data-number="0.4.11">
<h3 data-number="1.4.11"><span class="header-section-number">1.4.11</span> Pitfalls</h3>
<p>Gradient Descent methods work well in many settings, but several practical challenges can affect convergence and solution quality.</p>
<p>First, the shape of the loss surface plays an important role. For linear regression, the loss is convex and contains a single global minimum. However, in many modern machine learning models (especially neural networks), the loss is non-convex and contains multiple local minima and saddle points. Depending on initialization, the algorithm may converge to different solutions.</p>
<p>Second, optimization dynamics influence behavior. Batch Gradient Descent follows the exact gradient and can become trapped near flat regions or saddle points. Stochastic and Mini-batch Gradient Descent introduce noise into the updates, which can help the algorithm move away from saddle points and explore the loss surface more effectively. However, this noise also makes convergence more erratic.</p>
<p>Third, the learning rate is critical. A very small learning rate leads to extremely slow convergence, while a very large learning rate can cause divergence or oscillations. Choosing an appropriate learning rate (or using adaptive learning rate methods) is essential.</p>
<p>Another common issue is poor feature scaling. If features have very different magnitudes, the loss surface becomes elongated, causing Gradient Descent to zig-zag and converge slowly. Normalization or standardization significantly improves convergence.</p>
<p>Finally, numerical issues can arise when features are highly correlated, making the problem ill-conditioned. This leads to unstable updates and slow progress.</p>
<p>Understanding these pitfalls helps in selecting the right optimization strategy and preprocessing steps for reliable training.<br />
</p>
</section>
<section id="learning-rate-schedulers" data-number="0.4.12">
<h3 data-number="1.4.12"><span class="header-section-number">1.4.12</span> Learning Rate Schedulers</h3>
<p>Previously, we’ve emphasized the critical role of selecting a good learning rate in the success of Gradient Descent. The learning rate controls how large the step is that we take in the direction of the negative gradient. If the learning rate is too small, gradient descent may converge too slowly, while a large learning rate may cause the algorithm to overshoot the minimum or oscillate around it, potentially never converging.</p>
<p>Hence, in practice, using a fixed learning rate may not always be the best approach. The optimal learning rate can vary over the course of the optimization process. Early in training, larger learning rates encourage exploration of the loss surface, while later smaller learning rates allow fine-grained convergence near minima. For this reason we rely on <em>learning rate schedulers</em>, which are strategies for generating dynamic learning rates. The following are some standard learning rate schedulers:</p>
<section id="linear-decay" data-number="0.4.12.0.1">
<h5 data-number="1.4.12.0.1"><span class="header-section-number">1.4.12.0.1</span> 1. Linear Decay:</h5>
<p>In Linear Decay, the learning rate decreases by a fixed amount at each iteration: <span class="math display">\[\alpha(t) = \alpha_0 - \eta \cdot t\]</span> where <span class="math inline">\(\alpha_0\)</span> is the initial learning rate, and <span class="math inline">\(\eta\)</span> is the decay rate (e.g., <span class="math inline">\(0.01\)</span>). The learning rate decreases linearly with each iteration, making it a simple approach for controlling the learning rate. In practice, linear decay is often clipped to a minimum value to prevent the learning rate from becoming negative.</p>
</section>
<section id="step-decay" data-number="0.4.12.0.2">
<h5 data-number="1.4.12.0.2"><span class="header-section-number">1.4.12.0.2</span> 2. Step Decay:</h5>
<p>In Step Decay, the learning rate decreases by a fixed factor at specific intervals, which are usually predefined based on the number of epochs or iterations. The learning rate remains constant for a certain number of iterations, and then suddenly drops: <span class="math display">\[\alpha(t) = \alpha_0 \cdot \gamma^{\left\lfloor \frac{t}{T} \right\rfloor}\]</span> where <span class="math inline">\(\alpha_0\)</span> is the initial learning rate, <span class="math inline">\(\gamma\)</span> is the decay factor (e.g. <span class="math inline">\(0.5\)</span>), and <span class="math inline">\(T\)</span> is the number of iterations between each step (e.g. <span class="math inline">\(10\)</span>). This scheduler is simple to implement and provides sudden reductions in learning rate, which can be helpful for escaping plateaus in the loss landscape.</p>
</section>
<section id="exponential-decay" data-number="0.4.12.0.3">
<h5 data-number="1.4.12.0.3"><span class="header-section-number">1.4.12.0.3</span> 3. Exponential Decay:</h5>
<p>In Exponential Decay, the learning rate decreases at every iteration according to an exponential function: <span class="math display">\[\alpha(t) = \alpha_0 \cdot e^{-\lambda t}\]</span> where <span class="math inline">\(\alpha_0\)</span> is the initial learning rate and <span class="math inline">\(\lambda\)</span> is the decay rate (e.g., <span class="math inline">\(0.01\)</span>), controlling how quickly the learning rate decreases. This scheduler provides a smooth and continuous reduction in the learning rate over time, making it a common choice when steady convergence is preferred.</p>
</section>
<section id="cosine-annealing" data-number="0.4.12.0.4">
<h5 data-number="1.4.12.0.4"><span class="header-section-number">1.4.12.0.4</span> 4. Cosine Annealing:</h5>
<p>In Cosine Annealing, the learning rate follows a cosine curve over time: <span class="math display">\[\alpha(t) = \frac{\alpha_0}{2} \left(1 + \cos\left(\frac{t \pi}{T}\right)\right)\]</span> where <span class="math inline">\(\alpha_0\)</span> is the initial learning rate and <span class="math inline">\(T\)</span> is the total number of iterations. Cosine Annealing gradually reduces the learning rate in a cosine pattern, allowing for larger learning rates early in the optimization and smaller rates as convergence approaches. The cosine schedule can also be extended with restarts to allow for periodic exploration, which can help the optimizer escape shallow local minima, saddle points, or flat plateaus, or be reformulated to allow for a nonzero minimum learning rate.</p>
</section>
</section>
</section>
<section id="qa-section" data-number="0.5">
<h2 data-number="1.5"><span class="header-section-number">1.5</span> Q&amp;A Section</h2>
<ol>
<li><p><strong>Question 1:</strong></p>
<p>Suppose you are training a machine learning model using Gradient Descent (GD) on a dataset with <span class="math inline">\(N = 1{,}000{,}000\)</span> training examples and <span class="math inline">\(P = 100\)</span> features. You consider the following options:</p>
<ul>
<li><p>Batch Gradient Descent</p></li>
<li><p>Stochastic Gradient Descent</p></li>
<li><p>Mini-batch Gradient Descent with a batch size of <span class="math inline">\(b = 10{,}000\)</span></p></li>
</ul>
<p>Assuming you perform one full epoch of training for each method, calculate the number of iterations required for each optimization scheme.</p>
<ol>
<li><p>Batch GD: 1 iteration; Stochastic GD: 1<span>,</span>000<span>,</span>000 iterations; Mini-batch GD: 100 iterations</p></li>
<li><p>Batch GD: 1<span>,</span>000<span>,</span>000 iterations; Stochastic GD: 1 iteration; Mini-batch GD: 100 iterations</p></li>
<li><p>Batch GD: 100 iterations; Stochastic GD: 10 iterations; Mini-batch GD: 1<span>,</span>000 iterations</p></li>
<li><p>Batch GD: 1 iteration; Stochastic GD: 100<span>,</span>000 iterations; Mini-batch GD: 10 iterations</p></li>
</ol>
<p><strong>Answer:</strong> The correct answer is <strong>A)</strong>.</p>
<p><strong>Solution:</strong></p>
<p>In <strong>Batch GD</strong>, one iteration computes the gradient using the <em>entire</em> dataset. Therefore, <strong>one epoch</strong> corresponds to <strong>1 iteration</strong>.</p>
<p>In <strong>Stochastic GD</strong>, one iteration uses <em>one</em> training example. Therefore, <strong>one epoch</strong> requires <span class="math display">\[N = 1{,}000{,}000 \text{ iterations.}\]</span></p>
<p>In <strong>Mini-batch GD</strong> with batch size <span class="math inline">\(b = 10{,}000\)</span>, one iteration processes one batch. The number of batches per epoch is <span class="math display">\[\frac{N}{b} \;=\; \frac{1{,}000{,}000}{10{,}000} \;=\; 100,\]</span> so <strong>one epoch</strong> requires <strong>100 iterations</strong>.</p></li>
<li><p><strong>Question 2:</strong></p>
<p>Suppose you have a limited memory capacity that allows you to process a maximum of 50<span>,</span>000 data points at a time. Which optimization methods can you use without exceeding the memory limit?</p>
<ol>
<li><p>Only Stochastic GD</p></li>
<li><p>Stochastic GD and Mini-batch GD with <span class="math inline">\(b \leq 50{,}000\)</span></p></li>
<li><p>Batch GD and Mini-batch GD with <span class="math inline">\(b = 50{,}000\)</span></p></li>
<li><p>All methods can be used without exceeding the memory limit</p></li>
</ol>
<p><strong>Answer:</strong> The correct answer is <strong>B)</strong>.</p>
<p><strong>Solution:</strong></p>
<p><strong>Batch GD</strong> processes the entire dataset per iteration. For <span class="math inline">\(N=1{,}000{,}000\)</span>, this exceeds the memory limit of 50<span>,</span>000 samples.</p>
<p><strong>Stochastic GD</strong> processes one sample per iteration, which is always within the memory limit.</p>
<p><strong>Mini-batch GD</strong> processes <span class="math inline">\(b\)</span> samples per iteration, so it is feasible as long as <span class="math inline">\(b \le 50{,}000\)</span>.</p></li>
<li><p><strong>Question 3:</strong></p>
<p>If you wish to perform 10 full passes over the data using Mini-batch GD with <span class="math inline">\(b = 20{,}000\)</span>, how many iterations will this require?</p>
<ol>
<li><p>50 iterations</p></li>
<li><p>5<span>,</span>000 iterations</p></li>
<li><p>1<span>,</span>000 iterations</p></li>
<li><p>500 iterations</p></li>
</ol>
<p><strong>Answer:</strong> The correct answer is <strong>D)</strong>.</p>
<p><strong>Solution:</strong></p>
<p>Batches per epoch: <span class="math display">\[\frac{N}{b} \;=\; \frac{1{,}000{,}000}{20{,}000} \;=\; 50.\]</span> For <span class="math inline">\(10\)</span> epochs, total iterations: <span class="math display">\[50 \times 10 \;=\; 500 \text{ iterations.}\]</span></p></li>
<li><p><strong>Question 4:</strong></p>
<p>Consider the following contour plot for MSE loss of a 2D linear model, as well as the contour plot for the <span class="math inline">\(L_1\)</span>-norm of the parameters.</p>
<div class="center">
<p><img src="img/lecture8/qa-mse-contour.png" alt="image" /></p>
</div>
<p>Which of the following contour plots most likely represents the contour plot for the <span class="math inline">\(L_1\)</span>-regularized MSE loss on the same model?</p>
<div class="center">
<p><img src="img/lecture8/qa-mse-l1-options.png" alt="image" /></p>
</div>
<p><strong>Answer:</strong> The correct answer is <strong>A)</strong>.</p>
<p><strong>Solution:</strong></p>
<p>The <span class="math inline">\(L_1\)</span>-regularized objective is <span class="math display">\[J(\theta) = \mathrm{MSE}(\theta) + \lambda \|\theta\|_1 .\]</span></p>
<p>This objective combines two different contour geometries:</p>
<ul>
<li><p>The MSE loss produces <em>smooth elliptical contours</em>.</p></li>
<li><p>The <span class="math inline">\(L_1\)</span> norm produces <em>diamond-shaped contours with sharp corners on the coordinate axes</em>.</p></li>
</ul>
<p>When the <span class="math inline">\(L_1\)</span> penalty is added to the MSE loss, the resulting contours become a <em>distorted ellipse</em> that inherits the <em>axis-aligned corners (kinks)</em> from the <span class="math inline">\(L_1\)</span> norm. These corners reflect the sparsity-inducing behavior of <span class="math inline">\(L_1\)</span> regularization, which encourages parameters to become exactly zero.</p>
<p>Options (c) and (d) resemble the contours of the individual terms (MSE alone or <span class="math inline">\(L_1\)</span> alone), so they cannot represent the combined objective.</p>
<p>Among the remaining choices, option (b) still appears too smooth and elliptical, similar to the unregularized MSE loss. Option (a) shows the expected <em>elliptical shape with visible axis-aligned kinks</em>, which is characteristic of an <span class="math inline">\(L_1\)</span>-regularized objective.</p>
<p>Therefore, option (a) is the correct contour plot.</p></li>
<li><p><strong>Question 5: Linear vs Polynomial Regression (Numerical + Visual)</strong></p>
<p>We generate synthetic data from a nonlinear function and compare how well a linear model and a polynomial model fit the data using Mean Squared Error (MSE).</p>
<p>We sample data from the ground-truth function <span class="math display">\[y = \sin(2\pi x) + \epsilon, \qquad \epsilon \sim \mathcal{N}(0,\,0.1^2).\]</span></p>
<p>We fit two models:</p>
<ul>
<li><p>Linear regression (degree 1)</p></li>
<li><p>Polynomial regression (degree 5)</p></li>
</ul>
<p>Which model should achieve the lower training MSE? Why?</p>
<p><strong>Answer:</strong> Polynomial regression (degree 5).</p>
<p><strong>Solution:</strong></p>
<p><strong>Step 1: Generate synthetic dataset</strong></p>
<p>We sample training inputs and noisy targets: <span class="math display">\[x_i \sim \text{Uniform}(0,1), \qquad 
y_i = \sin(2\pi x_i) + \epsilon_i.\]</span></p>
<p>This produces a clearly <strong>nonlinear</strong> dataset.</p>
<figure>
<img src="img/lecture8/qa_q5_linear_vs_poly_fit.png" alt="Linear vs Polynomial Regression fit on nonlinear data" /><figcaption aria-hidden="true">Linear vs Polynomial Regression fit on nonlinear data</figcaption>
</figure>
<p><strong>Step 2: Fit linear regression</strong></p>
<p>Design matrix: <span class="math display">\[\mathbf{X}_{\text{lin}} =
\begin{bmatrix}
1 &amp; x_1\\
\vdots &amp; \vdots\\
1 &amp; x_N
\end{bmatrix}\]</span></p>
<p>Model: <span class="math display">\[\hat{\mathbf{y}} = \mathbf{X}_{\text{lin}}\boldsymbol{\theta}_{\text{lin}}.\]</span></p>
<p>Training MSE: <span class="math display">\[\mathrm{MSE}_{\text{linear}}
= \frac{1}{N}\|\mathbf{y} - \hat{\mathbf{y}}\|_2^2.\]</span></p>
<p>Because the true relationship is nonlinear, the linear model has <strong>high bias</strong> and cannot capture the curvature of the data.</p>
<p><strong>Step 3: Fit polynomial regression (degree 5)</strong></p>
<p>Polynomial feature map: <span class="math display">\[\boldsymbol{\phi}(x) = [1, x, x^2, x^3, x^4, x^5]^T.\]</span></p>
<p>Design matrix: <span class="math display">\[\mathbf{X}_{\text{poly}} =
\begin{bmatrix}
\boldsymbol{\phi}(x_1)^T\\
\vdots\\
\boldsymbol{\phi}(x_N)^T
\end{bmatrix}.\]</span></p>
<p>Model: <span class="math display">\[\hat{\mathbf{y}} = \mathbf{X}_{\text{poly}}\boldsymbol{\theta}_{\text{poly}}.\]</span></p>
<p>This model has greater flexibility and can approximate the sinusoidal shape of the data.</p>
<p><strong>Step 4: Numerical comparison (reproducible simulation)</strong></p>
<p>For the generated dataset (<span class="math inline">\(N=50\)</span>):</p>
<p><span class="math display">\[\mathrm{MSE}_{\text{linear}} = 0.1691\]</span> <span class="math display">\[\mathrm{MSE}_{\text{poly}} = 0.0052\]</span></p>
<figure>
<img src="img/lecture8/qa_q5_mse_comparison.png" alt="Training MSE comparison" /><figcaption aria-hidden="true">Training MSE comparison</figcaption>
</figure>
<p>Polynomial regression achieves a dramatically lower training error.</p>
<p><strong>Key Insight</strong></p>
<ul>
<li><p>Linear regression <strong>underfits</strong> nonlinear data (high bias).</p></li>
<li><p>Polynomial regression reduces bias and captures curvature.</p></li>
<li><p>Increasing model complexity can significantly reduce training MSE.</p></li>
</ul></li>
</ol>
</section>
</body>
</html>

</main>
</body>
</html>
