<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>lec17</title>
  <link rel="stylesheet" href="../assets/style.css"/>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav>
  <a href="../index.html">Home</a>
</nav>
<main>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>_Lecture17</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div class="center">

</div>
<p><strong>Contributors:</strong> Dr. Ahmad Mustafa, Dr. Motaz Alfarraj, Dr. Ashraf Alattar, Dr. Chen Zhou</p>
<p><strong>Teaching Assistants</strong> with remarkable contributions include: Kuo-Wei Lai, Wuyang Du, Shiva Mahato, Michael Zhou, Ninghan Zhong</p>
<p><strong>Disclaimer</strong>: <span>All content of these notes are part of this course at Georgia Tech. Any re-use or distribution is not permitted without pre-approved permission. All these notes belong to, created by, and copyrighted for Ghassan AlRegib and Mohit Prabhushankar, Georgia Tech, 2021–2028.</span></p>
<p><strong>License</strong>: <span>These lecture notes are licensed under the <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.</span></p>
<p><strong>Errata</strong>: <span><em>Please submit any errata you find using the following form: <a href="https://forms.office.com/r/fbg9dMWPgY">Errata Form for FunML Textbook</a> or visit: <a href="https://forms.office.com/r/fbg9dMWPgY">https://forms.office.com/r/fbg9dMWPgY</a></em></span></p>
<section id="lecture-objectives" data-number="0.1">
<h2 data-number="1.1"><span class="header-section-number">1.1</span> Lecture Objectives</h2>
<p>This lecture covers essential best practices in machine learning model design, evaluation, and debugging. Topics include selecting the appropriate model architecture, handling class imbalances, performance metrics, data augmentation techniques, hyperparameter tuning, and the importance of visualizations and diagnostics.</p>
</section>
<section id="previous-lecture" data-number="0.2">
<h2 data-number="1.2"><span class="header-section-number">1.2</span> Previous Lecture</h2>
<p>This section reviews essential concepts from previous lecture, which covered training CNN’s.</p>
<section id="optimization-of-parameters" data-number="0.2.1">
<h3 data-number="1.2.1"><span class="header-section-number">1.2.1</span> Optimization of Parameters</h3>
<ul>
<li><p>Weights and biases in CNN’s are learned through back-propagation and gradient updates using a loss function.</p></li>
<li><p>Optimization challenges include:</p>
<ul>
<li><p>Noisy gradients</p></li>
<li><p>Saddle points</p></li>
<li><p>Non-convex loss surfaces</p></li>
</ul></li>
</ul>
</section>
<section id="gradient-descent-variants" data-number="0.2.2">
<h3 data-number="1.2.2"><span class="header-section-number">1.2.2</span> Gradient Descent Variants</h3>
<ul>
<li><p><strong>Batch Gradient Descent</strong>: Calculates gradients over the entire dataset for stable convergence.</p></li>
<li><p><strong>Stochastic Gradient Descent (SGD)</strong>: Uses single samples per iteration, allowing quicker but noisier convergence.</p></li>
<li><p><strong>Mini-batch Gradient Descent</strong>: Combines aspects of both for a balance between computational efficiency and convergence stability.</p></li>
</ul>
</section>
<section id="advanced-optimization-techniques" data-number="0.2.3">
<h3 data-number="1.2.3"><span class="header-section-number">1.2.3</span> Advanced Optimization Techniques</h3>
<ul>
<li><p><strong>Momentum</strong>: Helps in bypassing saddle points by incorporating past gradients.</p></li>
<li><p><strong>AdaGrad, RMSProp, and Adam</strong>: Introduce adaptive learning rates, addressing issues in high-dimensional optimization landscapes.</p>
<ul>
<li><p><strong>Adam</strong> combines RMSProp and momentum, offering the benefits of both approaches.</p></li>
</ul></li>
</ul>
</section>
<section id="higher-order-methods" data-number="0.2.4">
<h3 data-number="1.2.4"><span class="header-section-number">1.2.4</span> Higher-order Methods</h3>
<ul>
<li><p><strong>Newton’s Method</strong> and <strong>Quasi-Newton Methods</strong> (BFGS, L-BFGS) offer faster convergence.</p></li>
<li><p>These methods require significant memory and computation, making them ideal for convex optimization problems.</p></li>
</ul>
</section>
</section>
<section id="model-architecture-selection" data-number="0.3">
<h2 data-number="1.3"><span class="header-section-number">1.3</span> Model Architecture Selection</h2>
<p>Choosing the right model architecture depends on the nature of the task. This lecture distinguishes between two main types of CNN architectures: those suited for classification and those for dense prediction tasks. Classification architectures produce a single prediction label for the entire image, while dense prediction architecture produces a label for each pixel in the image. Once the best architecture type is determined, it is generally accepted to use pre-trained models available online instead of starting from scratch. <a href="https://pytorch.org/vision/stable/models.html">PyTorch</a> has a plethora of pre-trained modules for most applications.</p>
<section id="classification-architectures" data-number="0.3.1">
<h3 data-number="1.3.1"><span class="header-section-number">1.3.1</span> Classification Architectures</h3>
<ul>
<li><p><strong>Pipeline</strong>: Classification CNNs use 2D kernels stacked across layers to progressively reduce the spatial size of activations.</p></li>
<li><p><strong>Vectorization and Fully Connected Layers</strong>: The final convolutional layer’s output is flattened and passed through fully connected layers, finishing with a softmax layer that outputs class probabilities.</p></li>
<li><p><strong>Tuning Hyperparameters</strong>:</p>
<ul>
<li><p><strong>Kernels</strong>: Increase kernel count in deeper layers.</p></li>
<li><p><strong>Pooling</strong>: Experiment with pooling techniques like max and average pooling.</p></li>
<li><p><strong>Activation Functions</strong>: Use ReLU, sigmoid, or variants like leaky ReLU.</p></li>
<li><p><strong>Regularization Techniques</strong>: Employ dropout and batch normalization to prevent overfitting.</p></li>
<li><p><strong>Skip Connections</strong>: Implement connections as in ResNet, especially useful for deeper networks.</p></li>
</ul></li>
</ul>
<figure>
<img src="img/lecture17/scribeim1.png" id="0" alt="Classification Pipeline" /><figcaption aria-hidden="true">Classification Pipeline</figcaption>
</figure>
</section>
<section id="dense-prediction-architectures" data-number="0.3.2">
<h3 data-number="1.3.2"><span class="header-section-number">1.3.2</span> Dense Prediction Architectures</h3>
<p>For tasks requiring dense outputs, like image segmentation and super-resolution, fully convolutional architectures are better suited.</p>
<ul>
<li><p><strong>Encoder-Decoder Structure</strong>:</p>
<ul>
<li><p><strong>Encoder</strong>: Similar to classification CNNs but without fully connected layers.</p></li>
<li><p><strong>Decoder</strong>: Upsamples the encoder’s output to the desired resolution using transposed convolution layers.</p></li>
</ul></li>
<li><p><strong>Avoid Pooling Layers</strong>: Pooling reduces resolution and quality; prefer convolutional layers with stride 1 or dilated convolutions.</p></li>
</ul>
<figure>
<img src="img/lecture17/scribeim2.png" id="1" alt="Dense prediction architecture using mainly convolution" /><figcaption aria-hidden="true">Dense prediction architecture using mainly convolution</figcaption>
</figure>
</section>
</section>
<section id="loss-functions" data-number="0.4">
<h2 data-number="1.4"><span class="header-section-number">1.4</span> Loss Functions</h2>
<p>Choosing an appropriate loss function is essential for effective model training.</p>
<ul>
<li><p><strong>Classification Tasks</strong>: Standard cross-entropy loss calculates the discrepancy between predicted class probabilities and true labels.</p></li>
<li><p><strong>Dense Prediction Tasks</strong>: Pixel-wise cross-entropy loss or dice loss for segmentation.</p></li>
<li><p><strong>Image Enhancement</strong>: MSE and L1 loss are commonly used for super-resolution and denoising tasks.</p></li>
</ul>
<section id="cross-entropy-loss" data-number="0.4.1">
<h3 data-number="1.4.1"><span class="header-section-number">1.4.1</span> Cross-Entropy Loss</h3>
<p>Standard loss function. Best used in cases where class imbalance is limited.</p>
<p><span class="math display">\[L = \sum_{i}  y_i \log(\hat{y}_i)\]</span></p>
<ul>
<li><p>i is the class number</p></li>
<li><p><span class="math inline">\({y}_i\)</span> is the ground truth label for the correct class</p></li>
<li><p><span class="math inline">\(\hat{y}_i\)</span> is the soft max pseudo probability predicted by network.</p></li>
</ul>
</section>
<section id="mean-square-error-mse-loss" data-number="0.4.2">
<h3 data-number="1.4.2"><span class="header-section-number">1.4.2</span> Mean Square Error (MSE) Loss</h3>
<p>Mean Square Error (MSE) loss is commonly used for regression tasks, penalizing the average squared difference between predicted and actual values. It’s particularly useful for tasks where we want to minimize large deviations as squaring the error emphasizes larger deviations, makes MSE more sensitive to large errors.</p>
<p><span class="math display">\[L = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2\]</span></p>
<ul>
<li><p><span class="math inline">\(N\)</span> is the number of samples in the dataset.</p></li>
<li><p><span class="math inline">\(y_i\)</span> is the actual (ground truth) value for sample <span class="math inline">\(i\)</span>.</p></li>
<li><p><span class="math inline">\(\hat{y}_i\)</span> is the predicted value for sample <span class="math inline">\(i\)</span>.</p></li>
</ul>
</section>
<section id="l1-loss" data-number="0.4.3">
<h3 data-number="1.4.3"><span class="header-section-number">1.4.3</span> L1 Loss</h3>
<p>L1 Loss, also known as Mean Absolute Error (MAE), calculates the absolute difference between predicted and actual values. This loss function is less sensitive to outliers than MSE, as it doesn’t square the errors.</p>
<p><span class="math display">\[L = \frac{1}{N} \sum_{i=1}^{N} |y_i - \hat{y}_i|\]</span></p>
<ul>
<li><p><span class="math inline">\(N\)</span> is the number of samples in the dataset.</p></li>
<li><p><span class="math inline">\(y_i\)</span> is the actual (ground truth) value for sample <span class="math inline">\(i\)</span>.</p></li>
<li><p><span class="math inline">\(\hat{y}_i\)</span> is the predicted value for sample <span class="math inline">\(i\)</span>.</p></li>
</ul>
</section>
</section>
<section id="handling-class-imbalance" data-number="0.5">
<h2 data-number="1.5"><span class="header-section-number">1.5</span> Handling Class Imbalance</h2>
<p>Class imbalance, where certain classes appear more frequently than others. This posses a problem because without taking special precautions the model will prioritize predicting the majority class accurately at the expense of handling the minority class less accurately. An example where class imbalance can occur is in diagnosing illnesses. Although a majority of tests for illnesses will be negative. It is very important to accurately detect when someone should test positive for the illness.</p>
<section id="weighted-cross-entropy" data-number="0.5.1">
<h3 data-number="1.5.1"><span class="header-section-number">1.5.1</span> Weighted Cross Entropy</h3>
<p>Weighted Cross Entropy loss makes up for inadquecies in the data set by giving more weight to less frequent classes. <span class="math display">\[L = \sum_i w_i \times y_i \log \hat{y}_i\]</span></p>
<ul>
<li><p>i is the class number</p></li>
<li><p><span class="math inline">\({y}_i\)</span> is the ground truth label for the correct class</p></li>
<li><p><span class="math inline">\(\hat{y}_i\)</span> is the soft max pseudo probability predicted by network</p></li>
<li><p><span class="math inline">\(w_i\)</span> is the weight assigned to class i. In general more weight is assigned to infrequent classes.</p>
<ul>
<li><p>A simple rule of thumb for weight assignment is <span class="math inline">\(w_i = \frac{1}{f_i}\)</span></p></li>
</ul></li>
</ul>
</section>
<section id="focal-loss" data-number="0.5.2">
<h3 data-number="1.5.2"><span class="header-section-number">1.5.2</span> Focal Loss</h3>
<p>Emphasizes harder examples, reducing the loss contribution of well-classified samples. <span class="math display">\[L = -\sum_{i} (1 - p_t)^{\gamma} \log(p_t)\]</span></p>
<ul>
<li><p><span class="math inline">\(p_t\)</span> is defined as <span class="math display">\[p_t = 
    \begin{cases} 
    \hat{p} &amp; \text{if the true label } y = 1, \\ 
    1 - \hat{p} &amp; \text{if the true label } y = 0.
    \end{cases}\]</span></p>
<ul>
<li><p>where <span class="math inline">\(\hat{p}\)</span> is the probability assigned to the truth class</p></li>
</ul></li>
<li><p><span class="math inline">\(\gamma\)</span> is a hyperparameter that must be greater than 0, typically set to values between 1 and 3. The larger <span class="math inline">\(\gamma\)</span> the more emphasis is put on harder examples.</p></li>
</ul>
<figure>
<img src="img/lecture17/scribeim3.png" id="2" alt="Effect of \gamma on focal loss" /><figcaption aria-hidden="true">Effect of <span class="math inline">\(\gamma\)</span> on focal loss</figcaption>
</figure>
</section>
</section>
<section id="performance-metrics" data-number="0.6">
<h2 data-number="1.6"><span class="header-section-number">1.6</span> Performance Metrics</h2>
<p>Performance metrics are used to evaluate the effectiveness of a trained model. Choosing the correct performance metric is crucial. Accuracy is not always the most accurate descriptor due to things like class imbalance. Performance metrics can also be used to tune hyperparameters by comparing performance for various hyperparameter values and choosing the best one.</p>
<section id="precision-recall-and-f1-score" data-number="0.6.1">
<h3 data-number="1.6.1"><span class="header-section-number">1.6.1</span> Precision, Recall, and F1-Score</h3>
<ul>
<li><p><strong>Precision</strong>: Measures how many of the predicted positives are actually positive.</p></li>
<li><p><strong>Recall</strong>: Measures how many of the actual positives where predicted as positive.</p></li>
<li><p><strong>F1-Score</strong>: The harmonic mean of precision and recall, useful when class distribution is imbalanced. Ranges from 0-1 where a value of one is the goal. <span class="math display">\[F = \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}\]</span></p></li>
<li><p>Where <span class="math display">\[\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}\]</span></p>
<p><span class="math display">\[\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}\]</span></p></li>
</ul>
</section>
<section id="structural-similarity-index-ssim" data-number="0.6.2">
<h3 data-number="1.6.2"><span class="header-section-number">1.6.2</span> Structural Similarity Index (SSIM)</h3>
<p>SSIM is preferred for dense prediction tasks such as denoising and super-resolution, as it accounts for spatial structure. <span class="math display">\[\text{SSIM}(X, Y) = \frac{(2\mu_X\mu_Y + C_1)(2\sigma_{XY} + C_2)}{(\mu_X^2 + \mu_Y^2 + C_1)(\sigma_X^2 + \sigma_Y^2 + C_2)}\]</span></p>
<ul>
<li><p><span class="math inline">\(\mu_x\)</span> is the pixel sample mean of x;</p></li>
<li><p><span class="math inline">\(\mu_y\)</span> the pixel sample mean of y;</p></li>
<li><p><span class="math inline">\(\sigma_x^2\)</span> the variance of x;</p></li>
<li><p><span class="math inline">\(\sigma_y^2\)</span> the variance of y;</p></li>
<li><p><span class="math inline">\(\sigma_xy\)</span> the covariance of x and y;</p></li>
<li><p><span class="math inline">\(C_1=(k_1*L)^2\)</span>, <span class="math inline">\(C_2=(k_2*L)^2\)</span> two variables to stabilize the division with weak denominator;</p>
<ul>
<li><p>L the dynamic range of the pixel-values (typically this is <span class="math inline">\(2^b - 1\)</span>;</p></li>
</ul>
<ul>
<li><p><span class="math inline">\(k_1\)</span>=0.01 and <span class="math inline">\(k_2\)</span>=0.03 by default.</p></li>
</ul></li>
</ul>
<p>The formula above is made from combing three formulas for luminance, contrast, and structure <span class="math display">\[l(x, y) = \frac{2 \mu_x \mu_y + C_1}{\mu_x^2 + \mu_y^2 + C_1}\]</span></p>
<p><span class="math display">\[c(x, y) = \frac{2 \sigma_x \sigma_y + C_2}{\sigma_x^2 + \sigma_y^2 + C_2}\]</span></p>
<p><span class="math display">\[s(x, y) = \frac{\sigma_{xy} + C_3}{\sigma_x \sigma_y + C_3}\]</span> where <span class="math inline">\(C_3= \frac{C_2}{2}\)</span> and <span class="math display">\[\text{SSIM}(X, Y) = \text{l}(x,y)^\alpha * \text{c}(x,y)^\beta *\text{s}(x,y)^\gamma\]</span> where <span class="math inline">\(\alpha, \beta, \gamma\)</span> are assumed to be 1.</p>
</section>
<section id="complex-wavelet-ssim-cw-ssim" data-number="0.6.3">
<h3 data-number="1.6.3"><span class="header-section-number">1.6.3</span> Complex Wavelet SSIM (CW-SSIM)</h3>
<p>The CW-SSIM is similar to the SSIM, except unlike the SSIM it is robost to transformations like scaling, translation, and rotation. The output is some value 0-1 where 1 would mean the 2 signals are perfectly structurally similar while 0 would indicate no structual similairty. <span class="math display">\[\text{CW-SSIM}(c_x, c_y) = \frac{2 \sum_{i=1}^N |c_{x,i}| |c_{y,i}| + K}{\sum_{i=1}^N |c_{x,i}|^2 + \sum_{i=1}^N |c_{y,i}|^2 + K} \cdot \frac{ 2 \left| \sum_{i=1}^N c_{x,i} c_{y,i}^* \right|+ K }{2 \sum_{i=1}^N |c_{x,i} c_{y,i}^*| + K}\]</span></p>
<ul>
<li><p><span class="math inline">\(c_{x,i}\)</span> and <span class="math inline">\(c_{y,i}\)</span>: Complex wavelet coefficients for images <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> at the <span class="math inline">\(i\)</span>-th location.</p></li>
<li><p><span class="math inline">\(K\)</span>: Small constant to avoid division by zero.</p></li>
</ul>
</section>
<section id="analyzing-image-reconstructions-with-various-performance-metrics" data-number="0.6.4">
<h3 data-number="1.6.4"><span class="header-section-number">1.6.4</span> Analyzing Image Reconstructions with Various Performance Metrics</h3>
<figure>
<img src="img/lecture17/scribeim4.png" id="3" alt="Image Reconstructions and their Performance Scores" /><figcaption aria-hidden="true">Image Reconstructions and their Performance Scores</figcaption>
</figure>
<p>The first row of reconstructions a-d are all valid reconstructions with image (a) being notable for being the perfect reconstruction of the original image and image (c) having a perfect CW-SSIM score meaning it is structurally the same as the original. images (e-g) show on flaw with MSE due to the images being relativly flawed but still having the same MSE scores as (b-d). Images (h-l) show flaws with MSE and SSIM because they are both visually very good recreations of the original image but have weak MSE and SSIM scores while the CW-SSIM score is high.</p>
</section>
</section>
<section id="model-debugging" data-number="0.7">
<h2 data-number="1.7"><span class="header-section-number">1.7</span> Model Debugging</h2>
<p>Effective model debugging ensures robust performance on training and testing datasets.</p>
<section id="low-model-capacity" data-number="0.7.1">
<h3 data-number="1.7.1"><span class="header-section-number">1.7.1</span> Low Model Capacity</h3>
<p>If a model underfits (low accuracy on both training and test sets):</p>
<ul>
<li><p><strong>Solution</strong>: Increase the model’s capacity by adding layers or units, or by tuning hyperparameters.</p></li>
</ul>
</section>
<section id="high-model-capacity" data-number="0.7.2">
<h3 data-number="1.7.2"><span class="header-section-number">1.7.2</span> High Model Capacity</h3>
<p>Overfitting indicates high training accuracy but low test accuracy.</p>
<ul>
<li><p><strong>Solution</strong>: Decrease model capacity by reducing layers/units or adding regularization techniques.</p></li>
</ul>
<figure>
<img src="img/lecture17/scribeim5.png" id="4" alt="Under Fitting, Proper Fitting, Over Fitting" /><figcaption aria-hidden="true">Under Fitting, Proper Fitting, Over Fitting</figcaption>
</figure>
</section>
</section>
<section id="data-augmentation" data-number="0.8">
<h2 data-number="1.8"><span class="header-section-number">1.8</span> Data Augmentation</h2>
<p>Augmenting data helps improve performance on limited datasets. Using data augmentation you are able to make your model more robust without having to gather more data.</p>
<ul>
<li><p><strong>Techniques</strong>:</p>
<ul>
<li><p><strong>Transformations</strong>: Random cropping, horizontal/vertical flipping, rotation, scaling, and Gaussian noise addition.</p></li>
<li><figure>
<img src="img/lecture17/scriveim6.png" id="fig:5" alt="Examples of Transformations Applied to an Image" /><figcaption aria-hidden="true">Examples of Transformations Applied to an Image</figcaption>
</figure></li>
</ul></li>
</ul>
</section>
<section id="transfer-learning" data-number="0.9">
<h2 data-number="1.9"><span class="header-section-number">1.9</span> Transfer Learning</h2>
<p>Transfer learning leverages models pre-trained on large datasets, which is useful when training data is limited. Usefulness of transfer learning is limited by finding a dataset that is closely related to your task.</p>
<ul>
<li><p><strong>Frozen Layers</strong>: Use pretrained weights and finetune only the final layers on the target dataset.</p></li>
</ul>
</section>
<section id="visualization-and-diagnostics" data-number="0.10">
<h2 data-number="1.10"><span class="header-section-number">1.10</span> Visualization and Diagnostics</h2>
<p>Visualizing activations, gradients, and network outputs aids in identifying model performance bottlenecks.</p>
<section id="gradient-checking" data-number="0.10.1">
<h3 data-number="1.10.1"><span class="header-section-number">1.10.1</span> Gradient Checking</h3>
<ul>
<li><p><strong>Method</strong>: Compare backpropagated gradients with numerically computed gradients to identify errors in implementation. <span class="math display">\[f&#39;(x) = \frac{f(x + \epsilon) - f(x)}{\epsilon}\]</span></p></li>
</ul>
</section>
<section id="activation-histograms" data-number="0.10.2">
<h3 data-number="1.10.2"><span class="header-section-number">1.10.2</span> Activation Histograms</h3>
<p>Activation histograms are an essential tool for understanding the behavior of neurons in a convolutional neural network (CNN). By examining the distribution of activation values across different layers, practitioners can gain insights into issues such as dead neurons, gradient vanishing, and gradient exploding. Activation histograms are generated by plotting the distribution of neuron activations for each layer. This is typically done by capturing the output values of each neuron in a layer and plotting them as a histogram, showing the range and frequency of activations.</p>
<ul>
<li><p><strong>Purpose</strong>: Activation histograms help identify issues in neural network layers:</p>
<ul>
<li><p><strong>Dead Neurons</strong>: Neurons that consistently output zero or near-zero values, which means they do not contribute to learning. This is common in networks with ReLU (Rectified Linear Unit) activation functions, where a large negative input can cause the neuron to output zero and stay inactive.</p></li>
</ul>
<figure>
<img src="img/lecture17/scribeim6.png" id="fig:6" alt="Activation Histogram with Dead Neurons and Vanishing Gradients" /><figcaption aria-hidden="true">Activation Histogram with Dead Neurons and Vanishing Gradients</figcaption>
</figure></li>
<li><p><strong>Gradient Vanishing/Exploding</strong>: By examining the range of activation values, one can detect if activations are either too small (indicating potential vanishing gradients) or too large (suggesting exploding gradients). If with each layer the slope of the histogram is increasing it is an exploding gradient, or if the slope is decreasing it is a vansihing histogram.This information can guide adjustments to network architecture, activation functions, or initialization methods. If you are dealing with an exploding histogram consider decreasing the learning or increasing the learning late for a vanishing histogram.</p></li>
</ul>
<figure>
<img src="img/lecture17/scribeim8.png" id="fig:7" alt="Activation Histogram With Exploding Gradients" /><figcaption aria-hidden="true">Activation Histogram With Exploding Gradients</figcaption>
</figure>
<ul>
<li><p><strong>Distribution Shape</strong>: A balanced distribution with values spread around zero often indicates healthy neuron activity. A spike around zero, resembling a narrow or normal distribution, can suggest many neurons are inactive or "dead." Maintaining the size of parameter updates with the magnitudes of the parameters themselves during training. Maintaining a ratio where updates are about 1% of the parameter magnitudes can contribute to stable and effective training of neural networks.</p>
<figure>
<img src="img/lecture17/scribeim9.png" id="fig:8" alt="Ideal Activation Histogram" /><figcaption aria-hidden="true">Ideal Activation Histogram</figcaption>
</figure></li>
<li><p><strong>Outliers</strong>: Extreme values (high or low) may indicate that some neurons are overactive, which can lead to gradient exploding issues in deep layers. If outliers are present in the activation histogram, normalization techniques like batch or layer normalization can help.</p>
<figure>
<img src="img/lecture17/scribeim7.png" id="fig:9" alt="Activation Histogram with Outliers" /><figcaption aria-hidden="true">Activation Histogram with Outliers</figcaption>
</figure></li>
</ul>
<ul>
<li><p><strong>Practical Usage</strong>:</p>
<ul>
<li><p>Regularly examine activation histograms during training to diagnose network health. For instance, observing dead neurons early on can prompt changes to initialization methods or hyperparameters.</p></li>
<li><p>Use the histogram data to guide hyperparameter tuning. If many neurons are dead, consider modifying the activation function (e.g., switching from ReLU to Leaky ReLU) or adjusting the learning rate.</p></li>
<li><p>Histograms can also reveal if batch normalization or layer normalization is necessary to maintain healthy activations.</p>
<ul>
<li><p>Batch normalization normalizes the inputs to each layer for each mini-batch, ensuring that the activations maintain a mean of zero and a variance of one. This stabilization can prevent issues like vanishing or exploding gradients and help to mitigate internal covariate shifts during training.</p></li>
<li><p>Unlike batch normalization, which normalizes across the batch dimension, layer normalization normalizes across the features for each individual sample. This is particularly useful for recurrent neural networks or other architectures where batch sizes may be small or variable.</p></li>
</ul></li>
</ul></li>
</ul>
</section>
</section>
<section id="weight-initialization" data-number="0.11">
<h2 data-number="1.11"><span class="header-section-number">1.11</span> Weight Initialization</h2>
<p>Proper initialization can prevent gradient vanishing/exploding problems.</p>
<section id="xavier-initialization" data-number="0.11.1">
<h3 data-number="1.11.1"><span class="header-section-number">1.11.1</span> Xavier Initialization</h3>
<p>Xavier initialization ensures that activation and gradient variances remain consistent eliminating vanishing/ exploding histograms.</p>
<ul>
<li><p><strong>Formula</strong>: <span class="math display">\[W \sim U\left(-\frac{\sqrt{6}}{\sqrt{n_{in} + n_{out}}}, \frac{\sqrt{6}}{\sqrt{n_{in} + n_{out}}}\right)\]</span> where <span class="math inline">\(n_{in}\)</span> and <span class="math inline">\(n_{out}\)</span> are the number of inputs and outputs to a layer, respectively.</p></li>
</ul>
</section>
<section id="kaiming-initialization" data-number="0.11.2">
<h3 data-number="1.11.2"><span class="header-section-number">1.11.2</span> Kaiming Initialization</h3>
<p>Kaiming initialization, is designed specifically for layers with ReLU (Rectified Linear Unit) activations or its variants. ReLU units deactivate for negative inputs, making it important to ensure weights are initialized to maintain forward-propagated variance and backpropagated gradient stability. The factor <span class="math inline">\(\frac{2}{n_{\text{in}}}\)</span> scales the weights to balance variance, thereby reducing the risk of gradients vanishing or exploding in deeper networks.</p>
<ul>
<li><p><strong>Formula</strong>: <span class="math display">\[W \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}}}\right)\]</span> where <span class="math inline">\(n_{\text{in}}\)</span> is the number of inputs to a layer.</p></li>
</ul>
</section>
</section>
<section id="hyperparameter-tuning" data-number="0.12">
<h2 data-number="1.12"><span class="header-section-number">1.12</span> Hyperparameter Tuning</h2>
<p>Efficient hyperparameter tuning can significantly improve model performance. Manually tuning hyperparameters is possible but it can be slow and unoptimal.</p>
<section id="grid-search" data-number="0.12.1">
<h3 data-number="1.12.1"><span class="header-section-number">1.12.1</span> Grid Search</h3>
<ul>
<li><p><strong>Method</strong>: Manually define and explore all possible combinations of hyperparameters, though computationally expensive.</p></li>
</ul>
</section>
<section id="random-search" data-number="0.12.2">
<h3 data-number="1.12.2"><span class="header-section-number">1.12.2</span> Random Search</h3>
<ul>
<li><p><strong>Method</strong>: Sample randomly from hyperparameter distributions, typically faster than grid search.</p></li>
</ul>
</section>
</section>
<section id="exercises" data-number="0.13">
<h2 data-number="1.13"><span class="header-section-number">1.13</span> Exercises</h2>
<ol>
<li><p>You are training a model to classify images into one of ten categories. The dataset has roughly an equal number of samples for each category. Select the most suitable loss function.</p>
<ul>
<li><p>A. Mean Squared Error (MSE) Loss</p></li>
<li><p>B. Cross-Entropy Loss</p></li>
<li><p>C. L1 Loss</p></li>
<li><p>D. Weighted Cross-Entropy Loss</p></li>
</ul></li>
<li><p>You are working on a medical image segmentation task where the goal is to identify regions of tumors in MRI scans, with the tumor pixels being far fewer than non-tumor pixels. Select the most suitable loss function.</p>
<ul>
<li><p>A. Cross-Entropy Loss</p></li>
<li><p>B. Weighted Cross-Entropy Loss</p></li>
<li><p>C. MSE Loss</p></li>
<li><p>D. Focal Loss</p></li>
</ul></li>
<li><p>The histogram slope increases in each successive layer, with values extending to large magnitudes, suggesting an exploding gradient. How would you fix this?</p>
<ul>
<li><p>A. Increase the learning rate</p></li>
<li><p>B. Decrease the learning rate</p></li>
<li><p>C. Change the activation function to a more aggressive variant like ReLU</p></li>
</ul></li>
<li><p>Correct Answers</p>
<ul>
<li><p>1. B</p></li>
<li><p>2. D</p></li>
<li><p>3. B</p></li>
</ul></li>
</ol>
</section>
<section id="common-notations" data-number="0.14">
<h2 data-number="1.14"><span class="header-section-number">1.14</span> Common Notations</h2>
<div class="multicols">
<p><span>2</span></p>
<ul>
<li><p><span class="math inline">\(\mathbf{X}\)</span>: Matrix of feature vectors (dataset)</p></li>
<li><p><span class="math inline">\(\mathbf{W}\)</span>: Weight matrix</p></li>
<li><p><span class="math inline">\(N\)</span>: Number of data samples</p></li>
<li><p><span class="math inline">\(\alpha\)</span>: Learning rate</p></li>
<li><p><span class="math inline">\(\mathbf{H(\theta)}\)</span>: Hessian matrix</p></li>
<li><p><span class="math inline">\(E_\theta\)</span>: Encoding function</p></li>
<li><p><span class="math inline">\(G_\Phi\)</span>: Decoding function</p></li>
<li><p><span class="math inline">\(\mathbf{b}\)</span>: Bias vector</p></li>
<li><p><span class="math inline">\(\mathbf{G}(t)\)</span>: Second moment at time <span class="math inline">\(t\)</span></p></li>
<li><p><span class="math inline">\(f(\cdot)\)</span>: Trained neural network</p></li>
<li><p><span class="math inline">\(\gamma\)</span>: Bias factor</p></li>
<li><p><span class="math inline">\(y_i\)</span>: Target class</p></li>
</ul>
</div>
</section>
</body>
</html>

</main>
</body>
</html>
