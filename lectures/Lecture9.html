<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>Lecture 8 (first-order methods)</title>
  <link rel="stylesheet" href="../assets/style.css"/>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav>
  <a href="../index.html">Home</a>
</nav>
<main>
<div class="center">

</div>
<p><strong>Contributors:</strong> Dr. Ahmad Mustafa, Dr. Motaz Alfarraj, Dr. Ashraf Alattar, Dr. Chen Zhou</p>
<p><strong>Teaching Assistants</strong> with remarkable contributions include: Kuo-Wei Lai, Wuyang Du, Shiva Mahato, Michael Zhou, Ninghan Zhong</p>
<p><strong>Disclaimer</strong>: <span>All content of these notes are part of this course at Georgia Tech. Any re-use or distribution is not permitted without pre-approved permission. All these notes belong to, created by, and copyrighted for Ghassan AlRegib and Mohit Prabhushankar, Georgia Tech, 2021–2028.</span></p>
<p><strong>License</strong>: <span>These lecture notes are licensed under the <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.</span></p>
<p><strong>Errata</strong>: <span><em>Please submit any errata you find using the following form: <a href="https://forms.office.com/r/fbg9dMWPgY">Errata Form for FunML Textbook</a> or visit: <a href="https://forms.office.com/r/fbg9dMWPgY">https://forms.office.com/r/fbg9dMWPgY</a></em></span></p>
<section id="recap-and-lecture-objectives" data-number="0.1">
<h2 data-number="1.1"><span class="header-section-number">1.1</span> Recap and Lecture Objectives</h2>
<p>In the previous lecture, we continued our study of regression models and the key tools used to train and evaluate them. We covered optimization methods for fitting regression models, including gradient-based approaches as well as <strong>Newton’s Method</strong> and <strong>Coordinate Search</strong>. We also discussed how <strong>regularization</strong> improves generalization in linear regression, with particular emphasis on <strong>Ridge</strong> and <strong>Lasso</strong>, and we reviewed common performance measures such as Mean Squared Error (MSE), Mean Absolute Error (MAE), and the Coefficient of Determination (<span class="math inline">\(R^2\)</span>).</p>
<p>In this lecture, we build on those foundations by organizing the regression workflow end-to-end. We begin by summarizing optimization methods beyond basic gradient descent, including <strong>Newton’s Method</strong> (which uses curvature through the Hessian) and <strong>coordinate-based methods</strong> such as Coordinate Search and Coordinate Descent. Next, we revisit <strong>regularization</strong>—including Ridge (<span class="math inline">\(\ell_2\)</span>), Lasso (<span class="math inline">\(\ell_1\)</span>), and Elastic Net—to understand how controlling coefficient magnitudes helps prevent overfitting. We then review key <strong>performance metrics</strong> for regression, including MSE, RMSE, MAE, and <span class="math inline">\(R^2\)</span>.</p>
<p>Finally, the main focus of this lecture is <strong>model validation and evaluation</strong>. We introduce the train/validation/test split, learning curves, cross-validation, and the bias–variance tradeoff, which together provide a practical framework for understanding model generalization and performance on unseen data. We also highlight how evaluation becomes more subtle when labels are noisy or ambiguous.</p>
</section>
<section id="second-order-optimization-newtons-method" data-number="0.2">
<h2 data-number="1.2"><span class="header-section-number">1.2</span> Second-Order Optimization: Newton’s Method</h2>
<p>Second-order optimization methods improve upon Gradient Descent by using curvature information of the loss surface. Instead of relying only on the slope (first derivative), these methods also use second derivatives to better estimate the location of the minimum.</p>
<section id="jacobian-and-hessian-matrices" data-number="0.2.1">
<h3 data-number="1.2.1"><span class="header-section-number">1.2.1</span> Jacobian and Hessian Matrices</h3>
<p>The Jacobian matrix is a matrix composed of first-order partial derivatives of a multivariable function. It describes how a function changes with respect to each variable and is commonly used in optimization and nonlinear systems. The Hessian matrix extends this idea by collecting second-order partial derivatives. It is an <span class="math inline">\(n \times n\)</span> square matrix that captures the curvature of a function and plays a central role in second-order optimization methods.</p>
<p>For example, consider the function <span class="math inline">\(f(x,y) = x^2y + y^2x\)</span>. The Hessian matrix for this function is <span class="math display">\[H_f(x,y)=
\begin{pmatrix}
2y &amp; 2x+2y\\
2x+2y &amp; 2x
\end{pmatrix}.\]</span></p>
</section>
<section id="optimization" data-number="0.2.2">
<h3 data-number="1.2.2"><span class="header-section-number">1.2.2</span> Optimization</h3>
<p>Second-order optimization methods use curvature information to guide the search for a minimum. While first-order methods such as Gradient Descent rely only on the slope of the loss surface, second-order methods additionally use the Hessian matrix to capture how the surface bends. This curvature information allows the optimizer to take more informed steps toward the minimum.</p>
<p>When the Hessian matrix is positive semi-definite, <span class="math display">\[\Delta\boldsymbol{\theta}^T \mathbf{H}(\boldsymbol{\theta})\Delta\boldsymbol{\theta} \ge 0
\quad \text{for any } \Delta\boldsymbol{\theta}.\]</span> the loss surface locally behaves like a convex parabola. In this setting, curvature information can be used to construct a more accurate local model of the loss function.</p>
<p>Using a second-order Taylor approximation, the loss near a point <span class="math inline">\(\boldsymbol{\theta}\)</span> can be written as <span class="math display">\[L(\boldsymbol{\theta}+\Delta\boldsymbol{\theta})
\approx
L(\boldsymbol{\theta})
+\Delta\boldsymbol{\theta}^T\nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta})
+\frac{1}{2}\Delta\boldsymbol{\theta}^T \mathbf{H}(\boldsymbol{\theta})\Delta\boldsymbol{\theta}.\]</span> To find the minimum of this quadratic approximation, we set the derivative with respect to <span class="math inline">\(\Delta\theta\)</span> equal to zero: <span class="math display">\[\nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta})
+\mathbf{H}(\boldsymbol{\theta})\Delta\boldsymbol{\theta}=0,\]</span></p>
<p>which yields the optimal update direction <span class="math display">\[\Delta\boldsymbol{\theta}
=
-\mathbf{H}(\boldsymbol{\theta})^{-1}
\nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta}).\]</span></p>
<p>Substituting this update into the iterative optimization framework leads to the Newton’s Method update rule <span class="math display">\[\boldsymbol{\theta}^{t+1}
=
\boldsymbol{\theta}^{t}
-\alpha \mathbf{H}(\boldsymbol{\theta})^{-1}
\nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta}).\]</span></p>
<p>For comparison, standard Gradient Descent uses only first-order information: <span class="math display">\[\boldsymbol{\theta}^{t+1}
=
\boldsymbol{\theta}^{t}
-\alpha \nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta}^{t}).\]</span></p>
<p>A matrix is positive semi-definite when it is symmetric and has non-negative eigenvalues, which guarantees locally convex curvature and enables reliable descent toward a minimum.</p>
<figure>
<img src="img/lecture9/P2.png" id="fig:P2.png" style="width:40.0%" alt="Visualization of the function with respect to parameters w_1 and w_2 (quadratic function in blue)" /><figcaption aria-hidden="true">Visualization of the function with respect to parameters <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span> (quadratic function in blue)</figcaption>
</figure>
</section>
<section id="convexity" data-number="0.2.3">
<h3 data-number="1.2.3"><span class="header-section-number">1.2.3</span> Convexity</h3>
<p>In optimization, the concept of convexity plays a crucial role in determining how easily we can find the global minimum of a function. A function <span class="math inline">\(f(\boldsymbol{\theta})\)</span> is said to be <em>convex</em> if, for any two points <span class="math inline">\(\boldsymbol{\theta}_1\)</span> and <span class="math inline">\(\boldsymbol{\theta}_2\)</span> within its domain, the line segment connecting <span class="math inline">\(f(\theta_1)\)</span> and <span class="math inline">\(f(\theta_2)\)</span> lies above or on the graph of the function itself. Formally, for any <span class="math inline">\(\boldsymbol{\theta}_1, \boldsymbol{\theta}_2 \in \mathrm{dom}(f)\)</span> and <span class="math inline">\(\lambda \in [0,1]\)</span>: <span class="math display">\[f(\lambda \boldsymbol{\theta}_1 + (1-\lambda) \boldsymbol{\theta}_2)
\leq \lambda f(\boldsymbol{\theta}_1) + (1-\lambda) f(\boldsymbol{\theta}_2).\]</span></p>
<p>This property guarantees that the function has a single global minimum and no other local minima, making optimization significantly easier.</p>
<p>For twice-differentiable functions, convexity can be checked using the Hessian matrix. A function is convex if its Hessian is positive semi-definite everywhere: <span class="math display">\[\mathbf{H}(\boldsymbol{\theta}) \succeq 0.\]</span> This means the loss surface curves upward in every direction, forming a bowl-shaped landscape that guides optimization algorithms toward the global minimum.</p>
<p>In contrast, <em>non-convex</em> functions do not satisfy the convexity condition. The line segment between two points may lie below the function curve, producing multiple local minima and maxima. In these cases, optimization algorithms such as gradient descent may become trapped in local minima and fail to reach the global solution.</p>
</section>
<section id="convergence" data-number="0.2.4">
<h3 data-number="1.2.4"><span class="header-section-number">1.2.4</span> Convergence</h3>
<p>Newton’s method is particularly powerful for minimizing <em>convex</em> functions. Because it incorporates curvature information through the Hessian, it produces a quadratic approximation of the loss surface at every step. This allows the algorithm to move directly toward the minimum rather than following the slower zig-zag path typical of gradient descent.</p>
<p>Near the optimum, Newton’s method exhibits <strong>quadratic convergence</strong>, meaning the error decreases extremely rapidly once the algorithm is close to the minimum. In contrast, gradient descent typically achieves only <strong>linear convergence</strong>, requiring many more iterations to reach a similar level of accuracy.</p>
<p>However, Newton’s method becomes less reliable for <em>non-convex</em> functions. When the Hessian is not positive semi-definite, the quadratic approximation may point toward a saddle point or even a local maximum. As illustrated in the figures, the method may converge to an undesirable local minimum, move toward a local maximum, or oscillate between regions due to incorrect curvature information.</p>
<p>For this reason, Newton’s method is most effective when the loss function is convex or when the optimization has already reached a region close to a local minimum.</p>
<figure>
<img src="img/lecture9/P3.png" id="fig:P3.png" style="width:40.0%" alt="Newton’s Method Convergence: Convex Functions" /><figcaption aria-hidden="true">Newton’s Method Convergence: Convex Functions</figcaption>
</figure>
<figure>
<img src="img/lecture9/P4.png" id="fig:P4.png" style="width:40.0%" alt="Newton’s Method Convergence: Non-Convex Functions" /><figcaption aria-hidden="true">Newton’s Method Convergence: Non-Convex Functions</figcaption>
</figure>
</section>
<section id="computational-complexity" data-number="0.2.5">
<h3 data-number="1.2.5"><span class="header-section-number">1.2.5</span> Computational Complexity</h3>
<p>Although Newton’s method can converge in fewer iterations than first-order methods, it is significantly more expensive per iteration. The main challenge is the Hessian matrix, which is a <span class="math inline">\(P \times P\)</span> matrix containing all second-order partial derivatives of the loss function.</p>
<p>Storing the Hessian requires <span class="math inline">\(O(P^2)\)</span> memory, which quickly becomes impractical for models with many parameters. Computing the Hessian itself requires <span class="math inline">\(O(P^2)\)</span> operations, and computing its inverse requires approximately <span class="math inline">\(O(P^3)\)</span> time. Therefore, the total computational cost per Newton update is dominated by the matrix inversion step:</p>
<p><span class="math display">\[\boldsymbol{\theta}^{t+1}
=
\boldsymbol{\theta}^{t}
-\alpha \mathbf{H}(\boldsymbol{\theta})^{-1}
\nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta})\]</span></p>
<p>Where <span class="math inline">\(\mathbf{H}(\boldsymbol{\theta})\)</span> is the Hessian matrix: <span class="math display">\[\mathbf{H}(\boldsymbol{\theta})=\nabla_{\boldsymbol{\theta}}^2 L(\boldsymbol{\theta})\]</span></p>
<p><span class="math display">\[\boxed{\textbf{Total time per iteration of Newton&#39;s Method: } O(P^3)}\]</span></p>
<p>Because modern machine learning models often contain millions of parameters, directly computing and inverting the Hessian is typically infeasible. For this reason, most large-scale learning algorithms rely on first-order methods such as Gradient Descent or use approximate second-order methods (e.g., quasi-Newton methods) that avoid explicit Hessian computation.</p>
</section>
</section>
<section id="coordinate-search" data-number="0.3">
<h2 data-number="1.3"><span class="header-section-number">1.3</span> Coordinate Search</h2>
<p>When gradient information is unavailable or too expensive to compute, derivative-free optimization methods provide an alternative strategy.</p>
<section id="optimization-1" data-number="0.3.1">
<h3 data-number="1.3.1"><span class="header-section-number">1.3.1</span> Optimization</h3>
<p>Coordinate search is a simple optimization technique that does not rely on gradients or second-order derivatives. Instead of computing the steepest descent direction, the algorithm explores the parameter space by moving along one coordinate axis at a time. At each iteration, the method tests whether moving in the positive or negative direction of a coordinate decreases the loss, and updates the parameters accordingly. The update rule is</p>
<p><span class="math display">\[\boldsymbol{\theta}^{t+1}
=
\boldsymbol{\theta}^{t}
\pm \alpha \mathbf{e}_j\]</span></p>
<p>where <span class="math inline">\(\mathbf{e}_j = [0, \ldots, 1, \ldots, 0]^T\)</span> is the <span class="math inline">\(j\)</span>-th standard basis vector, which selects the coordinate direction being explored.</p>
<p>This approach is useful when gradients are unavailable, difficult to compute, or expensive to evaluate. Because it does not require derivatives, coordinate search belongs to the class of <em>derivative-free optimization</em> methods. However, this simplicity comes at a cost: the algorithm may require many function evaluations and can be slower than gradient-based methods in high-dimensional problems.</p>
<p>Figure <a href="#fig:P5.png" data-reference-type="ref" data-reference="fig:P5.png">4</a> illustrates how the algorithm explores descent directions along coordinate axes, moving step-by-step toward lower values of the objective function.</p>
<figure>
<img src="img/lecture9/P5.png" id="fig:P5.png" style="width:40.0%" alt="Coordinate Search Algorithm: Exploring Descent Directions Along Coordinate Axes" /><figcaption aria-hidden="true">Coordinate Search Algorithm: Exploring Descent Directions Along Coordinate Axes</figcaption>
</figure>
</section>
</section>
<section id="coordinate-descent" data-number="0.4">
<h2 data-number="1.4"><span class="header-section-number">1.4</span> Coordinate Descent</h2>
<p>While Coordinate Search explores directions without using gradients, Coordinate Descent improves efficiency by incorporating partial derivative information.</p>
<section id="optimization-2" data-number="0.4.1">
<h3 data-number="1.4.1"><span class="header-section-number">1.4.1</span> Optimization</h3>
<p>Coordinate descent is an optimization strategy that improves the objective function by updating <em>one parameter (one coordinate of <span class="math inline">\(\boldsymbol{\theta}\)</span>) at a time</em>. Instead of computing a full gradient vector across all parameters, the algorithm isolates a single coordinate direction and performs a one–dimensional optimization step along that axis. This greatly simplifies each update and reduces computational cost, especially in high–dimensional problems.</p>
<p>At each iteration, the algorithm selects a coordinate index <span class="math inline">\(j \in \{1,\dots,P\}\)</span> and updates only the corresponding parameter while keeping all other parameters fixed. For a continuously differentiable loss function <span class="math inline">\(L(\boldsymbol{\theta})\)</span>, the update rule becomes</p>
<p><span class="math display">\[\theta_j^{t+1}
=
\theta_j^{t}
-\alpha \nabla_{\theta_j} L(\boldsymbol{\theta}).\]</span></p>
<p>This process is repeated across coordinates until convergence. The coordinates may be selected cyclically, randomly, or based on a heuristic such as the largest gradient magnitude. By decomposing a high–dimensional optimization problem into a sequence of simpler one–dimensional updates, coordinate descent can be particularly effective for large-scale machine learning problems where computing full gradients is expensive.</p>
<figure>
<img src="img/lecture9/P6.png" id="fig:P6.png" style="width:40.0%" alt="Coordinate Descent Algorithm: Stepwise Optimization Along Coordinate Axes" /><figcaption aria-hidden="true">Coordinate Descent Algorithm: Stepwise Optimization Along Coordinate Axes</figcaption>
</figure>
</section>
<section id="convergence-1" data-number="0.4.2">
<h3 data-number="1.4.2"><span class="header-section-number">1.4.2</span> Convergence</h3>
<p>In early iterations, coordinate search methods can make progress by exploring all coordinate directions, but coordinate descent quickly becomes more efficient once gradient information is used. Because each step directly follows the partial derivative with respect to a single parameter, the algorithm typically identifies descent directions faster and reduces the loss more effectively than coordinate search.</p>
<p>As illustrated in Figure <a href="#fig:P7.png" data-reference-type="ref" data-reference="fig:P7.png">6</a>, coordinate descent tends to reach lower-cost regions of the objective function in fewer iterations. Although each step only adjusts one parameter, the repeated sequence of updates gradually drives the parameters toward a minimum. This stepwise progress often produces a characteristic “zig-zag” path toward the optimum.</p>
<figure>
<img src="img/lecture9/P7.png" id="fig:P7.png" style="width:40.0%" alt="Comparison of Coordinate Search (Left) and Coordinate Descent (Right): Efficiency in Finding Lower Cost Function Values" /><figcaption aria-hidden="true">Comparison of Coordinate Search (Left) and Coordinate Descent (Right): Efficiency in Finding Lower Cost Function Values</figcaption>
</figure>
</section>
<section id="advantages" data-number="0.4.3">
<h3 data-number="1.4.3"><span class="header-section-number">1.4.3</span> Advantages</h3>
<p>One of the main strengths of coordinate descent is its simplicity. Each update requires only a partial derivative with respect to a single parameter, making the method easy to implement and computationally inexpensive. Unlike second-order methods such as Newton’s method, coordinate descent does not require storing or inverting large matrices, which makes it highly scalable to problems with many parameters.</p>
<p>Because of its low memory requirements and cheap updates, coordinate descent is widely used in large-scale machine learning applications such as Lasso regression, sparse optimization, and high-dimensional linear models.</p>
</section>
<section id="limitations" data-number="0.4.4">
<h3 data-number="1.4.4"><span class="header-section-number">1.4.4</span> Limitations</h3>
<p>Despite its simplicity, coordinate descent has important limitations. The algorithm can struggle when the objective function is non-smooth or when variables are highly coupled. In such cases, improving one parameter at a time may lead to slow progress, oscillations, or convergence to suboptimal points such as saddle points.</p>
<p>Figure <a href="#fig:P8.png" data-reference-type="ref" data-reference="fig:P8.png">7</a> illustrates an example of a non-smooth multivariable function. The sharp corners of the level curves make it difficult for the algorithm to determine a consistent descent direction. As a result, the optimization path (shown in red) can stall or progress very slowly toward the global minimum.</p>
<figure>
<img src="img/lecture9/P8.png" id="fig:P8.png" style="width:40.0%" alt="A Case of Non-Smooth Multivariable Function" /><figcaption aria-hidden="true">A Case of Non-Smooth Multivariable Function</figcaption>
</figure>
<p>The red trajectory highlights how the optimization may stagnate or move inefficiently due to the non-smooth geometry of the loss surface.</p>
</section>
</section>
<section id="summary-of-optimization-methods" data-number="0.5">
<h2 data-number="1.5"><span class="header-section-number">1.5</span> Summary of Optimization Methods</h2>
<p>We now summarize the optimization methods discussed in Lecture 8 (first-order methods) and Lecture 9 (second-order and coordinate-based methods).</p>
<table>
<caption>Comparison of Optimization Methods</caption>
<thead>
<tr class="header">
<th style="text-align: center;"><strong>Method</strong></th>
<th style="text-align: center;"><strong>Uses Gradient?</strong></th>
<th style="text-align: center;"><strong>Uses Hessian?</strong></th>
<th style="text-align: center;"><strong>Per–Iteration Cost</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Gradient Descent</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Low</td>
</tr>
<tr class="even">
<td style="text-align: center;">Newton’s Method</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">Very High (<span class="math inline">\(O(P^3)\)</span>)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Coordinate Search</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Medium</td>
</tr>
<tr class="even">
<td style="text-align: center;">Coordinate Descent</td>
<td style="text-align: center;">Partial</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">Low</td>
</tr>
</tbody>
</table>
<p>Each method presents a trade-off between computational cost and convergence speed. In practice, the choice of optimizer depends on the size of the dataset, the number of parameters, and the availability of derivative information.</p>
</section>
<section id="ridge-regularization" data-number="0.6">
<h2 data-number="1.6"><span class="header-section-number">1.6</span> Ridge Regularization</h2>
<section id="notation-note." data-number="0.6.0.0.1">
<h5 data-number="1.6.0.0.1"><span class="header-section-number">1.6.0.0.1</span> Notation note.</h5>
<p>Some figures in this lecture use the symbol <span class="math inline">\(\gamma\)</span> to denote the regularization strength. Throughout the notes, we use <span class="math inline">\(\lambda\)</span> for consistency with the course notation. These symbols refer to the same quantity.</p>
</section>
<section id="formulation" data-number="0.6.1">
<h3 data-number="1.6.1"><span class="header-section-number">1.6.1</span> Formulation</h3>
<p>Standard linear regression models attempt to fit data samples by minimizing the least squares cost. However, this approach can become problematic in practical settings. When features contain noise, the learned coefficients may change dramatically, making the model unstable. Similarly, when only a small amount of training data is available, the model can easily overfit and fail to generalize to new data. These issues arise because large coefficients can have a disproportionately high impact on model predictions.</p>
<p><strong>Ridge Regression</strong> (also called <strong>Tikhonov regularization</strong>) addresses this problem by shrinking the regression coefficients <span class="math inline">\(\boldsymbol{\theta}\)</span>. It does so by imposing a penalty on the squared <span class="math inline">\(\ell_2\)</span> norm <span class="math inline">\(\|\boldsymbol{\theta}\|_2^2\)</span>, which discourages excessively large parameter values. This penalty is incorporated directly into the training objective.</p>
<p>During training, the regularization term is added to the cost function: <span class="math display">\[L(\boldsymbol{\theta})
=
\frac{1}{N}\sum_{i=1}^{N}
\left(\boldsymbol{\theta}^T\mathbf{x}_i - y_i\right)^2
+
\frac{\lambda}{N}\|\boldsymbol{\theta}\|_2^2\]</span></p>
<p><strong>We use <span class="math inline">\(\lambda\)</span> to denote the regularization strength to remain consistent with the course notation file.</strong></p>
<p>where <span class="math inline">\(\lambda &gt; 0\)</span> is a hyperparameter that controls the strength of the shrinkage. Larger values of <span class="math inline">\(\lambda\)</span> lead to stronger regularization and smaller coefficients.</p>
<p>It is important to note that the regularization term is used only during training. Once the model has been trained, its performance is evaluated using the standard prediction error without including the regularization penalty.</p>
</section>
<section id="robustness-to-noise" data-number="0.6.2">
<h3 data-number="1.6.2"><span class="header-section-number">1.6.2</span> Robustness to Noise</h3>
<p>Standard linear regression can be highly sensitive to noise in the training data. When small perturbations or noisy samples are introduced, the fitted regression line can change dramatically, resulting in large variability in the estimated coefficients. This behavior is illustrated in Figure <a href="#fig:P9.png" data-reference-type="ref" data-reference="fig:P9.png">8</a>, where multiple models are trained on slightly different noisy datasets. The gray regression lines vary widely, demonstrating that ordinary least squares can produce unstable parameter estimates when the data contains noise.</p>
<p>Ridge regression addresses this issue by adding an <span class="math inline">\(L_2\)</span> penalty that discourages large coefficient values. By shrinking the parameters toward zero, the model becomes less sensitive to fluctuations in the data and produces more stable predictions. As shown in Figure <a href="#fig:P10.png" data-reference-type="ref" data-reference="fig:P10.png">9</a>, the fitted lines produced by ridge regression exhibit significantly less variation across noisy datasets. This reduction in variance highlights one of the key benefits of regularization: improved robustness to noise and better generalization to unseen data.</p>
<figure>
<img src="img/lecture9/P9.png" id="fig:P9.png" style="width:40.0%" alt="Large variance of slope (coefficients) of linear regression models (gray lines) with induced noise samples (gray dots)" /><figcaption aria-hidden="true">Large variance of slope (coefficients) of linear regression models (gray lines) with induced noise samples (gray dots)</figcaption>
</figure>
<figure>
<img src="img/lecture9/P10.png" id="fig:P10.png" style="width:40.0%" alt="Reduced variance of ridge regression models (gray lines) with induced noise samples (gray dots)" /><figcaption aria-hidden="true">Reduced variance of ridge regression models (gray lines) with induced noise samples (gray dots)</figcaption>
</figure>
</section>
<section id="reduce-overfitting-to-limited-training-data" data-number="0.6.3">
<h3 data-number="1.6.3"><span class="header-section-number">1.6.3</span> Reduce Overfitting to Limited Training Data</h3>
<p>Overfitting becomes particularly severe when only a small amount of training data is available. In such cases, standard linear regression tends to fit the training points too closely, capturing random fluctuations rather than the underlying trend. This behavior is illustrated in Figure <a href="#fig:P11.png" data-reference-type="ref" data-reference="fig:P11.png">10</a>, where the linear regression model (red line) fits the limited training samples almost perfectly but fails to generalize well to new data.</p>
<p>Ridge regression mitigates this problem by constraining the magnitude of the coefficients. The resulting model (blue line) does not pass exactly through the training points but instead captures a smoother and more general trend. As a result, it performs better on unseen test samples (green dots). This example demonstrates how regularization introduces a small amount of bias in exchange for a large reduction in variance, ultimately improving predictive performance.</p>
<figure>
<img src="img/lecture9/P11.png" id="fig:P11.png" style="width:40.0%" alt="Ridge Regression vs. Linear Regression" /><figcaption aria-hidden="true">Ridge Regression vs. Linear Regression</figcaption>
</figure>
</section>
<section id="effect-of-lambda-on-boldsymboltheta-magnitudes" data-number="0.6.4">
<h3 data-number="1.6.4"><span class="header-section-number">1.6.4</span> Effect of <span class="math inline">\(\lambda\)</span> on <span class="math inline">\(\boldsymbol{\theta}\)</span> Magnitudes</h3>
<p>The regularization strength in ridge regression is controlled by the hyperparameter <span class="math inline">\(\lambda\)</span>, which determines how strongly large coefficients are penalized. When <span class="math inline">\(\lambda\)</span> is very small, the penalty term has little influence, and the model behaves similarly to standard linear regression. In this regime, the coefficients can take on larger values, allowing the model to closely fit the training data.</p>
<p>As <span class="math inline">\(\lambda\)</span> increases, the penalty becomes stronger and the coefficients are increasingly shrunk toward zero. This shrinkage reduces model complexity and helps prevent overfitting. When <span class="math inline">\(\lambda\)</span> becomes very large, the coefficients approach zero and the model converges toward a nearly flat line that approximates the mean of the target variable. Figures <a href="#fig:P12.png" data-reference-type="ref" data-reference="fig:P12.png">11</a> and <a href="#fig:P13.png" data-reference-type="ref" data-reference="fig:P13.png">12</a> illustrate how increasing <span class="math inline">\(\lambda\)</span> progressively reduces coefficient magnitudes and simplifies model behavior.</p>
<p>This trade-off between model flexibility and regularization strength highlights the bias–variance trade-off: small <span class="math inline">\(\lambda\)</span> values yield low bias but high variance, while large <span class="math inline">\(\lambda\)</span> values produce higher bias but lower variance.</p>
<figure>
<img src="img/lecture9/P12.png" id="fig:P12.png" style="width:50.0%" alt="Impact of \lambda on Ridge Regression Coefficients" /><figcaption aria-hidden="true">Impact of <span class="math inline">\(\lambda\)</span> on Ridge Regression Coefficients</figcaption>
</figure>
<figure>
<img src="img/lecture9/P13.png" id="fig:P13.png" style="width:40.0%" alt="Impact of \lambda on Model Behavior" /><figcaption aria-hidden="true">Impact of <span class="math inline">\(\lambda\)</span> on Model Behavior</figcaption>
</figure>
</section>
<section id="normal-equation" data-number="0.6.5">
<h3 data-number="1.6.5"><span class="header-section-number">1.6.5</span> Normal Equation</h3>
<p>The optimal parameter vector for ridge regression can be derived analytically using the <strong>normal equation</strong>. Recall that ridge regression augments the standard least-squares objective with an <span class="math inline">\(L_2\)</span> penalty on the parameter vector. In matrix form, the ridge loss function is written as</p>
<p><span class="math display">\[L(\boldsymbol{\theta})
=
\frac{1}{N}
(\mathbf{X}\boldsymbol{\theta}-\mathbf{y})^T
(\mathbf{X}\boldsymbol{\theta}-\mathbf{y})
+
\frac{\lambda}{N}\boldsymbol{\theta}^T\boldsymbol{\theta}.\]</span></p>
<p>where <span class="math inline">\(\mathbf{X}\)</span> is the design matrix, <span class="math inline">\(\mathbf{y}\)</span> is the target vector, and <span class="math inline">\(\lambda &gt; 0\)</span> is the regularization hyperparameter that controls the strength of the penalty.</p>
<p>To obtain the optimal parameters, we differentiate the loss with respect to <span class="math inline">\(\boldsymbol{\theta}\)</span> and set the gradient equal to zero:</p>
<p><span class="math display">\[\nabla_{\boldsymbol{\theta}}L
=
2\mathbf{X}^T\mathbf{X}\boldsymbol{\theta}
-2\mathbf{X}^T\mathbf{y}
+2\lambda\boldsymbol{\theta}.\]</span></p>
<p>Setting this derivative to zero yields the linear system</p>
<p><span class="math display">\[(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})\boldsymbol{\theta}
=
\mathbf{X}^T\mathbf{y}.\]</span></p>
<p>where <span class="math inline">\(\mathbf{I}\)</span> is the identity matrix. Solving for <span class="math inline">\(\boldsymbol{\theta}\)</span> gives the closed-form ridge regression solution</p>
<p><span class="math display">\[\boldsymbol{\theta}
=
(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}
\mathbf{X}^T\mathbf{y}.\]</span></p>
<p>This equation shows how ridge regression stabilizes the solution by adding <span class="math inline">\(\lambda \mathbf{I}\)</span> to <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span>. This additional term improves numerical stability and ensures the matrix is invertible, especially when features are highly correlated or when the dataset is small.</p>
</section>
<section id="gradient-descent" data-number="0.6.6">
<h3 data-number="1.6.6"><span class="header-section-number">1.6.6</span> Gradient Descent</h3>
<p>Although ridge regression admits a closed-form solution, in many practical machine learning settings the number of parameters is very large. Computing a matrix inverse becomes expensive and memory intensive. For this reason, ridge regression is often solved using <strong>gradient descent</strong>.</p>
<p>Starting from an initial parameter vector, gradient descent iteratively updates the parameters according to</p>
<p><span class="math display">\[\boldsymbol{\theta}^{(t+1)}
=
\boldsymbol{\theta}^{(t)}
-\alpha \nabla_{\boldsymbol{\theta}}L.\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is the learning rate. For ridge regression, the gradient of the loss function is</p>
<p><span class="math display">\[\nabla_{\boldsymbol{\theta}}L
=
-\frac{2}{N}\sum_{i=1}^{N}
(y_i-\boldsymbol{\theta}^T\mathbf{x}_i)\mathbf{x}_i
+\frac{2\lambda}{N}\boldsymbol{\theta}.\]</span></p>
<p>The hyperparameter <span class="math inline">\(\lambda\)</span> controls the trade-off between accurately fitting the training data and keeping the parameter magnitudes small. A small <span class="math inline">\(\lambda\)</span> produces behavior similar to standard linear regression, while a large <span class="math inline">\(\lambda\)</span> increases regularization and produces a smoother, lower-variance model.</p>
<figure>
<img src="img/lecture9/P14.png" id="fig:P14.png" style="width:40.0%" alt="Gradient Descent Path to Optimal Weights: Minimizing Loss in Ridge Regression" /><figcaption aria-hidden="true">Gradient Descent Path to Optimal Weights: Minimizing Loss in Ridge Regression</figcaption>
</figure>
</section>
</section>
<section id="lasso-regression-formulation" data-number="0.7">
<h2 data-number="1.7"><span class="header-section-number">1.7</span> Lasso Regression: Formulation</h2>
<section id="overview" data-number="0.7.1">
<h3 data-number="1.7.1"><span class="header-section-number">1.7.1</span> Overview</h3>
<p><strong>Lasso regression</strong> (Least Absolute Shrinkage and Selection Operator) is a linear regression technique that incorporates <strong>L1 regularization</strong>, which adds a penalty proportional to the absolute value of the regression coefficients.</p>
<p>This approach is particularly useful when working with datasets that contain many features, some of which may be irrelevant or only weakly related to the target variable. In such settings, ordinary linear regression can produce unstable models that overfit the training data. Lasso addresses this issue by encouraging simpler models that rely only on the most informative features.</p>
</section>
<section id="l1-regularization" data-number="0.7.2">
<h3 data-number="1.7.2"><span class="header-section-number">1.7.2</span> L1 Regularization</h3>
<p>The key difference between Lasso regression and ordinary linear regression is the addition of an <strong>L1 penalty</strong> to the loss function. This penalty is the sum of the absolute values of the regression coefficients, which discourages large parameter values and promotes sparsity.</p>
<p>The Lasso objective function is given by: <span class="math display">\[L(\boldsymbol{\theta})
=
\frac{1}{N}\sum_{i=1}^{N}
(\boldsymbol{\theta}^T\mathbf{x}_i-y_i)^2
+
\lambda\sum_{j=1}^{P}|\theta_j|\]</span></p>
<p>The regularization parameter <span class="math inline">\(\lambda\)</span> controls the strength of the penalty. When <span class="math inline">\(\lambda=0\)</span>, the model reduces to standard linear regression. As <span class="math inline">\(\lambda\)</span> increases, the penalty becomes stronger, forcing coefficients to shrink toward zero.</p>
</section>
<section id="sparse-model" data-number="0.7.3">
<h3 data-number="1.7.3"><span class="header-section-number">1.7.3</span> Sparse Model</h3>
<p>A defining property of Lasso regression is that it produces a <strong>sparse model</strong>. In a sparse model, many regression coefficients become exactly zero.</p>
<p>This behavior occurs because the L1 penalty introduces sharp “corners” in the optimization landscape. During optimization, the solution often lands exactly on these corners, causing some coefficients to become zero. As a result, Lasso automatically removes features that do not meaningfully contribute to predicting the target variable.</p>
<p>This property makes Lasso especially useful for high-dimensional datasets where the number of features is large.</p>
</section>
<section id="feature-selection" data-number="0.7.4">
<h3 data-number="1.7.4"><span class="header-section-number">1.7.4</span> Feature Selection</h3>
<p>One of the most important advantages of Lasso over Ridge regression (which uses L2 regularization) is its ability to perform <strong>automatic feature selection</strong>. Because Lasso can drive coefficients to exactly zero, it effectively identifies and retains only the most relevant features while discarding irrelevant ones.</p>
<p>This leads to several practical benefits, including:</p>
<p>- Improved generalization and reduced overfitting</p>
<p>- Simpler and more interpretable models</p>
<p>- Reduced computational cost for downstream tasks</p>
<p>For datasets with many redundant or noisy features, Lasso provides a powerful tool for building compact and interpretable models.</p>
<figure>
<img src="img/lecture9/Screenshot 2024-09-21 at 3.39.06 PM.png" alt="As \lambda increases, coefficients of less important features shrink to zero before those of more important features." /><figcaption aria-hidden="true">As <span class="math inline">\(\lambda\)</span> increases, coefficients of less important features shrink to zero before those of more important features.</figcaption>
</figure>
</section>
</section>
<section id="lasso-regularization-gradient-descent" data-number="0.8">
<h2 data-number="1.8"><span class="header-section-number">1.8</span> Lasso Regularization: Gradient Descent</h2>
<p>Unlike Ridge regression, the L1 penalty used in Lasso is <strong>not differentiable at zero</strong>. Therefore, we cannot compute a standard gradient. Instead, we use a <strong>subgradient</strong>.</p>
<p>The subgradient of the Lasso objective is:</p>
<p><span class="math display">\[\nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta})
=
-\frac{2}{N}\sum_{i=1}^{N}
(y_i-\boldsymbol{\theta}^T\mathbf{x}_i)\mathbf{x}_i
+\lambda\,\text{sign}(\boldsymbol{\theta})\]</span></p>
<p>where the sign function is defined as</p>
<p><span class="math display">\[\text{sign}(\theta_j)=
\begin{cases}
+1 &amp; \theta_j&gt;0\\
-1 &amp; \theta_j&lt;0\\
[-1,1] &amp; \theta_j=0
\end{cases}\]</span></p>
<p>The hyperparameter <span class="math inline">\(\lambda\)</span> controls the balance between fitting the training data and encouraging sparsity. As <span class="math inline">\(\lambda\)</span> increases, the regularization strength increases, pushing more coefficients toward zero.</p>
</section>
<section id="lasso-vs-ridge-regression" data-number="0.9">
<h2 data-number="1.9"><span class="header-section-number">1.9</span> Lasso vs Ridge Regression</h2>
<p>We now summarize the key differences between Lasso (L1) and Ridge (L2) regularization:</p>
<p>The key takeaway is that Lasso and Ridge address overfitting in different ways. Ridge regression keeps all features in the model but reduces their influence by shrinking coefficients smoothly toward zero. In contrast, Lasso regression can eliminate features entirely by forcing some coefficients to become exactly zero, producing a simpler and more interpretable model.</p>
<p>In practice, Ridge is often preferred when most features are believed to contain useful information but may be noisy or correlated. Lasso is particularly useful when the dataset contains many irrelevant or redundant features and automatic feature selection is desirable.</p>
</section>
<section id="elastic-net-regularization" data-number="0.10">
<h2 data-number="1.10"><span class="header-section-number">1.10</span> Elastic Net Regularization</h2>
<section id="overview-1" data-number="0.10.1">
<h3 data-number="1.10.1"><span class="header-section-number">1.10.1</span> Overview</h3>
<p>Elastic Net regularization is a linear regression technique that combines two regularization methods: <strong>Lasso (L1) regularization</strong> and <strong>Ridge (L2) regularization</strong>. It’s used to prevent overfitting by penalizing the magnitude of regression coefficients, and it is particularly useful when dealing with datasets that have a large number of features or when the features are highly correlated (multicollinearity).</p>
<p>The Elastic Net formula is as follows:</p>
<p><span class="math display">\[L(\boldsymbol{\theta}) =
\frac{1}{N}\sum_{i=1}^{N}(\boldsymbol{\theta}^T\mathbf{x}_i-y_i)^2
+ \lambda \left(
r \sum_{j=1}^{P} |\theta_j| + (1 - r)\frac{1}{2}\|\boldsymbol{\theta}\|_2^2
\right)\]</span></p>
<p>The hyperparameter <span class="math inline">\(r \in [0,1]\)</span> controls the balance between the L1 and L2 penalties. When <span class="math inline">\(r = 1\)</span>, Elastic Net becomes Lasso regression. When <span class="math inline">\(r = 0\)</span>, Elastic Net becomes Ridge regression. Values between 0 and 1 create a weighted combination of both penalties.</p>
<p>Elastic Net was introduced to address limitations of using Lasso or Ridge alone. Lasso can perform feature selection, but it may behave unstably when features are highly correlated, often selecting only one feature from a correlated group. Ridge regression handles correlated features well but cannot remove irrelevant features. Elastic Net combines the strengths of both methods by encouraging sparsity while also stabilizing coefficient estimates in the presence of multicollinearity.</p>
</section>
<section id="compared-to-lasso-and-ridge" data-number="0.10.2">
<h3 data-number="1.10.2"><span class="header-section-number">1.10.2</span> Compared to Lasso and Ridge</h3>
<p>How does Elastic Net compare to Lasso and Ridge regularization separately? To highlight their differences, we consider a fixed MSE loss with the following contour plot:</p>
<figure>
<img src="img/lecture9/P15.png" id="fig:P15.png" style="width:85.0%" alt="Geometric intuition of L1 vs L2 regularization. L1 regularization (Lasso) tends to produce sparse solutions by driving some coefficients exactly to zero, while L2 regularization (Ridge) shrinks coefficients smoothly toward zero without fully eliminating them." /><figcaption aria-hidden="true">Geometric intuition of L1 vs L2 regularization. L1 regularization (Lasso) tends to produce sparse solutions by driving some coefficients exactly to zero, while L2 regularization (Ridge) shrinks coefficients smoothly toward zero without fully eliminating them.</figcaption>
</figure>
<p>The figure illustrates the geometric intuition behind L1 and L2 regularization and helps explain why Elastic Net combines the strengths of both approaches.</p>
<p>The contour lines represent level sets of the Mean Squared Error (MSE) loss in parameter space. The goal of training is to move toward the minimum of this loss surface. The white dots illustrate the path taken by gradient descent starting from an initial point (red square) and moving toward the optimal solution.</p>
<section id="top-left-ell_1-penalty-geometry." data-number="0.10.2.0.1">
<h5 data-number="1.10.2.0.1"><span class="header-section-number">1.10.2.0.1</span> Top-left: <span class="math inline">\(\ell_1\)</span> penalty geometry.</h5>
<p>The diamond-shaped constraint region corresponds to the L1 penalty used in Lasso. Notice that the corners of the diamond lie on the coordinate axes. When the loss contours first touch this constraint region, they often intersect at one of these corners. Because corners lie on the axes, one of the coefficients becomes exactly zero. This explains why Lasso produces <strong>sparse solutions</strong> and performs <strong>automatic feature selection</strong>.</p>
</section>
<section id="top-right-lasso-optimization-path." data-number="0.10.2.0.2">
<h5 data-number="1.10.2.0.2"><span class="header-section-number">1.10.2.0.2</span> Top-right: Lasso optimization path.</h5>
<p>The gradient descent trajectory shows that the solution is pulled toward the coordinate axes. The final optimum lies on the <span class="math inline">\(\theta_2 = 0\)</span> axis, meaning one parameter has been eliminated entirely.</p>
</section>
<section id="bottom-left-ell_2-penalty-geometry." data-number="0.10.2.0.3">
<h5 data-number="1.10.2.0.3"><span class="header-section-number">1.10.2.0.3</span> Bottom-left: <span class="math inline">\(\ell_2\)</span> penalty geometry.</h5>
<p>The circular constraint region corresponds to the L2 penalty used in Ridge regression. Unlike the diamond shape, the circle has no sharp corners. As a result, the loss contours typically touch the constraint boundary at a smooth point rather than on an axis. This leads to coefficients being <strong>shrunk toward zero</strong> but rarely becoming exactly zero.</p>
</section>
<section id="bottom-right-ridge-optimization-path." data-number="0.10.2.0.4">
<h5 data-number="1.10.2.0.4"><span class="header-section-number">1.10.2.0.4</span> Bottom-right: Ridge optimization path.</h5>
<p>The gradient descent path moves smoothly toward the center. The final solution lies near the origin, but both parameters remain nonzero. Ridge therefore provides <strong>stability and shrinkage</strong> without performing feature selection.</p>
</section>
<section id="elastic-net-as-a-compromise." data-number="0.10.2.0.5">
<h5 data-number="1.10.2.0.5"><span class="header-section-number">1.10.2.0.5</span> Elastic Net as a compromise.</h5>
<p>Elastic Net combines both penalties, creating a constraint region that lies between the diamond and the circle. This gives it two key advantages:</p>
<ul>
<li><p>Like Lasso, it can produce sparse solutions and perform feature selection.</p></li>
<li><p>Like Ridge, it remains stable when features are highly correlated, avoiding the instability that Lasso can exhibit in such settings.</p></li>
</ul>
<p>For this reason, Elastic Net is especially useful in high-dimensional datasets where the number of features is large relative to the number of training samples.</p>
</section>
</section>
</section>
<section id="performance-metrics" data-number="0.11">
<h2 data-number="1.11"><span class="header-section-number">1.11</span> Performance Metrics</h2>
<p>After training a regression model, we need quantitative ways to evaluate how well it performs. Performance metrics measure the difference between predicted values and the true targets. Different metrics emphasize different types of errors, so in practice multiple metrics are often reported together.</p>
<section id="mean-squared-error-mse" data-number="0.11.1">
<h3 data-number="1.11.1"><span class="header-section-number">1.11.1</span> Mean Squared Error (MSE)</h3>
<p>The <strong>Mean Squared Error (MSE)</strong> is one of the most common metrics for regression. It measures the average squared difference between predicted values and true values.</p>
<p><span class="math display">\[\text{MSE}(\mathbf{X},\boldsymbol{\theta})=\frac{1}{N}\sum_{i=1}^{N}(\hat{y}_i-y_i)^2\]</span></p>
<p>where <span class="math inline">\(\hat{y}_i=\boldsymbol{\theta}^T\mathbf{x}_i\)</span> is the predicted value.</p>
<p>Squaring the error has two important effects:</p>
<ul>
<li><p>Large errors are penalized more heavily than small errors.</p></li>
<li><p>The function is smooth and differentiable, making it convenient for optimization.</p></li>
</ul>
<p>However, because errors are squared, MSE can be sensitive to outliers.</p>
</section>
<section id="root-mean-squared-error-rmse" data-number="0.11.2">
<h3 data-number="1.11.2"><span class="header-section-number">1.11.2</span> Root Mean Squared Error (RMSE)</h3>
<p>The <strong>Root Mean Squared Error (RMSE)</strong> is the square root of the MSE:</p>
<p><span class="math display">\[\text{RMSE}(\mathbf{X},\boldsymbol{\theta})=\sqrt{\frac{1}{N}\sum_{i=1}^{N}(\hat{y}_i-y_i)^2}\]</span></p>
<p>RMSE has the same units as the target variable, making it easier to interpret than MSE. For example, if the target variable is measured in dollars, RMSE is also measured in dollars.</p>
</section>
<section id="mean-absolute-error-mae" data-number="0.11.3">
<h3 data-number="1.11.3"><span class="header-section-number">1.11.3</span> Mean Absolute Error (MAE)</h3>
<p>The <strong>Mean Absolute Error (MAE)</strong> measures the average magnitude of prediction errors:</p>
<p><span class="math display">\[\text{MAE}(\mathbf{X},\boldsymbol{\theta})=\frac{1}{N}\sum_{i=1}^{N}|\hat{y}_i-y_i|\]</span></p>
<p>Unlike MSE, MAE does not square the error. This makes MAE:</p>
<ul>
<li><p>More robust to outliers</p></li>
<li><p>Less sensitive to large individual errors</p></li>
</ul>
<p>In practice, MAE and MSE are often reported together to better understand model behavior.</p>
</section>
<section id="coefficient-of-determination-r2" data-number="0.11.4">
<h3 data-number="1.11.4"><span class="header-section-number">1.11.4</span> Coefficient of Determination (<span class="math inline">\(R^2\)</span>)</h3>
<p>The <strong>Coefficient of Determination</strong> (<span class="math inline">\(R^2\)</span>) measures how well the model explains the variance in the target variable. It compares the prediction error of the model to the error of a simple baseline model that always predicts the mean of the data.</p>
<p><span class="math display">\[R^2 = 1 - \frac{SS_{\text{res}}}{SS_{\text{tot}}}\]</span></p>
<p>where</p>
<p><span class="math display">\[SS_{\text{res}}=\sum_{i=1}^{N}(y_i-\hat{y}_i)^2\]</span> <span class="math display">\[SS_{\text{tot}}=\sum_{i=1}^{N}(y_i-\bar{y})^2\]</span></p>
<p>Interpretation:</p>
<ul>
<li><p><strong><span class="math inline">\(R^2 = 1\)</span></strong>: Perfect predictions.</p></li>
<li><p><strong><span class="math inline">\(R^2 = 0\)</span></strong>: Model performs no better than predicting the mean.</p></li>
<li><p><strong><span class="math inline">\(R^2 &lt; 0\)</span></strong>: Model performs worse than the mean predictor.</p></li>
</ul>
<p><span class="math inline">\(R^2\)</span> provides a normalized measure of performance and is especially useful when comparing different models on the same dataset.</p>
</section>
</section>
<section id="model-validation" data-number="0.12">
<h2 data-number="1.12"><span class="header-section-number">1.12</span> Model Validation</h2>
<p>Machine learning models must be evaluated carefully to ensure they generalize to unseen data. This section introduces the core tools used to assess model performance and diagnose common issues such as underfitting and overfitting. We discuss dataset splitting, learning curves, cross-validation, model complexity, and the bias–variance tradeoff.</p>
<section id="trainingvalidationtest-split" data-number="0.12.1">
<h3 data-number="1.12.1"><span class="header-section-number">1.12.1</span> Training/Validation/Test Split</h3>
<p>The test set is kept completely separate from model training and validation. Its purpose is to evaluate performance on unseen data and simulate real-world deployment. By assessing the model on this independent dataset, we ensure that reported performance metrics are reliable indicators of real-world performance.</p>
<p>It is critical that the <strong>test set is never used during model development</strong>. If the test set influences model design or hyperparameter tuning, the evaluation becomes biased and overly optimistic. The test set should only be used once, after the final model has been selected.</p>
<p>The training dataset is where the model learns. It’s typically divided into two parts:</p>
<ol>
<li><p>Training Data: The larger portion used to teach the model by adjusting its parameters based on the loss function.</p></li>
<li><p>Validation Data: A smaller portion used to monitor the model’s performance during training. It helps fine-tune hyperparameters and assess the risk of overfitting by providing an unbiased evaluation.</p></li>
</ol>
<p>The validation subset serves as a checkpoint during training. It helps identify issues like overfitting and informs adjustments to model design and hyperparameters. This ensures that the model remains robust and can generalize well to new data. Typically, 20–30% of the dataset is reserved for testing, while the remaining data is used for training and validation.</p>
<figure>
<img src="img/lecture9/Screenshot 2024-09-22 at 6.02.54 PM.png" alt="Visualization of dataset split" /><figcaption aria-hidden="true">Visualization of dataset split</figcaption>
</figure>
<section id="k-fold-cross-validation-procedure" data-number="0.12.1.0.1">
<h5 data-number="1.12.1.0.1"><span class="header-section-number">1.12.1.0.1</span> K-fold Cross-Validation Procedure</h5>
<p>A widely used approach for hyperparameter tuning is <span class="math inline">\(k\)</span>-fold cross-validation. The procedure is:</p>
<ol>
<li><p>Shuffle the dataset randomly.</p></li>
<li><p>Split the dataset into <span class="math inline">\(k\)</span> roughly equal subsets (folds).</p></li>
<li><p>For each fold:</p>
<ul>
<li><p>Use the fold as the validation set.</p></li>
<li><p>Use the remaining <span class="math inline">\(k-1\)</span> folds as the training set.</p></li>
<li><p>Train the model and record the validation score.</p></li>
</ul></li>
<li><p>Average the validation scores across all folds.</p></li>
</ol>
<p>After cross-validation, the model is retrained on the full training data before final evaluation on the test set.</p>
<figure>
<img src="img/lecture10/crossvalidation.png" alt="K-fold cross-validation procedure." /><figcaption aria-hidden="true">K-fold cross-validation procedure.</figcaption>
</figure>
</section>
</section>
<section id="learning-curves" data-number="0.12.2">
<h3 data-number="1.12.2"><span class="header-section-number">1.12.2</span> Learning Curves</h3>
<p>When we evaluate a model’s performance, we typically look at two key metrics: the training score and the validation score. As we increase the size of the training set, these scores can behave differently, providing valuable insights into the model’s learning process and its ability to generalize.</p>
<ol>
<li><p><strong>Training Score</strong>: The training score indicates how well the model fits the training data. As the size of the training set increases, the training score usually improves. This is because a larger training set provides more examples for the model to learn from, allowing it to better capture the underlying patterns in the data.</p></li>
<li><p><strong>Validation Score</strong>: The validation score reflects the model’s performance on a separate validation dataset. This score helps assess how well the model can generalize to unseen data. Initially, as the training set size increases, the validation score may also improve, indicating that the model is effectively learning relevant features and relationships.</p></li>
</ol>
<p>Learning curves are a powerful diagnostic tool. By comparing training and validation errors as the dataset size increases, we can determine whether a model is suffering from underfitting (high bias) or overfitting (high variance).</p>
<section id="how-learning-curves-are-generated" data-number="0.12.2.0.1">
<h5 data-number="1.12.2.0.1"><span class="header-section-number">1.12.2.0.1</span> How Learning Curves Are Generated</h5>
<p>To generate learning curves, we repeatedly train the model using increasing amounts of training data and evaluate on a fixed validation set:</p>
<ol>
<li><p>Reserve a validation set of size <span class="math inline">\(v\)</span> from the dataset of size <span class="math inline">\(n\)</span>.</p></li>
<li><p>For <span class="math inline">\(k = 1, 2, \dots, n-v\)</span>:</p>
<ul>
<li><p>Train the model using the first <span class="math inline">\(k\)</span> training samples.</p></li>
<li><p>Evaluate training and validation error.</p></li>
</ul></li>
</ol>
<figure>
<img src="img/lecture9/Screenshot 2024-09-22 at 6.09.00 PM.png" alt="Underfit linear regression model" /><figcaption aria-hidden="true">Underfit linear regression model</figcaption>
</figure>
<figure>
<img src="img/lecture9/Screenshot 2024-09-22 at 6.09.19 PM.png" alt="Overfit polynomial regression model" /><figcaption aria-hidden="true">Overfit polynomial regression model</figcaption>
</figure>
<p>What we can see from this is that the linear model shows higher error rates and plateaus early in training and validation causing underfitting. The polynomial model on the other hand has a lower error than the linear model but has a wider gap which shows overfitting.</p>
</section>
</section>
<section id="cross-validation" data-number="0.12.3">
<h3 data-number="1.12.3"><span class="header-section-number">1.12.3</span> Cross Validation</h3>
<p>Cross-validation is a robust statistical technique used in machine learning to evaluate how well a model will perform on independent, unseen data. By partitioning the data into subsets, cross-validation helps ensure that the evaluation of the model is reliable and minimizes the risk of overfitting.</p>
<p>In <span class="math inline">\(k\)</span>-fold cross-validation, the dataset is divided into <span class="math inline">\(k\)</span> equal parts. The model is trained <span class="math inline">\(k\)</span> times, each time using a different subset as the validation set and the remaining data for training. The final performance estimate is the average across all <span class="math inline">\(k\)</span> runs.</p>
</section>
<section id="model-complexity-vs-prediction-error" data-number="0.12.4">
<h3 data-number="1.12.4"><span class="header-section-number">1.12.4</span> Model Complexity vs Prediction Error</h3>
<p>Model complexity in machine learning, often measured by degrees of freedom, refers to a model’s capacity to fit data, as seen in polynomial regression. As model complexity increases, training error typically decreases because the model can better fit the training data, potentially reaching a point where training error approaches zero, indicating it captures even the noise. Initially, testing error may also decrease as the model learns relevant patterns, but after a certain complexity level, testing error begins to rise due to overfitting—where the model learns noise instead of generalizable patterns. This highlights the bias-variance tradeoff: low-complexity models may have high bias, leading to oversimplified patterns, while high-complexity models can suffer from high variance, fitting training data well but failing on unseen data. Cross-validation helps identify the optimal level of complexity that minimizes generalization error, while regularization mitigates overfitting by penalizing excessive complexity.</p>
<figure>
<img src="img/lecture9/Screenshot 2024-09-22 at 6.22.40 PM.png" style="width:50.0%" alt="Diagram depicting the effects of model complexity as degrees of freedom increase" /><figcaption aria-hidden="true">Diagram depicting the effects of model complexity as degrees of freedom increase</figcaption>
</figure>
<figure>
<img src="img/lecture10/modelcomplexity.png" alt="Model complexity vs training and testing error." /><figcaption aria-hidden="true">Model complexity vs training and testing error.</figcaption>
</figure>
</section>
<section id="bias-variance-tradeoff" data-number="0.12.5">
<h3 data-number="1.12.5"><span class="header-section-number">1.12.5</span> Bias-Variance Tradeoff</h3>
<p>The bias–variance tradeoff provides a theoretical explanation for the behavior observed in learning curves and model complexity plots.</p>
<p>Training a Linear Regression model multiple times with different datasets reveals a range of prediction scores, which can provide valuable insights into model performance. The average of these scores, referred to as bias, indicates how consistently the models perform. Models exhibiting high average error are categorized as high-bias, a result of low complexity to capture the underlying patterns in the data. Such models may cause underfitting. In the context of a high-degree Polynomial Regression model, while the bias is significantly reduced due to the model’s increased complexity and flexibility, this comes at a cost. The predictions for a given test point can vary dramatically between different polynomial models, indicating a pronounced sensitivity to the training data. This phenomenon, known as variance, arises when the model becomes overly tailored to the nuances of the training dataset, capturing noise rather than the underlying trend. Consequently, high-variance models struggle to generalize effectively to unseen data, resulting in substantially higher prediction errors when applied to new instances. Moreover, this trade-off highlights the delicate balance between bias and variance, underscoring the importance of model selection and regularization techniques to mitigate overfitting/underfitting while still achieving accurate predictions.</p>
<section id="biasvariance-decomposition" data-number="0.12.5.0.1">
<h5 data-number="1.12.5.0.1"><span class="header-section-number">1.12.5.0.1</span> Bias–Variance Decomposition</h5>
<p>Let <span class="math inline">\(x\)</span> be a test sample, <span class="math inline">\(f(x)\)</span> the true target, and <span class="math inline">\(\hat{f}(x)\)</span> the model prediction. The expected squared error can be decomposed as</p>
<p><span class="math display">\[E[(f(x) - \hat{f}(x))^2]
= \underbrace{(E[\hat{f}(x)] - f(x))^2}_{\text{Bias}^2}
+ \underbrace{E[(\hat{f}(x) - E[\hat{f}(x)])^2]}_{\text{Variance}}
+ \underbrace{\sigma_e^2}_{\text{Irreducible Error}}.\]</span></p>
<p>This decomposition explains why increasing model complexity can reduce bias but increase variance, and motivates the search for an optimal tradeoff.</p>
<figure>
<img src="img/lecture9/Screenshot 2024-09-22 at 6.28.08 PM.png" alt="A diagram depicting the relationship between model complexity and error" /><figcaption aria-hidden="true">A diagram depicting the relationship between model complexity and error</figcaption>
</figure>
<figure>
<img src="img/lecture9/Screenshot 2024-09-22 at 6.30.13 PM.png" alt="Diagram depicting the bias-variance tradeoff" /><figcaption aria-hidden="true">Diagram depicting the bias-variance tradeoff</figcaption>
</figure>
<figure>
<img src="img/lecture9/Screenshot 2024-09-22 at 6.41.12 PM.png" />
</figure>
<p>This decomposition motivates the bias–variance tradeoff: increasing model complexity typically reduces bias but increases variance.</p>
</section>
</section>
</section>
<section id="additional-details" data-number="0.13">
<h2 data-number="1.13"><span class="header-section-number">1.13</span> Additional Details</h2>
<p>In this final section, we briefly connect the practical ideas from this lecture to deeper theoretical concepts from statistical learning theory. The goal is not to develop full theory, but to build intuition for why model complexity, data size, and regularization are fundamentally linked.</p>
<section id="model-complexity" data-number="0.13.1">
<h3 data-number="1.13.1"><span class="header-section-number">1.13.1</span> Model Complexity</h3>
<p>So far, we have discussed model complexity intuitively using learning curves, regularization, and the bias–variance tradeoff. We now take a slightly more formal look at what “model complexity” means.</p>
<p>In general, <em>model complexity</em> refers to the capacity of a model to fit a wide variety of functions or datasets. As discussed earlier, a more complex model has a higher capacity to capture intricate patterns in the data, but this can also increase the risk of overfitting. One way to think about model complexity is in terms of how many training samples are needed for the model to learn patterns that generalize well to new data, i.e. produce low test error. However, this quantity can be difficult to characterize. Here are a few typical notions:</p>
<ul>
<li><p>The degree of the model (e.g., the degree of a polynomial in polynomial regression).</p></li>
<li><p>The number of parameters in the model (e.g., weights in a neural network).</p></li>
<li><p>The flexibility of the <em>hypothesis space</em> the model operates within.</p></li>
</ul>
<p>The first two are essentially the same in that they describe the <em>size</em> of the hypothesis space. In particular, the hypothesis space describes the set of all possible functions the model can select from, given the available data and parameters. For finite spaces, the size is a fair measure of complexity. In fact, in this case we can show that the number of samples needed to adequately fit a model is bounded by the (log of the) size of the space. However, when the space is infinite, as is often the case with continuous models like neural networks, then this notion of counting parameters can break down and we need a more careful measure of complexity.</p>
<p>A classical theoretical way to measure model capacity is the <em>VC (Vapnik–Chervonenkis) dimension</em>. You do not need to compute this in practice, but it provides useful intuition about why complex models require more data. The VC dimension is a theoretical tool used to quantify the capacity of a model by determining the maximum number of points that a model can <em>shatter</em>. Shattering means that for a given set of points, the model can perfectly classify all possible binary labelings of those points. The higher the VC dimension, the more complex the model and the larger the class of functions it can represent.</p>
<p>For example, a linear classifier in 2D (like a linear SVM or perceptron) has a VC dimension of 3, meaning it can shatter any set of 3 points in the plane, but not necessarily 4.</p>
<p>In practice, models with high VC dimensions are prone to overfitting, as they can represent highly complex decision boundaries. However, models with a VC dimension that is too low may underfit, failing to capture important relationships in the data. The key is to choose a model with an appropriate VC dimension for the problem at hand.</p>
<p>While VC dimension is a powerful theoretical concept, it is not always easy to compute for real-world models. Nonetheless, it serves as a guiding principle: models with higher capacity (e.g., more parameters, higher VC dimension) need more training data to generalize well, and regularization becomes crucial to control overfitting in such cases.</p>
<p>This theoretical perspective reinforces the key message of this lecture: as model capacity increases, we must rely on more data, cross-validation, and regularization to ensure good generalization.</p>
</section>
<section id="model-evaluation-under-ambiguous-ground-truth" data-number="0.13.2">
<h3 data-number="1.13.2"><span class="header-section-number">1.13.2</span> Model Evaluation Under Ambiguous Ground Truth</h3>
<p>So far, we have assumed that every training example has a single correct label. However, in many real-world datasets the true label is not perfectly known. Different annotators may disagree, labels may be noisy, and some examples may naturally belong to multiple categories. This situation is known as <strong>ambiguous ground truth</strong> or <strong>label noise</strong>.</p>
<section id="why-accuracy-can-be-misleading" data-number="0.13.2.1">
<h4 data-number="1.13.2.1"><span class="header-section-number">1.13.2.1</span> Why Accuracy Can Be Misleading</h4>
<p>Standard accuracy assumes that dataset labels are perfectly correct. But suppose that a fraction <span class="math inline">\(\epsilon\)</span> of labels are incorrect or uncertain. Even a perfect model cannot exceed the accuracy of the noisy labels.</p>
<p><span class="math display">\[\boxed{
\text{Observed Accuracy} \;\le\; 1 - \epsilon
}\]</span></p>
<p>This means a model might appear to plateau at <span class="math inline">\(90\%\)</span> accuracy even if its true performance is much higher. In practice, label noise creates uncertainty about how measured accuracy relates to the model’s true accuracy <span class="citation" data-cites="DBLP:journals/corr/abs-1908-07086 Quesada2025Largescale"></span>.</p>
<figure>
<img src="img/lecture9/P16.png" id="fig:label_noise_accuracy" alt="Measured vs. true accuracy under label noise. The diagonal line represents perfect labels. When labels contain noise, the measured accuracy lies within a band around this line. The shaded region shows the range of possible true accuracies for a given measured accuracy when ground-truth labels are uncertain." /><figcaption aria-hidden="true"><strong>Measured vs. true accuracy under label noise.</strong> The diagonal line represents perfect labels. When labels contain noise, the measured accuracy lies within a band around this line. The shaded region shows the range of possible true accuracies for a given measured accuracy when ground-truth labels are uncertain.</figcaption>
</figure>
<p>The shaded region illustrates an important takeaway:</p>
<blockquote>
<p>Measured accuracy is not a single number — it represents a <em>range of possible true performances</em>.</p>
</blockquote>
<p>This relationship between measured and true accuracy under noisy labels has been studied extensively in recent work <span class="citation" data-cites="DBLP:journals/corr/abs-1908-07086"></span>.</p>
</section>
<section id="human-label-disagreement" data-number="0.13.2.2">
<h4 data-number="1.13.2.2"><span class="header-section-number">1.13.2.2</span> Human Label Disagreement</h4>
<p>In many tasks, disagreement between humans is not a mistake — it is a signal of inherent ambiguity in the data. For example, an image might contain features of both a dog and a wolf, or a handwritten digit might look like both a 3 and an 8.</p>
<p>Recent research shows that training models using <strong>distributions of human labels</strong> instead of single “hard” labels can improve generalization and robustness <span class="citation" data-cites="cifar10h"></span>.</p>
<figure>
<img src="img/lecture9/P17.png" id="fig:human_cnn_uncertainty" alt="Humans and neural networks disagree in their uncertainty. Top: examples where humans are confident but CNN predictions are uncertain. Middle: examples where humans are uncertain but CNN predictions are confident. Bottom: examples where both distributions diverge. This highlights that label disagreement contains useful information about ambiguity in the data." /><figcaption aria-hidden="true"><strong>Humans and neural networks disagree in their uncertainty.</strong> Top: examples where humans are confident but CNN predictions are uncertain. Middle: examples where humans are uncertain but CNN predictions are confident. Bottom: examples where both distributions diverge. This highlights that label disagreement contains useful information about ambiguity in the data.</figcaption>
</figure>
<p>Rather than treating disagreement as noise, we can treat it as <strong>information about uncertainty</strong>.</p>
</section>
<section id="practical-implications" data-number="0.13.2.3">
<h4 data-number="1.13.2.3"><span class="header-section-number">1.13.2.3</span> Practical Implications</h4>
<p>When labels are noisy or ambiguous, model evaluation becomes more subtle. Standard accuracy may <strong>underestimate true model performance</strong> because the model is being judged against imperfect labels. At the same time, models can <strong>overfit to label noise</strong>, memorizing incorrect annotations rather than learning generalizable patterns. For this reason, techniques such as <strong>cross-validation</strong> become even more important, and <strong>regularization</strong> plays a key role in preventing models from memorizing incorrect labels.</p>
<p>These challenges motivate the use of more robust training and evaluation strategies. In practice, modern machine learning often relies on <strong>robust evaluation metrics</strong>, <strong>label smoothing</strong>, and <strong>probabilistic (soft) labels</strong> that represent uncertainty in annotations rather than assuming a single perfectly correct label. These ideas have become increasingly important in modern machine learning, where large-scale datasets often contain imperfect or ambiguous annotations <span class="citation" data-cites="Quesada2025Largescale cifar10h"></span>.</p>
<p><br />
<br />
<strong>Key takeaway:</strong> Modern machine learning increasingly treats labels as <em>probabilistic</em> rather than perfectly correct.</p>
</section>
</section>
</section>
<section id="qa-section" data-number="0.14">
<h2 data-number="1.14"><span class="header-section-number">1.14</span> Q&amp;A Section</h2>
<ol>
<li><p><strong>Question 1:</strong></p>
<p>Which optimization method uses Second-order curvature information of the loss function?</p>
<ol>
<li><p>Gradient Descent</p></li>
<li><p>Coordinate Descent</p></li>
<li><p>Newton’s Method</p></li>
<li><p>Stochastic Gradient Descent</p></li>
</ol>
<p><strong>Answer:</strong> The correct answer is <strong>C)</strong>.</p>
<p><strong>Solution:</strong></p>
<p>Newton’s Method uses both the gradient and the Hessian matrix: <span class="math display">\[\boldsymbol{\theta}^{t+1}
=
\boldsymbol{\theta}^{t}
-\alpha \mathbf{H}(\boldsymbol{\theta})^{-1}
\nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta}).\]</span></p>
<p>The Hessian contains second derivatives and captures curvature of the loss surface.</p></li>
<li><p><strong>Question 2:</strong></p>
<p>Why is Newton’s Method rarely used for very large models?</p>
<ol>
<li><p>It does not converge.</p></li>
<li><p>It requires storing and inverting a <span class="math inline">\(P\times P\)</span> Hessian matrix.</p></li>
<li><p>It cannot minimize convex functions.</p></li>
<li><p>It requires labeled data.</p></li>
</ol>
<p><strong>Answer:</strong> The correct answer is <strong>B)</strong>.</p>
<p><strong>Solution:</strong></p>
<p>The Hessian has size <span class="math inline">\(P\times P\)</span>. Storing it costs <span class="math inline">\(O(P^2)\)</span> memory and inverting it costs <span class="math inline">\(O(P^3)\)</span> time, which becomes infeasible when <span class="math inline">\(P\)</span> is large.</p></li>
<li><p><strong>Question 3:</strong></p>
<p>Which optimization method can be used when gradients are unavailable?</p>
<ol>
<li><p>Newton’s Method</p></li>
<li><p>Gradient Descent</p></li>
<li><p>Coordinate Search</p></li>
<li><p>Ridge Regression</p></li>
</ol>
<p><strong>Answer:</strong> The correct answer is <strong>C)</strong>.</p>
<p><strong>Solution:</strong></p>
<p>Coordinate Search is a derivative-free method. It explores directions <span class="math inline">\(\boldsymbol{\theta} \pm \alpha \mathbf{e}_j\)</span> and accepts moves that reduce the loss.</p></li>
<li><p><strong>Question 4:</strong></p>
<p>What is the main effect of Ridge (<span class="math inline">\(\ell_2\)</span>) regularization on model coefficients?</p>
<ol>
<li><p>Sets many coefficients exactly to zero</p></li>
<li><p>Shrinks coefficients smoothly toward zero</p></li>
<li><p>Increases model variance</p></li>
<li><p>Removes correlated features automatically</p></li>
</ol>
<p><strong>Answer:</strong> The correct answer is <strong>B)</strong>.</p>
<p><strong>Solution:</strong></p>
<p>Ridge adds the penalty <span class="math inline">\(\lambda\|\boldsymbol{\theta}\|_2^2\)</span>, which discourages large parameter values and reduces variance, but does not usually force coefficients to exactly zero.</p></li>
<li><p><strong>Question 5:</strong></p>
<p>Which regularization technique performs automatic feature selection?</p>
<ol>
<li><p>Ridge Regression</p></li>
<li><p>Lasso Regression</p></li>
<li><p>Elastic Net with <span class="math inline">\(r=0\)</span></p></li>
<li><p>Mean Squared Error</p></li>
</ol>
<p><strong>Answer:</strong> The correct answer is <strong>B)</strong>.</p>
<p><strong>Solution:</strong></p>
<p>Lasso uses the <span class="math inline">\(L_1\)</span> penalty applied to the parameter vector <span class="math inline">\(\boldsymbol{\theta}\)</span>: <span class="math inline">\(\sum |\theta_j|\)</span>, which creates sharp corners in the optimization landscape. Solutions often lie on these corners, causing some coefficients to become exactly zero.</p></li>
<li><p><strong>Question 6:</strong></p>
<p>Which metric is most sensitive to large outliers?</p>
<ol>
<li><p>MAE</p></li>
<li><p>MSE</p></li>
<li><p>Accuracy</p></li>
<li><p><span class="math inline">\(R^2\)</span></p></li>
</ol>
<p><strong>Answer:</strong> The correct answer is <strong>B)</strong>.</p>
<p><strong>Solution:</strong></p>
<p>MSE squares the error: <span class="math display">\[(\hat{y}_i - y_i)^2,\]</span> which heavily penalizes large errors, making it sensitive to outliers.</p></li>
<li><p><strong>Question 7:</strong></p>
<p>A model achieves <span class="math inline">\(R^2 = -0.3\)</span> on a test set. What does this mean?</p>
<ol>
<li><p>Perfect predictions</p></li>
<li><p>Better than predicting the mean</p></li>
<li><p>Worse than predicting the mean</p></li>
<li><p>Overfitting has been eliminated</p></li>
</ol>
<p><strong>Answer:</strong> The correct answer is <strong>C)</strong>.</p>
<p><strong>Solution:</strong></p>
<p><span class="math display">\[R^2 = 1 - \frac{SS_{res}}{SS_{tot}}.\]</span> If <span class="math inline">\(R^2 &lt; 0\)</span>, the model performs worse than simply predicting the mean of the data.</p></li>
<li><p><strong>Question 8:</strong></p>
<p>Why must the test set never be used during training or hyperparameter tuning?</p>
<ol>
<li><p>It increases computation time</p></li>
<li><p>It causes underfitting</p></li>
<li><p>It introduces evaluation bias</p></li>
<li><p>It reduces training accuracy</p></li>
</ol>
<p><strong>Answer:</strong> The correct answer is <strong>C)</strong>.</p>
<p><strong>Solution:</strong></p>
<p>Using the test set during model development leaks information and produces overly optimistic performance estimates.</p></li>
<li><p><strong>Question 9:</strong></p>
<p>Suppose <span class="math inline">\(10\%\)</span> of dataset labels are incorrect. What is the maximum achievable measured accuracy of a perfect model?</p>
<ol>
<li><p>100%</p></li>
<li><p>95%</p></li>
<li><p>90%</p></li>
<li><p>80%</p></li>
</ol>
<p><strong>Answer:</strong> The correct answer is <strong>C)</strong>.</p>
<p><strong>Solution:</strong></p>
<p>With label noise <span class="math inline">\(\epsilon\)</span>: <span class="math display">\[\text{Observed Accuracy} \le 1-\epsilon.\]</span> If <span class="math inline">\(\epsilon=0.1\)</span>, the maximum observed accuracy is <span class="math inline">\(0.9\)</span>.</p></li>
<li><p><strong>Question 10:</strong></p>
<p>Which method helps estimate generalization performance while using all available data efficiently?</p>
<ol>
<li><p>Cross-validation</p></li>
<li><p>Gradient Descent</p></li>
<li><p>Newton’s Method</p></li>
<li><p>Coordinate Search</p></li>
</ol>
<p><strong>Answer:</strong> The correct answer is <strong>A)</strong>.</p>
<p><strong>Solution:</strong></p>
<p>In <span class="math inline">\(k\)</span>-fold cross-validation, the model is trained <span class="math inline">\(k\)</span> times using different validation splits, and performance is averaged to estimate generalization error.</p></li>
<li><p><strong>Question 11:</strong></p>
<p>Consider the model complexity vs. prediction error curve below.</p>
<div class="center">
<p><img src="img/lecture9/qa_bias_variance_curve.png" alt="image" /></p>
</div>
<p>Which region corresponds to an <strong>underfitting</strong> model?</p>
<ol>
<li><p>Region A (left side)</p></li>
<li><p>Region B (middle)</p></li>
<li><p>Region C (right side)</p></li>
<li><p>All regions equally</p></li>
</ol>
<p><strong>Answer:</strong> The correct answer is <strong>A)</strong>.</p>
<p><strong>Solution:</strong></p>
<p>On the left side of the curve, the model has low complexity and high bias. It cannot capture patterns in the data, leading to high training and validation error. This is the definition of <strong>underfitting</strong>.</p></li>
</ol>
</section>

</main>
</body>
</html>
