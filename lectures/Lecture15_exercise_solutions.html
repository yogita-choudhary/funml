<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>Lecture15 In-class Exercise Solutions</title>
  <link rel="stylesheet" href="../assets/style.css"/>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<nav>
  <a href="../index.html">Home</a>
</nav>
<main>
<div class="center">
<p><span><strong>In-Class Exercise (Canvas Quiz) — 10 minutes</strong></span><br />
<span><strong>Lecture 15: CNN Architectures</strong></span></p>
</div>
<p><strong>Question 1 (VGG Design Insight)</strong><br />
VGG networks often replace a single <span class="math inline">\(5\times5\)</span> convolution with two stacked <span class="math inline">\(3\times3\)</span> convolutions (stride <span class="math inline">\(1\)</span>, padding chosen to preserve spatial size).</p>
<p>Which statement is <strong>most correct</strong>?</p>
<ol>
<li><p>The stacked <span class="math inline">\(3\times3\)</span> layers have a larger receptive field than <span class="math inline">\(5\times5\)</span>.</p></li>
<li><p>The stacked <span class="math inline">\(3\times3\)</span> layers have the same receptive field and introduce an extra nonlinearity.</p></li>
<li><p>The stacked <span class="math inline">\(3\times3\)</span> layers always use more parameters.</p></li>
<li><p>The stacked <span class="math inline">\(3\times3\)</span> layers reduce spatial resolution.</p></li>
</ol>
<p><strong>Solution 1</strong><br />
Two <span class="math inline">\(3\times 3\)</span> convolutions (stride 1, same padding) have an effective receptive field of <span class="math inline">\(5\times 5\)</span>, matching a single <span class="math inline">\(5\times 5\)</span> conv. Stacking also typically inserts an additional nonlinearity (e.g., ReLU) between layers, increasing expressivity. <span class="math display">\[\boxed{\textbf{Answer: (b)}}\]</span></p>
<p><strong>Question 2 (Inception / <span class="math inline">\(1\times1\)</span> Convolutions)</strong><br />
In GoogLeNet, a <span class="math inline">\(1\times1\)</span> convolution is often placed before a <span class="math inline">\(3\times3\)</span> convolution.</p>
<p>What is the <strong>primary reason</strong> for this design choice?</p>
<ol>
<li><p>Increase the spatial resolution of the feature maps.</p></li>
<li><p>Reduce computational cost by shrinking the channel dimension before expensive convolutions.</p></li>
<li><p>Increase the receptive field of the network.</p></li>
<li><p>Replace the need for activation functions.</p></li>
</ol>
<p><strong>Solution 2</strong><br />
A <span class="math inline">\(1\times 1\)</span> convolution acts as a learnable channel projection that can reduce the number of channels (a “bottleneck”), cutting parameters and computation before applying more expensive convolutions like <span class="math inline">\(3\times 3\)</span>. <span class="math display">\[\boxed{\textbf{Answer: (b)}}\]</span></p>
<p><strong>Question 3 (ResNet Skip Connections)</strong><br />
A residual block computes <span class="math display">\[\mathbf{y} = \mathbf{x} + F(\mathbf{x}).\]</span></p>
<p>What is the <strong>main benefit</strong> of this skip connection?</p>
<ol>
<li><p>It guarantees the model cannot overfit.</p></li>
<li><p>It allows gradients to flow more easily, making very deep networks easier to train.</p></li>
<li><p>It always reduces the number of parameters.</p></li>
<li><p>It doubles the receptive field at each layer.</p></li>
</ol>
<p><strong>Solution 3</strong><br />
The identity (skip) path provides a direct route for gradient flow and helps optimization, enabling much deeper networks to train effectively without degradation issues. <span class="math display">\[\boxed{\textbf{Answer: (b)}}\]</span></p>

</main>
</body>
</html>
